{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucrSg4uCcYwv",
        "outputId": "f4f66e7f-43fc-4ac7-d736-af7dff813325"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting finance-datareader\n",
            "  Downloading finance_datareader-0.9.50-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.10/dist-packages (from finance-datareader) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from finance-datareader) (2.27.1)\n",
            "Collecting requests-file (from finance-datareader)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from finance-datareader) (4.9.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from finance-datareader) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.2->finance-datareader) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.2->finance-datareader) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.2->finance-datareader) (1.22.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->finance-datareader) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->finance-datareader) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->finance-datareader) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->finance-datareader) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from requests-file->finance-datareader) (1.16.0)\n",
            "Installing collected packages: requests-file, finance-datareader\n",
            "Successfully installed finance-datareader-0.9.50 requests-file-1.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U finance-datareader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMetHaZlmpm1",
        "outputId": "1f36ecfd-d221-424c-81ab-55ac759d63f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.25)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.27.1)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.3)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2022.7.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.3.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.11.2)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.4.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yfinance) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yfinance) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26->yfinance) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKWceaSQVrGP",
        "outputId": "95dc80e9-868a-4e28-ee78-89df01764114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.10.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ta) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ta) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->ta) (1.16.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.10.2-py3-none-any.whl size=29088 sha256=fcc9c8a2717fafc3831b8803c16a9abe02fcd5b9a3b6b563e0207e598992e379\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/51/06/380dc516ea78621870b93ff65527c251afdfdc5fa9d7f4d248\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.10.2\n"
          ]
        }
      ],
      "source": [
        "!pip install ta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7XsbUfpLH3d",
        "outputId": "c2639fe0-c04a-402f-b151-9fe625387e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pmdarima\n",
            "  Downloading pmdarima-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.3.1)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.29.36)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.10.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.13.5)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.26.16)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->pmdarima) (3.2.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (23.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels>=0.13.2->pmdarima) (1.16.0)\n",
            "Installing collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pmdarima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxBrA0_fyKEs",
        "outputId": "f8992648-e995-4c17-a9cb-3907ffdf808b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {
        "id": "TdsolCZKALLs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pywt\n",
        "from google.colab import drive\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import FinanceDataReader as fdr\n",
        "from pandas_datareader import data as pdr\n",
        "import yfinance as yf\n",
        "import ta\n",
        "import time\n",
        "import sys\n",
        "\n",
        "from sklearn import linear_model as lm\n",
        "import xgboost as xgb\n",
        "import sklearn\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
        "\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.robust import mad\n",
        "import pmdarima as pm\n",
        "from pmdarima.arima import ndiffs\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.nn.utils import weight_norm\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print('python version: ', sys.version)\n",
        "#print('python interpreter: ', sys.executable)\n",
        "#print('operating system: ', sys.platform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N5EJHieOrhK",
        "outputId": "ddc6ce3a-2fb5-4574-d884-644a8c7620ee"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python version:  3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0]\n",
            "python interpreter:  /usr/bin/python3\n",
            "operating system:  linux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Yg2SveB_T3Kx"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "SEED = 42\n",
        "seed_everything(SEED) # Seed 고정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzlGsH4bA1zZ",
        "outputId": "55570702-9ca0-4326-ee1f-05adeabdb264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PY5gaabPBKHX"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/drive/MyDrive/open (2)/'\n",
        "train = pd.read_csv(data_dir + 'past_open/train.csv')\n",
        "train_additional = pd.read_csv(data_dir + 'train_additional.csv')\n",
        "#train = pd.read_parquet(data_dir + 'full_train.parquet')\n",
        "sample_submission = pd.read_csv(data_dir + 'past_open/sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pfV_maR5_w_",
        "outputId": "6c020dfc-5f91-44c3-dc4e-5b54ba6cb490"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(       Date item_Code item_Name  Volume   Open   High    Low  Close  Date_ym  \\\n",
              " 0  20210601   A000020      동화약품  114966  14700  14700  14450  14600   202106   \n",
              " 1  20210602   A000020      동화약품  109559  14700  14700  14450  14500   202106   \n",
              " 2  20210603   A000020      동화약품   96158  14550  14650  14450  14600   202106   \n",
              " 3  20210604   A000020      동화약품  133900  14600  14800  14550  14700   202106   \n",
              " 4  20210607   A000020      동화약품  511140  14800  15550  14750  15150   202106   \n",
              " \n",
              "    Close_usd_krw  ...         SR         WR   AO          KAMA  ROC       PPO  \\\n",
              " 0    1106.979980  ...  60.000000 -40.000000  0.0  14600.000000  0.0  0.000000   \n",
              " 1    1107.209961  ...  20.000000 -80.000000  0.0  14599.583767  0.0 -0.054666   \n",
              " 2    1109.880005  ...  60.000000 -40.000000  0.0  14599.585499  0.0 -0.042205   \n",
              " 3    1115.439941  ...  71.428571 -28.571429  0.0  14600.003458  0.0  0.022689   \n",
              " 4    1109.680054  ...  63.636364 -36.363636  0.0  14602.292725  0.0  0.318224   \n",
              " \n",
              "          PVO  Daily_Range     Mean   ARIMA_close  \n",
              " 0   0.000000         -100  14575.0  14600.000000  \n",
              " 1  -0.376490         -200  14575.0  14234.067126  \n",
              " 2  -1.619559           50  14550.0  14440.362082  \n",
              " 3   0.086992          100  14675.0  14562.525683  \n",
              " 4  22.003352          350  15150.0  14667.648856  \n",
              " \n",
              " [5 rows x 96 columns],\n",
              " (1086000, 96))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "train.head(), train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7NhP8yCBcE4",
        "outputId": "aa9c8cff-5252-4492-cb2c-a8cdc883ce95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(         종목코드    순위\n",
              " 0     A000020     1\n",
              " 1     A000040     2\n",
              " 2     A000050     3\n",
              " 3     A000070     4\n",
              " 4     A000080     5\n",
              " ...       ...   ...\n",
              " 1995  A375500  1996\n",
              " 1996  A378850  1997\n",
              " 1997  A383220  1998\n",
              " 1998  A383310  1999\n",
              " 1999  A383800  2000\n",
              " \n",
              " [2000 rows x 2 columns],\n",
              " (2000, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "sample_submission, sample_submission.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9VzsCuBCoW3"
      },
      "source": [
        "- 전체 2000주 중 상위 200주와 하위 200주를 선정하여 매일 동일 금액 비중으로 거래를 진행한다고 가정\n",
        "- 상위 200주에 대해서는 매수 전략(Long)을 취하고, 하위 200주에 대해서는 공매도 전략(Short Selling)을 취함\n",
        "- 거래기간 중 매일 종가에 매수와 공매도가 이루어진다고 가정\n",
        "  - 매수를 통해 보유한 주식은 다음날 종가에 판매한다고 가정\n",
        "  - 공매도의 상환 역시 다음날 종가에 이루어진다고 가정\n",
        "- 결론적으로 참가자들은 거래기간 동안 매일 Long-Short을 반복한다고 가정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J0pVw-XDnDx"
      },
      "source": [
        "public)\n",
        "- Public 기간 중 예측시점은 2023/5/30\n",
        "- 참가자들은 예측시점으로부터 향후 15일의 주식 거래일 동안 포트폴리오에 따라 주식거래를 진행\n",
        "- 해당 기간은 2023/5/31부터 2023/6/21이며, 주식거래가 이뤄지지 않는 날은 제외 (6/6)\n",
        "\n",
        "private)\n",
        "- Private 기간 전 예측시점은 2023/7/28, 즉 대회 종료시점\n",
        "- 참가자들은 예측시점으로부터 향후 15일 주식 거래일 동안 포트폴리오에 따라 주식거래를 진행\n",
        "- 해당 기간은 2023/7/31부터 2023/8/21이며, 주식거래가 이뤄지지 않는 날은 제외 (8/15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzTzPclOE7Ty"
      },
      "source": [
        "Private 리더보드는 8/3일부터 공개되기 시작하며, 8/21일의 샤프 지수로 최종 순위가 결정됨 (이는 수식상 n이 1과 2일때 샤프 지수가 계산되지 않기 때문)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tACVogbAF_2K"
      },
      "source": [
        "액면분할, 액면병합에 따른 주가의 변동도 고려대상에 포함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkbfJNFAZyij",
        "outputId": "a13ab44e-a0b3-4d46-9672-ef6b7cc66dd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['일자', '종목코드', '종목명', '거래량', '시가', '고가', '저가', '종가'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([train, train_additional], axis = 0)"
      ],
      "metadata": {
        "id": "eFNTPdCiJN46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "8fGfJX8MJe7R",
        "outputId": "f1bb344d-ff7e-49a9-ca2c-0c11d5301654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             일자     종목코드     종목명     거래량     시가     고가     저가     종가\n",
              "0      20210601  A060310      3S  166690   2890   2970   2885   2920\n",
              "1      20210601  A095570  AJ네트웍스   63836   5860   5940   5750   5780\n",
              "2      20210601  A006840   AK홀딩스  103691  35500  35600  34150  34400\n",
              "3      20210601  A054620     APS  462544  14600  14950  13800  14950\n",
              "4      20210601  A265520   AP시스템  131987  29150  29150  28800  29050\n",
              "...         ...      ...     ...     ...    ...    ...    ...    ...\n",
              "83995  20230728  A001080    만호제강   12964  35550  36000  34700  36000\n",
              "83996  20230728  A104700    한국철강   72644   5780   6030   5780   6030\n",
              "83997  20230728  A045100   한양이엔지   59562  16230  16390  15970  16330\n",
              "83998  20230728  A000020    동화약품   86169   9870  10080   9700   9800\n",
              "83999  20230728  A060380   동양에스텍  368634   2785   2925   2770   2865\n",
              "\n",
              "[1072000 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-793ebca9-ac55-42b2-8e6b-edbf967dae78\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>일자</th>\n",
              "      <th>종목코드</th>\n",
              "      <th>종목명</th>\n",
              "      <th>거래량</th>\n",
              "      <th>시가</th>\n",
              "      <th>고가</th>\n",
              "      <th>저가</th>\n",
              "      <th>종가</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A060310</td>\n",
              "      <td>3S</td>\n",
              "      <td>166690</td>\n",
              "      <td>2890</td>\n",
              "      <td>2970</td>\n",
              "      <td>2885</td>\n",
              "      <td>2920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A095570</td>\n",
              "      <td>AJ네트웍스</td>\n",
              "      <td>63836</td>\n",
              "      <td>5860</td>\n",
              "      <td>5940</td>\n",
              "      <td>5750</td>\n",
              "      <td>5780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A006840</td>\n",
              "      <td>AK홀딩스</td>\n",
              "      <td>103691</td>\n",
              "      <td>35500</td>\n",
              "      <td>35600</td>\n",
              "      <td>34150</td>\n",
              "      <td>34400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A054620</td>\n",
              "      <td>APS</td>\n",
              "      <td>462544</td>\n",
              "      <td>14600</td>\n",
              "      <td>14950</td>\n",
              "      <td>13800</td>\n",
              "      <td>14950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A265520</td>\n",
              "      <td>AP시스템</td>\n",
              "      <td>131987</td>\n",
              "      <td>29150</td>\n",
              "      <td>29150</td>\n",
              "      <td>28800</td>\n",
              "      <td>29050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83995</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A001080</td>\n",
              "      <td>만호제강</td>\n",
              "      <td>12964</td>\n",
              "      <td>35550</td>\n",
              "      <td>36000</td>\n",
              "      <td>34700</td>\n",
              "      <td>36000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83996</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A104700</td>\n",
              "      <td>한국철강</td>\n",
              "      <td>72644</td>\n",
              "      <td>5780</td>\n",
              "      <td>6030</td>\n",
              "      <td>5780</td>\n",
              "      <td>6030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83997</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A045100</td>\n",
              "      <td>한양이엔지</td>\n",
              "      <td>59562</td>\n",
              "      <td>16230</td>\n",
              "      <td>16390</td>\n",
              "      <td>15970</td>\n",
              "      <td>16330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83998</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>86169</td>\n",
              "      <td>9870</td>\n",
              "      <td>10080</td>\n",
              "      <td>9700</td>\n",
              "      <td>9800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83999</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A060380</td>\n",
              "      <td>동양에스텍</td>\n",
              "      <td>368634</td>\n",
              "      <td>2785</td>\n",
              "      <td>2925</td>\n",
              "      <td>2770</td>\n",
              "      <td>2865</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1072000 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-793ebca9-ac55-42b2-8e6b-edbf967dae78')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-24d3b629-3793-4cff-8c0d-d577fb7ebd91\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-24d3b629-3793-4cff-8c0d-d577fb7ebd91')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-24d3b629-3793-4cff-8c0d-d577fb7ebd91 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-793ebca9-ac55-42b2-8e6b-edbf967dae78 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-793ebca9-ac55-42b2-8e6b-edbf967dae78');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2Ftr1uK2tEK"
      },
      "outputs": [],
      "source": [
        "train = train.sort_values(by = ['종목코드', '일자'])\n",
        "train = train.reset_index(drop = True)\n",
        "rename_dict = {\n",
        "    train.columns[0] : 'Date',\n",
        "    train.columns[1] : 'item_Code',\n",
        "    train.columns[2] : 'item_Name',\n",
        "    train.columns[3] : 'Volume',\n",
        "    train.columns[4] : 'Open',\n",
        "    train.columns[5] : 'High',\n",
        "    train.columns[6] : 'Low',\n",
        "    train.columns[-1] : 'Close'\n",
        "}\n",
        "train.rename(columns = rename_dict, inplace = True)\n",
        "train['Date'] = train['Date'].astype(int)\n",
        "train['Date_ym'] = train['Date'].astype(str).str[:-2].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDCYTvOrpr-o",
        "outputId": "1d72dcb8-92e4-448b-fdc6-ca5f9028938e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A006580\n",
            "A012600\n",
            "A015540\n",
            "A033340\n",
            "A065560\n",
            "A069110\n",
            "A078590\n",
            "A099520\n",
            "A109070\n",
            "A160600\n",
            "A178780\n",
            "A257370\n",
            "A263540\n"
          ]
        }
      ],
      "source": [
        "for item_Code in train['item_Code'].unique().tolist():\n",
        "  temp = train[train['item_Code'] == item_Code]\n",
        "  sum_of_temp = temp[['Volume', 'Open', 'High', 'Low']].sum()\n",
        "  if sum_of_temp.sum() == 0:\n",
        "    print(item_Code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxQjmUmgvvN_",
        "outputId": "3c0069f5-1966-493e-bc00-06937ebc6d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://finance.naver.com/item/sise_day.nhn?code=006580\n"
          ]
        }
      ],
      "source": [
        "code = '006580'\n",
        "url = f\"http://finance.naver.com/item/sise_day.nhn?code={code}\"\n",
        "print(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "biJZg6W3uudP",
        "outputId": "6296510c-583d-43d5-92b8-87c7bdd45f9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Date item_Code item_Name  Volume  Open  High  Low  Close  Date_ym\n",
              "146328  20210601   A006580      대양제지       0     0     0    0   3260   202106\n",
              "146329  20210602   A006580      대양제지       0     0     0    0   3260   202106\n",
              "146330  20210603   A006580      대양제지       0     0     0    0   3260   202106\n",
              "146331  20210604   A006580      대양제지       0     0     0    0   3260   202106\n",
              "146332  20210607   A006580      대양제지       0     0     0    0   3260   202106"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-5b36964a-3257-4c06-bcbd-e0d027525ab6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>item_Code</th>\n",
              "      <th>item_Name</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Date_ym</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>146328</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146329</th>\n",
              "      <td>20210602</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146330</th>\n",
              "      <td>20210603</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146331</th>\n",
              "      <td>20210604</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146332</th>\n",
              "      <td>20210607</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202106</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b36964a-3257-4c06-bcbd-e0d027525ab6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-036f7809-67af-4216-aa9a-011e0fec6ee5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-036f7809-67af-4216-aa9a-011e0fec6ee5')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-036f7809-67af-4216-aa9a-011e0fec6ee5 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5b36964a-3257-4c06-bcbd-e0d027525ab6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5b36964a-3257-4c06-bcbd-e0d027525ab6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "train[train['item_Code'] == 'A006580'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "VjnjDXkxd8kn",
        "outputId": "64ba97c2-520a-4c69-b36a-586a176ff855"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Date item_Code item_Name   Volume   Open   High    Low  Close  \\\n",
              "0        20210601   A000020      동화약품   114966  14700  14700  14450  14600   \n",
              "1        20210602   A000020      동화약품   109559  14700  14700  14450  14500   \n",
              "2        20210603   A000020      동화약품    96158  14550  14650  14450  14600   \n",
              "3        20210604   A000020      동화약품   133900  14600  14800  14550  14700   \n",
              "4        20210607   A000020      동화약품   511140  14800  15550  14750  15150   \n",
              "...           ...       ...       ...      ...    ...    ...    ...    ...   \n",
              "1071995  20230724   A383800     LX홀딩스   232782   7850   8000   7710   7800   \n",
              "1071996  20230725   A383800     LX홀딩스  3965325   7850   8960   7700   8140   \n",
              "1071997  20230726   A383800     LX홀딩스  2042248   8150   8850   7780   7980   \n",
              "1071998  20230727   A383800     LX홀딩스   538081   8050   8430   7910   8220   \n",
              "1071999  20230728   A383800     LX홀딩스   223590   8280   8300   8100   8230   \n",
              "\n",
              "         Date_ym    Change  \n",
              "0         202106       NaN  \n",
              "1         202106 -0.006849  \n",
              "2         202106  0.006897  \n",
              "3         202106  0.006849  \n",
              "4         202106  0.030612  \n",
              "...          ...       ...  \n",
              "1071995   202307 -0.007634  \n",
              "1071996   202307  0.043590  \n",
              "1071997   202307 -0.019656  \n",
              "1071998   202307  0.030075  \n",
              "1071999   202307  0.001217  \n",
              "\n",
              "[1072000 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-fc3a5cba-0eac-4276-8e70-04fbf4320a65\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>item_Code</th>\n",
              "      <th>item_Name</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Date_ym</th>\n",
              "      <th>Change</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>114966</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>202106</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20210602</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>109559</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14500</td>\n",
              "      <td>202106</td>\n",
              "      <td>-0.006849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20210603</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>96158</td>\n",
              "      <td>14550</td>\n",
              "      <td>14650</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>202106</td>\n",
              "      <td>0.006897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20210604</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>133900</td>\n",
              "      <td>14600</td>\n",
              "      <td>14800</td>\n",
              "      <td>14550</td>\n",
              "      <td>14700</td>\n",
              "      <td>202106</td>\n",
              "      <td>0.006849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20210607</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>511140</td>\n",
              "      <td>14800</td>\n",
              "      <td>15550</td>\n",
              "      <td>14750</td>\n",
              "      <td>15150</td>\n",
              "      <td>202106</td>\n",
              "      <td>0.030612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1071995</th>\n",
              "      <td>20230724</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>232782</td>\n",
              "      <td>7850</td>\n",
              "      <td>8000</td>\n",
              "      <td>7710</td>\n",
              "      <td>7800</td>\n",
              "      <td>202307</td>\n",
              "      <td>-0.007634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1071996</th>\n",
              "      <td>20230725</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>3965325</td>\n",
              "      <td>7850</td>\n",
              "      <td>8960</td>\n",
              "      <td>7700</td>\n",
              "      <td>8140</td>\n",
              "      <td>202307</td>\n",
              "      <td>0.043590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1071997</th>\n",
              "      <td>20230726</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>2042248</td>\n",
              "      <td>8150</td>\n",
              "      <td>8850</td>\n",
              "      <td>7780</td>\n",
              "      <td>7980</td>\n",
              "      <td>202307</td>\n",
              "      <td>-0.019656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1071998</th>\n",
              "      <td>20230727</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>538081</td>\n",
              "      <td>8050</td>\n",
              "      <td>8430</td>\n",
              "      <td>7910</td>\n",
              "      <td>8220</td>\n",
              "      <td>202307</td>\n",
              "      <td>0.030075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1071999</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>223590</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>8100</td>\n",
              "      <td>8230</td>\n",
              "      <td>202307</td>\n",
              "      <td>0.001217</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1072000 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc3a5cba-0eac-4276-8e70-04fbf4320a65')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-d4db8e43-3807-4e8d-8f9c-6440ad3c4fe7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d4db8e43-3807-4e8d-8f9c-6440ad3c4fe7')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-d4db8e43-3807-4e8d-8f9c-6440ad3c4fe7 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fc3a5cba-0eac-4276-8e70-04fbf4320a65 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fc3a5cba-0eac-4276-8e70-04fbf4320a65');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "train['Change'] = train.groupby('item_Code')['Close'].pct_change() # 변동성 구하기\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "x9KMDBzBNIB5",
        "outputId": "6a497ff5-6ef9-4dc7-9d25-e9621130bead"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Date item_Code item_Name  Volume  Open  High  Low  Close  Date_ym  \\\n",
              "146328  20210601   A006580      대양제지       0     0     0    0   3260   202106   \n",
              "146329  20210602   A006580      대양제지       0     0     0    0   3260   202106   \n",
              "146330  20210603   A006580      대양제지       0     0     0    0   3260   202106   \n",
              "146331  20210604   A006580      대양제지       0     0     0    0   3260   202106   \n",
              "146332  20210607   A006580      대양제지       0     0     0    0   3260   202106   \n",
              "...          ...       ...       ...     ...   ...   ...  ...    ...      ...   \n",
              "146859  20230724   A006580      대양제지       0     0     0    0   3260   202307   \n",
              "146860  20230725   A006580      대양제지       0     0     0    0   3260   202307   \n",
              "146861  20230726   A006580      대양제지       0     0     0    0   3260   202307   \n",
              "146862  20230727   A006580      대양제지       0     0     0    0   3260   202307   \n",
              "146863  20230728   A006580      대양제지       0     0     0    0   3260   202307   \n",
              "\n",
              "        Change  \n",
              "146328     NaN  \n",
              "146329     0.0  \n",
              "146330     0.0  \n",
              "146331     0.0  \n",
              "146332     0.0  \n",
              "...        ...  \n",
              "146859     0.0  \n",
              "146860     0.0  \n",
              "146861     0.0  \n",
              "146862     0.0  \n",
              "146863     0.0  \n",
              "\n",
              "[536 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-6171b8dc-6f61-46e4-9434-f40665035350\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>item_Code</th>\n",
              "      <th>item_Name</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Date_ym</th>\n",
              "      <th>Change</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>146328</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202106</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146329</th>\n",
              "      <td>20210602</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202106</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146330</th>\n",
              "      <td>20210603</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202106</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146331</th>\n",
              "      <td>20210604</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202106</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146332</th>\n",
              "      <td>20210607</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202106</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146859</th>\n",
              "      <td>20230724</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202307</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146860</th>\n",
              "      <td>20230725</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202307</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146861</th>\n",
              "      <td>20230726</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202307</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146862</th>\n",
              "      <td>20230727</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202307</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146863</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A006580</td>\n",
              "      <td>대양제지</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3260</td>\n",
              "      <td>202307</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>536 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6171b8dc-6f61-46e4-9434-f40665035350')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-77593e3b-95ba-4838-a182-f0f2bc55297a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-77593e3b-95ba-4838-a182-f0f2bc55297a')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-77593e3b-95ba-4838-a182-f0f2bc55297a button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6171b8dc-6f61-46e4-9434-f40665035350 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6171b8dc-6f61-46e4-9434-f40665035350');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "train[train['item_Code'] == 'A006580']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Jnv3Vq2Xqd_"
      },
      "outputs": [],
      "source": [
        "def read_dataset(dataset_dir, is_display = True, ignore_index = False):\n",
        "  dataset_name = dataset_dir.split('/')[-1]\n",
        "  print('dataset: ', dataset_name)\n",
        "  try:\n",
        "    if ignore_index == True:\n",
        "      save_dataset = pd.read_csv(f'{dataset_dir}', index_col = 0)\n",
        "    else:\n",
        "      save_dataset = pd.read_csv(f'{dataset_dir}')\n",
        "  except:\n",
        "    if ignore_index == True:\n",
        "      save_dataset = pd.read_csv(f'{dataset_dir}', encoding = 'CP949', index_col = 0)\n",
        "    else:\n",
        "      save_dataset = pd.read_csv(f'{dataset_dir}', encoding = 'CP949')\n",
        "  if is_display == True:\n",
        "    display(save_dataset)\n",
        "  return save_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dolgs-n0C4To"
      },
      "source": [
        "**상관 자산**\n",
        "\n",
        "경기종합지수 (2021.06 ~ 2023.05 : 월단위)\n",
        "- 선행종합지수: 가까운 장래의 경기 흐름을 가늠하는 지표로 지수가 전월보다 올라가면 경기 상승, 내려가면 경기 하강을 의미\n",
        "- 동행종합지수: 현재의 경기 상태를 나타내는 지표\n",
        "- 후행종합지수: 경기의 변동을 사후에 확인하는 지표\n",
        "- 순환변동치: 경기지수를 산출하는데 있어 계절, 천재지변, 경제성장에 따른 추세적 요인을 제외한 순수한 경기지수의 변동치\n",
        "- 경제심리지수\n",
        "\n",
        "https://velog.io/@chaliechu117/KOSIS%EA%B5%AD%EA%B0%80%ED%86%B5%EA%B3%84%ED%8F%AC%ED%84%B8-%EA%B2%BD%EA%B8%B0%EC%A2%85%ED%95%A9%EC%A7%80%EC%88%98-%EC%8B%9C%EA%B3%84%EC%97%B4-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D\n",
        "\n",
        "https://sgsg.hankyung.com/article/2022070851821\n",
        "\n",
        "https://kosis.kr/statHtml/statHtml.do?orgId=301&tblId=DT_513Y001&vw_cd=MT_ZTITLE&list_id=J1_301025&scrId=&seqNo=&lang_mode=ko&obj_var_id=&itm_id=&conn_path=MT_ZTITLE&path=%252FstatisticsList%252FstatisticsListIndex.do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dqazmh7fC3b6",
        "outputId": "9b1bc90e-9d1a-4d03-e0ec-212996258a04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset:  경기종합지수_2020100__10차__20230711222648.csv\n"
          ]
        }
      ],
      "source": [
        "composite_index = read_dataset(data_dir + '경기종합지수_2020100__10차__20230711222648.csv', is_display = False, ignore_index = True) # 경기 종합 지수\n",
        "composite_index = composite_index.T\n",
        "composite_index.reset_index(inplace = True)\n",
        "composite_index.rename(columns = {'index':'Date_ym'}, inplace = True)\n",
        "composite_index['Date_ym'] = composite_index['Date_ym'].str.replace(pat = r'[.p)]', repl = r'', regex = True).astype(int)\n",
        "composite_index.columns = composite_index.columns.str.replace('[()〔〕p%2020=100 ]', '')\n",
        "composite_index.columns = ['Date_ym', '선행종합지수', '선행종합지수전월비', '재고순환지표전월차', '경제심리지수전월차',\n",
        "       '기계류내수출하지수선박제외전월비', '건설수주액전월비', '수출입물가비율전월비', '코스피전월비', '장단기금리차전월차',\n",
        "       '선행지수순환변동치', '선행지수순환변동치전월차', '선행지수전년동월비', '전년동월비전월차', '동행종합지수', '동행종합지수전월비',\n",
        "       '광공업생산지수전월비', '서비스업생산지수도소매업제외전월비', '건설기성액전월비', '소매판매액지수전월비',\n",
        "       '내수출하지수전월비', '수입액전월비', '비농림어업취업자수전월비', '동행지수순환변동치', '동행지수순환변동치전월차',\n",
        "       '후행종합지수', '후행종합지수전월비', '생산자제품재고지수전월비', '소비자물가지수변화율서비스전월차', '소비재수입액전월비',\n",
        "       '취업자수전월비', 'CP유통수익률전월차']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqXvznpm3eo_"
      },
      "outputs": [],
      "source": [
        "train = train.merge(composite_index, on = 'Date_ym', how = 'left')\n",
        "train.drop(['Date_ym'], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-fuo02yMr9J"
      },
      "source": [
        "FRED (2021 06.01 ~ 2023 05.30)\n",
        "- 나스닥 지수\n",
        "- 주간 실업수당 청구 건수(ICSA)\n",
        "- 미시건대 소비자심리지수(UMCSENT)\n",
        "- 주택판매지수(HSN1F)\n",
        "- M2통화량(M2)\n",
        "- 하이일드 채권 스프레드\n",
        "\n",
        "https://tariat.tistory.com/1252\n",
        "\n",
        "https://coding-kindergarten.tistory.com/125\n",
        "\n",
        "https://www.facebook.com/groups/pythonkorea/posts/3370820856334385/\n",
        "\n",
        "https://psystat.tistory.com/164\n",
        "\n",
        "https://fred.stlouisfed.org/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "Yt4bJ3RLMrug",
        "outputId": "bcaf81f1-39f1-458a-f88d-b8312bbe0aeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset:  NASDAQCOM.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-8d21e445-9fb6-4494-a2e8-8c42c37b73a2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>item_Code</th>\n",
              "      <th>item_Name</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Change</th>\n",
              "      <th>선행종합지수</th>\n",
              "      <th>...</th>\n",
              "      <th>동행지수순환변동치</th>\n",
              "      <th>동행지수순환변동치전월차</th>\n",
              "      <th>후행종합지수</th>\n",
              "      <th>후행종합지수전월비</th>\n",
              "      <th>생산자제품재고지수전월비</th>\n",
              "      <th>소비자물가지수변화율서비스전월차</th>\n",
              "      <th>소비재수입액전월비</th>\n",
              "      <th>취업자수전월비</th>\n",
              "      <th>CP유통수익률전월차</th>\n",
              "      <th>NASDAQCOM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>114966</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>99.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13736.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20210602</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>109559</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14500</td>\n",
              "      <td>-0.006849</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>99.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13756.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20210603</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>96158</td>\n",
              "      <td>14550</td>\n",
              "      <td>14650</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>0.006897</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>99.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13614.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20210604</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>133900</td>\n",
              "      <td>14600</td>\n",
              "      <td>14800</td>\n",
              "      <td>14550</td>\n",
              "      <td>14700</td>\n",
              "      <td>0.006849</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>99.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13814.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20210607</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>511140</td>\n",
              "      <td>14800</td>\n",
              "      <td>15550</td>\n",
              "      <td>14750</td>\n",
              "      <td>15150</td>\n",
              "      <td>0.030612</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>99.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13881.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987995</th>\n",
              "      <td>20230523</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>150364</td>\n",
              "      <td>8390</td>\n",
              "      <td>8390</td>\n",
              "      <td>8310</td>\n",
              "      <td>8330</td>\n",
              "      <td>-0.003589</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>99.9</td>\n",
              "      <td>0.1</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>12560.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987996</th>\n",
              "      <td>20230524</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>122457</td>\n",
              "      <td>8310</td>\n",
              "      <td>8340</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>-0.003601</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>99.9</td>\n",
              "      <td>0.1</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>12484.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987997</th>\n",
              "      <td>20230525</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>84241</td>\n",
              "      <td>8300</td>\n",
              "      <td>8310</td>\n",
              "      <td>8270</td>\n",
              "      <td>8310</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>99.9</td>\n",
              "      <td>0.1</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>12698.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987998</th>\n",
              "      <td>20230526</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>126681</td>\n",
              "      <td>8300</td>\n",
              "      <td>8310</td>\n",
              "      <td>8270</td>\n",
              "      <td>8280</td>\n",
              "      <td>-0.003610</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>99.9</td>\n",
              "      <td>0.1</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>12975.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987999</th>\n",
              "      <td>20230530</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>70489</td>\n",
              "      <td>8300</td>\n",
              "      <td>8300</td>\n",
              "      <td>8270</td>\n",
              "      <td>8290</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>99.9</td>\n",
              "      <td>0.1</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>13017.43</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>988000 rows × 41 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d21e445-9fb6-4494-a2e8-8c42c37b73a2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-a6d8d746-4efe-4e62-80ad-44bf658b988f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a6d8d746-4efe-4e62-80ad-44bf658b988f')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-a6d8d746-4efe-4e62-80ad-44bf658b988f button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d21e445-9fb6-4494-a2e8-8c42c37b73a2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d21e445-9fb6-4494-a2e8-8c42c37b73a2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "            Date item_Code item_Name  Volume   Open   High    Low  Close  \\\n",
              "0       20210601   A000020      동화약품  114966  14700  14700  14450  14600   \n",
              "1       20210602   A000020      동화약품  109559  14700  14700  14450  14500   \n",
              "2       20210603   A000020      동화약품   96158  14550  14650  14450  14600   \n",
              "3       20210604   A000020      동화약품  133900  14600  14800  14550  14700   \n",
              "4       20210607   A000020      동화약품  511140  14800  15550  14750  15150   \n",
              "...          ...       ...       ...     ...    ...    ...    ...    ...   \n",
              "987995  20230523   A383800     LX홀딩스  150364   8390   8390   8310   8330   \n",
              "987996  20230524   A383800     LX홀딩스  122457   8310   8340   8280   8300   \n",
              "987997  20230525   A383800     LX홀딩스   84241   8300   8310   8270   8310   \n",
              "987998  20230526   A383800     LX홀딩스  126681   8300   8310   8270   8280   \n",
              "987999  20230530   A383800     LX홀딩스   70489   8300   8300   8270   8290   \n",
              "\n",
              "          Change  선행종합지수  ...  동행지수순환변동치  동행지수순환변동치전월차  후행종합지수  후행종합지수전월비  \\\n",
              "0            NaN   106.9  ...       99.2          -0.1   103.4        0.2   \n",
              "1      -0.006849   106.9  ...       99.2          -0.1   103.4        0.2   \n",
              "2       0.006897   106.9  ...       99.2          -0.1   103.4        0.2   \n",
              "3       0.006849   106.9  ...       99.2          -0.1   103.4        0.2   \n",
              "4       0.030612   106.9  ...       99.2          -0.1   103.4        0.2   \n",
              "...          ...     ...  ...        ...           ...     ...        ...   \n",
              "987995 -0.003589   109.7  ...       99.9           0.1   113.7        0.3   \n",
              "987996 -0.003601   109.7  ...       99.9           0.1   113.7        0.3   \n",
              "987997  0.001205   109.7  ...       99.9           0.1   113.7        0.3   \n",
              "987998 -0.003610   109.7  ...       99.9           0.1   113.7        0.3   \n",
              "987999  0.001208   109.7  ...       99.9           0.1   113.7        0.3   \n",
              "\n",
              "        생산자제품재고지수전월비  소비자물가지수변화율서비스전월차  소비재수입액전월비  취업자수전월비  CP유통수익률전월차  \\\n",
              "0               -0.5               0.1        0.0      0.2         0.0   \n",
              "1               -0.5               0.1        0.0      0.2         0.0   \n",
              "2               -0.5               0.1        0.0      0.2         0.0   \n",
              "3               -0.5               0.1        0.0      0.2         0.0   \n",
              "4               -0.5               0.1        0.0      0.2         0.0   \n",
              "...              ...               ...        ...      ...         ...   \n",
              "987995           1.8               0.0       -2.1      0.2        -0.1   \n",
              "987996           1.8               0.0       -2.1      0.2        -0.1   \n",
              "987997           1.8               0.0       -2.1      0.2        -0.1   \n",
              "987998           1.8               0.0       -2.1      0.2        -0.1   \n",
              "987999           1.8               0.0       -2.1      0.2        -0.1   \n",
              "\n",
              "        NASDAQCOM  \n",
              "0        13736.48  \n",
              "1        13756.33  \n",
              "2        13614.51  \n",
              "3        13814.49  \n",
              "4        13881.72  \n",
              "...           ...  \n",
              "987995   12560.25  \n",
              "987996   12484.16  \n",
              "987997   12698.09  \n",
              "987998   12975.69  \n",
              "987999   13017.43  \n",
              "\n",
              "[988000 rows x 41 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nasdaq_index = read_dataset(data_dir + 'NASDAQCOM.csv', is_display = False, ignore_index = False) # 나스닥 종합 지수\n",
        "nasdaq_index['Date'] = nasdaq_index['DATE'].str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "nasdaq_index.drop(['DATE'], axis = 1, inplace = True)\n",
        "train = train.merge(nasdaq_index, on = 'Date', how = 'left')\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNBYJ4kObUHI"
      },
      "outputs": [],
      "source": [
        "# icsa = read_dataset('/content/ICSA.csv', is_display = False, ignore_index = False) # 주간 실업수당 청구 건수 (ICSA)\n",
        "# icsa['Date'] = icsa['DATE'].str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "# icsa.drop(['DATE'], axis = 1, inplace = True)\n",
        "# train = train.merge(icsa, on = 'Date', how = 'left')\n",
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-4ZsdkLbWW2"
      },
      "outputs": [],
      "source": [
        "# umcsent = read_dataset('/content/UMCSENT.csv', is_display = False, ignore_index = False) # 미시건대 소비자심리지수(UMCSENT)\n",
        "# umcsent['Date'] = umcsent['DATE'].str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "# umcsent.drop(['DATE'], axis = 1, inplace = True)\n",
        "# train = train.merge(umcsent, on = 'Date', how = 'left')\n",
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yu65R88jbWxv"
      },
      "outputs": [],
      "source": [
        "# hsn1f = read_dataset('/content/HSN1F.csv', is_display = False, ignore_index = False) # 주택판매지수(HSN1F)\n",
        "# hsn1f['Date'] = hsn1f['DATE'].str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "# hsn1f.drop(['DATE'], axis = 1, inplace = True)\n",
        "# train = train.merge(hsn1f, on = 'Date', how = 'left')\n",
        "# train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "k-Ex2kBPbXRs",
        "outputId": "dec9a788-07ba-4214-8baa-602d679dc9de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset:  BAMLH0A0HYM2EY.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-44111f25-8aa6-45f3-9217-1a6cb84b62e9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>item_Code</th>\n",
              "      <th>item_Name</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Change</th>\n",
              "      <th>선행종합지수</th>\n",
              "      <th>...</th>\n",
              "      <th>동행지수순환변동치전월차</th>\n",
              "      <th>후행종합지수</th>\n",
              "      <th>후행종합지수전월비</th>\n",
              "      <th>생산자제품재고지수전월비</th>\n",
              "      <th>소비자물가지수변화율서비스전월차</th>\n",
              "      <th>소비재수입액전월비</th>\n",
              "      <th>취업자수전월비</th>\n",
              "      <th>CP유통수익률전월차</th>\n",
              "      <th>NASDAQCOM</th>\n",
              "      <th>BAMLH0A0HYM2EY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>114966</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13736.48</td>\n",
              "      <td>4.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20210602</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>109559</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14500</td>\n",
              "      <td>-0.006849</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13756.33</td>\n",
              "      <td>4.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20210603</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>96158</td>\n",
              "      <td>14550</td>\n",
              "      <td>14650</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>0.006897</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13614.51</td>\n",
              "      <td>4.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20210604</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>133900</td>\n",
              "      <td>14600</td>\n",
              "      <td>14800</td>\n",
              "      <td>14550</td>\n",
              "      <td>14700</td>\n",
              "      <td>0.006849</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13814.49</td>\n",
              "      <td>4.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20210607</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>511140</td>\n",
              "      <td>14800</td>\n",
              "      <td>15550</td>\n",
              "      <td>14750</td>\n",
              "      <td>15150</td>\n",
              "      <td>0.030612</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13881.72</td>\n",
              "      <td>4.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987995</th>\n",
              "      <td>20230523</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>150364</td>\n",
              "      <td>8390</td>\n",
              "      <td>8390</td>\n",
              "      <td>8310</td>\n",
              "      <td>8330</td>\n",
              "      <td>-0.003589</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>12560.25</td>\n",
              "      <td>8.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987996</th>\n",
              "      <td>20230524</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>122457</td>\n",
              "      <td>8310</td>\n",
              "      <td>8340</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>-0.003601</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>12484.16</td>\n",
              "      <td>8.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987997</th>\n",
              "      <td>20230525</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>84241</td>\n",
              "      <td>8300</td>\n",
              "      <td>8310</td>\n",
              "      <td>8270</td>\n",
              "      <td>8310</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>12698.09</td>\n",
              "      <td>8.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987998</th>\n",
              "      <td>20230526</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>126681</td>\n",
              "      <td>8300</td>\n",
              "      <td>8310</td>\n",
              "      <td>8270</td>\n",
              "      <td>8280</td>\n",
              "      <td>-0.003610</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>12975.69</td>\n",
              "      <td>8.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987999</th>\n",
              "      <td>20230530</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>70489</td>\n",
              "      <td>8300</td>\n",
              "      <td>8300</td>\n",
              "      <td>8270</td>\n",
              "      <td>8290</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>0.1</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>13017.43</td>\n",
              "      <td>8.57</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>988000 rows × 42 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44111f25-8aa6-45f3-9217-1a6cb84b62e9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-01116b93-2a8b-40e2-becd-f1e993ab2809\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-01116b93-2a8b-40e2-becd-f1e993ab2809')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-01116b93-2a8b-40e2-becd-f1e993ab2809 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-44111f25-8aa6-45f3-9217-1a6cb84b62e9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-44111f25-8aa6-45f3-9217-1a6cb84b62e9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "            Date item_Code item_Name  Volume   Open   High    Low  Close  \\\n",
              "0       20210601   A000020      동화약품  114966  14700  14700  14450  14600   \n",
              "1       20210602   A000020      동화약품  109559  14700  14700  14450  14500   \n",
              "2       20210603   A000020      동화약품   96158  14550  14650  14450  14600   \n",
              "3       20210604   A000020      동화약품  133900  14600  14800  14550  14700   \n",
              "4       20210607   A000020      동화약품  511140  14800  15550  14750  15150   \n",
              "...          ...       ...       ...     ...    ...    ...    ...    ...   \n",
              "987995  20230523   A383800     LX홀딩스  150364   8390   8390   8310   8330   \n",
              "987996  20230524   A383800     LX홀딩스  122457   8310   8340   8280   8300   \n",
              "987997  20230525   A383800     LX홀딩스   84241   8300   8310   8270   8310   \n",
              "987998  20230526   A383800     LX홀딩스  126681   8300   8310   8270   8280   \n",
              "987999  20230530   A383800     LX홀딩스   70489   8300   8300   8270   8290   \n",
              "\n",
              "          Change  선행종합지수  ...  동행지수순환변동치전월차  후행종합지수  후행종합지수전월비  생산자제품재고지수전월비  \\\n",
              "0            NaN   106.9  ...          -0.1   103.4        0.2          -0.5   \n",
              "1      -0.006849   106.9  ...          -0.1   103.4        0.2          -0.5   \n",
              "2       0.006897   106.9  ...          -0.1   103.4        0.2          -0.5   \n",
              "3       0.006849   106.9  ...          -0.1   103.4        0.2          -0.5   \n",
              "4       0.030612   106.9  ...          -0.1   103.4        0.2          -0.5   \n",
              "...          ...     ...  ...           ...     ...        ...           ...   \n",
              "987995 -0.003589   109.7  ...           0.1   113.7        0.3           1.8   \n",
              "987996 -0.003601   109.7  ...           0.1   113.7        0.3           1.8   \n",
              "987997  0.001205   109.7  ...           0.1   113.7        0.3           1.8   \n",
              "987998 -0.003610   109.7  ...           0.1   113.7        0.3           1.8   \n",
              "987999  0.001208   109.7  ...           0.1   113.7        0.3           1.8   \n",
              "\n",
              "        소비자물가지수변화율서비스전월차  소비재수입액전월비  취업자수전월비  CP유통수익률전월차  NASDAQCOM  \\\n",
              "0                    0.1        0.0      0.2         0.0   13736.48   \n",
              "1                    0.1        0.0      0.2         0.0   13756.33   \n",
              "2                    0.1        0.0      0.2         0.0   13614.51   \n",
              "3                    0.1        0.0      0.2         0.0   13814.49   \n",
              "4                    0.1        0.0      0.2         0.0   13881.72   \n",
              "...                  ...        ...      ...         ...        ...   \n",
              "987995               0.0       -2.1      0.2        -0.1   12560.25   \n",
              "987996               0.0       -2.1      0.2        -0.1   12484.16   \n",
              "987997               0.0       -2.1      0.2        -0.1   12698.09   \n",
              "987998               0.0       -2.1      0.2        -0.1   12975.69   \n",
              "987999               0.0       -2.1      0.2        -0.1   13017.43   \n",
              "\n",
              "        BAMLH0A0HYM2EY  \n",
              "0                 4.29  \n",
              "1                 4.26  \n",
              "2                 4.26  \n",
              "3                 4.21  \n",
              "4                 4.20  \n",
              "...                ...  \n",
              "987995            8.52  \n",
              "987996            8.66  \n",
              "987997            8.68  \n",
              "987998            8.67  \n",
              "987999            8.57  \n",
              "\n",
              "[988000 rows x 42 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m2 = read_dataset(data_dir + 'BAMLH0A0HYM2EY.csv', is_display = False, ignore_index = False) # M2통화량(M2)\n",
        "m2['Date'] = m2['DATE'].str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "m2.drop(['DATE'], axis = 1, inplace = True)\n",
        "train = train.merge(m2, on = 'Date', how = 'left')\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "Q_noxHuHbY7b",
        "outputId": "27bfba79-d31e-404b-9a89-c2171d309385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset:  WM2NS.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-9879b545-be73-45cb-9302-72e69f060775\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>item_Code</th>\n",
              "      <th>item_Name</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Change</th>\n",
              "      <th>선행종합지수</th>\n",
              "      <th>...</th>\n",
              "      <th>후행종합지수</th>\n",
              "      <th>후행종합지수전월비</th>\n",
              "      <th>생산자제품재고지수전월비</th>\n",
              "      <th>소비자물가지수변화율서비스전월차</th>\n",
              "      <th>소비재수입액전월비</th>\n",
              "      <th>취업자수전월비</th>\n",
              "      <th>CP유통수익률전월차</th>\n",
              "      <th>NASDAQCOM</th>\n",
              "      <th>BAMLH0A0HYM2EY</th>\n",
              "      <th>WM2NS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>114966</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13736.48</td>\n",
              "      <td>4.29</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20210602</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>109559</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14500</td>\n",
              "      <td>-0.006849</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13756.33</td>\n",
              "      <td>4.26</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20210603</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>96158</td>\n",
              "      <td>14550</td>\n",
              "      <td>14650</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>0.006897</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13614.51</td>\n",
              "      <td>4.26</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20210604</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>133900</td>\n",
              "      <td>14600</td>\n",
              "      <td>14800</td>\n",
              "      <td>14550</td>\n",
              "      <td>14700</td>\n",
              "      <td>0.006849</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13814.49</td>\n",
              "      <td>4.21</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20210607</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>511140</td>\n",
              "      <td>14800</td>\n",
              "      <td>15550</td>\n",
              "      <td>14750</td>\n",
              "      <td>15150</td>\n",
              "      <td>0.030612</td>\n",
              "      <td>106.9</td>\n",
              "      <td>...</td>\n",
              "      <td>103.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13881.72</td>\n",
              "      <td>4.20</td>\n",
              "      <td>20418.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987995</th>\n",
              "      <td>20230523</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>150364</td>\n",
              "      <td>8390</td>\n",
              "      <td>8390</td>\n",
              "      <td>8310</td>\n",
              "      <td>8330</td>\n",
              "      <td>-0.003589</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>12560.25</td>\n",
              "      <td>8.52</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987996</th>\n",
              "      <td>20230524</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>122457</td>\n",
              "      <td>8310</td>\n",
              "      <td>8340</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>-0.003601</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>12484.16</td>\n",
              "      <td>8.66</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987997</th>\n",
              "      <td>20230525</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>84241</td>\n",
              "      <td>8300</td>\n",
              "      <td>8310</td>\n",
              "      <td>8270</td>\n",
              "      <td>8310</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>12698.09</td>\n",
              "      <td>8.68</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987998</th>\n",
              "      <td>20230526</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>126681</td>\n",
              "      <td>8300</td>\n",
              "      <td>8310</td>\n",
              "      <td>8270</td>\n",
              "      <td>8280</td>\n",
              "      <td>-0.003610</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>12975.69</td>\n",
              "      <td>8.67</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987999</th>\n",
              "      <td>20230530</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>70489</td>\n",
              "      <td>8300</td>\n",
              "      <td>8300</td>\n",
              "      <td>8270</td>\n",
              "      <td>8290</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>109.7</td>\n",
              "      <td>...</td>\n",
              "      <td>113.7</td>\n",
              "      <td>0.3</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>13017.43</td>\n",
              "      <td>8.57</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>988000 rows × 43 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9879b545-be73-45cb-9302-72e69f060775')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-6c91b2fa-6eef-4b3c-b457-25f554e56506\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6c91b2fa-6eef-4b3c-b457-25f554e56506')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-6c91b2fa-6eef-4b3c-b457-25f554e56506 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9879b545-be73-45cb-9302-72e69f060775 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9879b545-be73-45cb-9302-72e69f060775');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "            Date item_Code item_Name  Volume   Open   High    Low  Close  \\\n",
              "0       20210601   A000020      동화약품  114966  14700  14700  14450  14600   \n",
              "1       20210602   A000020      동화약품  109559  14700  14700  14450  14500   \n",
              "2       20210603   A000020      동화약품   96158  14550  14650  14450  14600   \n",
              "3       20210604   A000020      동화약품  133900  14600  14800  14550  14700   \n",
              "4       20210607   A000020      동화약품  511140  14800  15550  14750  15150   \n",
              "...          ...       ...       ...     ...    ...    ...    ...    ...   \n",
              "987995  20230523   A383800     LX홀딩스  150364   8390   8390   8310   8330   \n",
              "987996  20230524   A383800     LX홀딩스  122457   8310   8340   8280   8300   \n",
              "987997  20230525   A383800     LX홀딩스   84241   8300   8310   8270   8310   \n",
              "987998  20230526   A383800     LX홀딩스  126681   8300   8310   8270   8280   \n",
              "987999  20230530   A383800     LX홀딩스   70489   8300   8300   8270   8290   \n",
              "\n",
              "          Change  선행종합지수  ...  후행종합지수  후행종합지수전월비  생산자제품재고지수전월비  \\\n",
              "0            NaN   106.9  ...   103.4        0.2          -0.5   \n",
              "1      -0.006849   106.9  ...   103.4        0.2          -0.5   \n",
              "2       0.006897   106.9  ...   103.4        0.2          -0.5   \n",
              "3       0.006849   106.9  ...   103.4        0.2          -0.5   \n",
              "4       0.030612   106.9  ...   103.4        0.2          -0.5   \n",
              "...          ...     ...  ...     ...        ...           ...   \n",
              "987995 -0.003589   109.7  ...   113.7        0.3           1.8   \n",
              "987996 -0.003601   109.7  ...   113.7        0.3           1.8   \n",
              "987997  0.001205   109.7  ...   113.7        0.3           1.8   \n",
              "987998 -0.003610   109.7  ...   113.7        0.3           1.8   \n",
              "987999  0.001208   109.7  ...   113.7        0.3           1.8   \n",
              "\n",
              "        소비자물가지수변화율서비스전월차  소비재수입액전월비  취업자수전월비  CP유통수익률전월차  NASDAQCOM  \\\n",
              "0                    0.1        0.0      0.2         0.0   13736.48   \n",
              "1                    0.1        0.0      0.2         0.0   13756.33   \n",
              "2                    0.1        0.0      0.2         0.0   13614.51   \n",
              "3                    0.1        0.0      0.2         0.0   13814.49   \n",
              "4                    0.1        0.0      0.2         0.0   13881.72   \n",
              "...                  ...        ...      ...         ...        ...   \n",
              "987995               0.0       -2.1      0.2        -0.1   12560.25   \n",
              "987996               0.0       -2.1      0.2        -0.1   12484.16   \n",
              "987997               0.0       -2.1      0.2        -0.1   12698.09   \n",
              "987998               0.0       -2.1      0.2        -0.1   12975.69   \n",
              "987999               0.0       -2.1      0.2        -0.1   13017.43   \n",
              "\n",
              "        BAMLH0A0HYM2EY    WM2NS  \n",
              "0                 4.29      NaN  \n",
              "1                 4.26      NaN  \n",
              "2                 4.26      NaN  \n",
              "3                 4.21      NaN  \n",
              "4                 4.20  20418.6  \n",
              "...                ...      ...  \n",
              "987995            8.52      NaN  \n",
              "987996            8.66      NaN  \n",
              "987997            8.68      NaN  \n",
              "987998            8.67      NaN  \n",
              "987999            8.57      NaN  \n",
              "\n",
              "[988000 rows x 43 columns]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "highyeild = read_dataset(data_dir + 'WM2NS.csv', is_display = False, ignore_index = False) # 하이일드 채권 스프레드\n",
        "highyeild['Date'] = highyeild['DATE'].str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "highyeild.drop(['DATE'], axis = 1, inplace = True)\n",
        "train = train.merge(highyeild, on = 'Date', how = 'left')\n",
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIC5e3IARvEt"
      },
      "source": [
        "환율 데이터 (2021 06.01 ~ 2023 05.30)\n",
        "\n",
        "- USD/KRW\n",
        "- USD/EUR\n",
        "- USD/JPY\n",
        "- USD/RUB\n",
        "- KOSPI\n",
        "- KOSDAQ\n",
        "- DJI\n",
        "- IXIC\n",
        "- S&P500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_qFxIATct75"
      },
      "outputs": [],
      "source": [
        "usd_krw = fdr.DataReader('USD/KRW', start = '2021-06-01', end = '2023-07-28') # USD/KRW\n",
        "usd_krw.drop(['Open', 'High', 'Low', 'Adj Close', 'Volume'], axis = 1, inplace = True)\n",
        "usd_krw = usd_krw.add_suffix('_usd_krw')\n",
        "usd_krw = usd_krw.reset_index()\n",
        "usd_krw['Date'] = usd_krw['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "\n",
        "usd_eur = fdr.DataReader('USD/EUR', start = '2021-06-01', end = '2023-07-28') # USD/EUR\n",
        "usd_eur.drop(['Open', 'High', 'Low', 'Adj Close', 'Volume'], axis = 1, inplace = True)\n",
        "usd_eur = usd_eur.add_suffix('_usd_eur')\n",
        "usd_eur = usd_eur.reset_index()\n",
        "usd_eur['Date'] = usd_eur['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "\n",
        "usd_jpy = fdr.DataReader('USD/JPY', start = '2021-06-01', end = '2023-07-28') # USD/JPY\n",
        "usd_jpy.drop(['Open', 'High', 'Low', 'Adj Close', 'Volume'], axis = 1, inplace = True)\n",
        "usd_jpy = usd_jpy.add_suffix('_usd_jpy')\n",
        "usd_jpy = usd_jpy.reset_index()\n",
        "usd_jpy['Date'] = usd_jpy['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "\n",
        "usd_rub = fdr.DataReader('USD/RUB', start = '2021-06-01', end = '2023-07-28') # USD/RUB\n",
        "usd_rub.drop(['Open', 'High', 'Low', 'Adj Close', 'Volume'], axis = 1, inplace = True)\n",
        "usd_rub = usd_rub.add_suffix('_usd_rub')\n",
        "usd_rub = usd_rub.reset_index()\n",
        "usd_rub['Date'] = usd_rub['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "\n",
        "kospi = fdr.DataReader('KS11', start = '2021-06-01', end = '2023-07-28') # kospi\n",
        "kospi.drop(['Open', 'High', 'Low', 'Adj Close'], axis = 1, inplace = True)\n",
        "kospi = kospi.add_suffix('_kospi')\n",
        "kospi = kospi.reset_index()\n",
        "kospi['Date'] = kospi['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "\n",
        "kosdaq = fdr.DataReader('KQ11', start = '2021-06-01', end = '2023-07-28') # kosdaq\n",
        "kosdaq.drop(['Open', 'High', 'Low', 'Adj Close'], axis = 1, inplace = True)\n",
        "kosdaq = kosdaq.add_suffix('_kosdaq')\n",
        "kosdaq = kosdaq.reset_index()\n",
        "kosdaq['Date'] = kosdaq['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "\n",
        "dji = fdr.DataReader('DJI', start = '2021-06-01', end = '2023-07-29') # dji\n",
        "dji.drop(['Open', 'High', 'Low', 'Adj Close'], axis = 1, inplace = True)\n",
        "dji = dji.add_suffix('_dji')\n",
        "dji = dji.reset_index()\n",
        "dji['Date'] = dji['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "\n",
        "ixic = fdr.DataReader('IXIC', start = '2021-06-01', end = '2023-07-29') # ixic\n",
        "ixic.drop(['Open', 'High', 'Low', 'Adj Close'], axis = 1, inplace = True)\n",
        "ixic = ixic.add_suffix('_ixic')\n",
        "ixic = ixic.reset_index()\n",
        "ixic['Date'] = ixic['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "\n",
        "sp500 = fdr.DataReader('S&P500', start = '2021-06-01', end = '2023-07-29') # s&p500\n",
        "sp500.drop(['Open', 'High', 'Low', 'Adj Close'], axis = 1, inplace = True)\n",
        "sp500 = sp500.add_suffix('_sp500')\n",
        "sp500 = sp500.reset_index()\n",
        "sp500['Date'] = sp500['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiWAzbGNSoPt"
      },
      "source": [
        "- BTC/KRW\n",
        "- GOLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyy2WfGtSoGp"
      },
      "outputs": [],
      "source": [
        "btc_krw = fdr.DataReader('BTC/KRW', start = '2021-06-01', end = '2023-07-28') # BTC/KRW\n",
        "btc_krw.drop(['Open', 'High', 'Low', 'Adj Close'], axis = 1, inplace = True)\n",
        "btc_krw = btc_krw.add_suffix('_btc_krw')\n",
        "btc_krw = btc_krw.reset_index()\n",
        "btc_krw['Date'] = btc_krw['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEBxfKXDoYle",
        "outputId": "eb26533e-efbf-40ca-917a-f1ce2f0f7ac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "yf.pdr_override()\n",
        "gold = pdr.get_data_yahoo('GC=F', start = '2021-06-01', end = '2023-07-29') # GOLD\n",
        "gold.drop(['Open', 'High', 'Low', 'Adj Close'], axis = 1, inplace = True)\n",
        "gold = gold.add_suffix('_gold')\n",
        "gold = gold.reset_index()\n",
        "gold['Date'] = gold['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN1rciLcwhgW",
        "outputId": "fedad4a4-9d07-40cf-b6cb-d7bba887af98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "irx = pdr.get_data_yahoo('^IRX', start = '2021-06-01', end = '2023-07-29') # 미국 국채 금리 13주 국채\n",
        "irx.drop(['Open', 'High', 'Low', 'Adj Close', 'Volume'], axis = 1, inplace = True)\n",
        "irx = irx.add_suffix('_irx')\n",
        "irx = irx.reset_index()\n",
        "irx['Date'] = irx['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "\n",
        "fvx = pdr.get_data_yahoo('^FVX', start = '2021-06-01', end = '2023-07-29') # 미국 국채 금리 국채 수익률 5년\n",
        "fvx.drop(['Open', 'High', 'Low', 'Adj Close', 'Volume'], axis = 1, inplace = True)\n",
        "fvx = fvx.add_suffix('_fvx')\n",
        "fvx = fvx.reset_index()\n",
        "fvx['Date'] = fvx['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "\n",
        "tnx = pdr.get_data_yahoo('^TNX', start = '2021-06-01', end = '2023-07-29') # 미국 국채 금리 국채 수익률 10년\n",
        "tnx.drop(['Open', 'High', 'Low', 'Adj Close', 'Volume'], axis = 1, inplace = True)\n",
        "tnx = tnx.add_suffix('_tnx')\n",
        "tnx = tnx.reset_index()\n",
        "tnx['Date'] = tnx['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "\n",
        "us_bond = pdr.get_data_yahoo('IEF', start = '2021-06-01', end = '2023-07-29') # 미국 채권 7~10년물\n",
        "us_bond.drop(['Open', 'High', 'Low'], axis = 1, inplace = True)\n",
        "us_bond = us_bond.add_suffix('_us_bond')\n",
        "us_bond = us_bond.reset_index()\n",
        "us_bond['Date'] = us_bond['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)\n",
        "\n",
        "kr_bond = pdr.get_data_yahoo('148070.KS', start = '2021-06-01', end = '2023-07-29') # 한국 채권 7~10년물\n",
        "kr_bond.drop(['Open', 'High', 'Low'], axis = 1, inplace = True)\n",
        "kr_bond = kr_bond.add_suffix('_kr_bond')\n",
        "kr_bond = kr_bond.reset_index()\n",
        "kr_bond['Date'] = kr_bond['Date'].astype(str).str.replace(pat = r'[-]', repl = r'', regex = True).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NC2DtGYxaOq"
      },
      "outputs": [],
      "source": [
        "merge_datasets = [usd_krw, usd_eur, usd_jpy, usd_rub, kospi, kosdaq, dji, ixic, sp500, btc_krw, gold, irx, fvx, tnx, us_bond, kr_bond]\n",
        "for merge_dataset in merge_datasets:\n",
        "  train = train.merge(merge_dataset, on = 'Date', how = 'left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "w0cHTaC3Nuk4",
        "outputId": "832402ba-97ab-4e52-c9b2-42283703e32f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Date item_Code item_Name  Volume   Open   High    Low  Close  \\\n",
              "0        20210601   A000020      동화약품  114966  14700  14700  14450  14600   \n",
              "1        20210602   A000020      동화약품  109559  14700  14700  14450  14500   \n",
              "2        20210603   A000020      동화약품   96158  14550  14650  14450  14600   \n",
              "3        20210604   A000020      동화약품  133900  14600  14800  14550  14700   \n",
              "4        20210607   A000020      동화약품  511140  14800  15550  14750  15150   \n",
              "...           ...       ...       ...     ...    ...    ...    ...    ...   \n",
              "1085995  20230728   A383800     LX홀딩스  223590   8280   8300   8100   8230   \n",
              "1085996  20230728   A383800     LX홀딩스  223590   8280   8300   8100   8230   \n",
              "1085997  20230728   A383800     LX홀딩스  223590   8280   8300   8100   8230   \n",
              "1085998  20230728   A383800     LX홀딩스  223590   8280   8300   8100   8230   \n",
              "1085999  20230728   A383800     LX홀딩스  223590   8280   8300   8100   8230   \n",
              "\n",
              "         Date_ym  Close_usd_krw  ...  Volume_gold  Close_irx  Close_fvx  \\\n",
              "0         202106    1106.979980  ...        640.0      0.010      0.809   \n",
              "1         202106    1107.209961  ...       1267.0      0.015      0.796   \n",
              "2         202106    1109.880005  ...       1021.0      0.015      0.845   \n",
              "3         202106    1115.439941  ...        367.0      0.015      0.784   \n",
              "4         202106    1109.680054  ...         87.0      0.018      0.793   \n",
              "...          ...            ...  ...          ...        ...        ...   \n",
              "1085995   202307    1283.880005  ...     194253.0      5.255      4.195   \n",
              "1085996   202307    1272.810059  ...     194253.0      5.255      4.195   \n",
              "1085997   202307    1272.810059  ...     194253.0      5.255      4.195   \n",
              "1085998   202307    1272.810059  ...     194253.0      5.255      4.195   \n",
              "1085999   202307    1272.810059  ...     194253.0      5.255      4.195   \n",
              "\n",
              "         Close_tnx  Close_us_bond  Adj Close_us_bond  Volume_us_bond  \\\n",
              "0            1.615     114.300003         110.153992       9498100.0   \n",
              "1            1.591     114.430000         110.279266       4269900.0   \n",
              "2            1.625     114.139999         109.999779       3591200.0   \n",
              "3            1.560     114.769997         110.606934       6326000.0   \n",
              "4            1.569     114.680000         110.520195       3068500.0   \n",
              "...            ...            ...                ...             ...   \n",
              "1085995      3.969      95.620003          95.620003       5794100.0   \n",
              "1085996      3.969      95.620003          95.620003       5794100.0   \n",
              "1085997      3.969      95.620003          95.620003       5794100.0   \n",
              "1085998      3.969      95.620003          95.620003       5794100.0   \n",
              "1085999      3.969      95.620003          95.620003       5794100.0   \n",
              "\n",
              "         Close_kr_bond  Adj Close_kr_bond  Volume_kr_bond  \n",
              "0             119330.0      118022.039062         13680.0  \n",
              "1             119110.0      117804.453125         10724.0  \n",
              "2             119240.0      117933.023438         48541.0  \n",
              "3             119440.0      118130.835938         36730.0  \n",
              "4             119655.0      118343.476562          4780.0  \n",
              "...                ...                ...             ...  \n",
              "1085995       109725.0      109725.000000         10557.0  \n",
              "1085996       109725.0      109725.000000         10557.0  \n",
              "1085997       109725.0      109725.000000         10557.0  \n",
              "1085998       109725.0      109725.000000         10557.0  \n",
              "1085999       109725.0      109725.000000         10557.0  \n",
              "\n",
              "[1086000 rows x 36 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-906f5f4d-4618-469c-be8d-2b399d55fc65\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>item_Code</th>\n",
              "      <th>item_Name</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Date_ym</th>\n",
              "      <th>Close_usd_krw</th>\n",
              "      <th>...</th>\n",
              "      <th>Volume_gold</th>\n",
              "      <th>Close_irx</th>\n",
              "      <th>Close_fvx</th>\n",
              "      <th>Close_tnx</th>\n",
              "      <th>Close_us_bond</th>\n",
              "      <th>Adj Close_us_bond</th>\n",
              "      <th>Volume_us_bond</th>\n",
              "      <th>Close_kr_bond</th>\n",
              "      <th>Adj Close_kr_bond</th>\n",
              "      <th>Volume_kr_bond</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>114966</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>202106</td>\n",
              "      <td>1106.979980</td>\n",
              "      <td>...</td>\n",
              "      <td>640.0</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.809</td>\n",
              "      <td>1.615</td>\n",
              "      <td>114.300003</td>\n",
              "      <td>110.153992</td>\n",
              "      <td>9498100.0</td>\n",
              "      <td>119330.0</td>\n",
              "      <td>118022.039062</td>\n",
              "      <td>13680.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20210602</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>109559</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14500</td>\n",
              "      <td>202106</td>\n",
              "      <td>1107.209961</td>\n",
              "      <td>...</td>\n",
              "      <td>1267.0</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.796</td>\n",
              "      <td>1.591</td>\n",
              "      <td>114.430000</td>\n",
              "      <td>110.279266</td>\n",
              "      <td>4269900.0</td>\n",
              "      <td>119110.0</td>\n",
              "      <td>117804.453125</td>\n",
              "      <td>10724.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20210603</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>96158</td>\n",
              "      <td>14550</td>\n",
              "      <td>14650</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>202106</td>\n",
              "      <td>1109.880005</td>\n",
              "      <td>...</td>\n",
              "      <td>1021.0</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.845</td>\n",
              "      <td>1.625</td>\n",
              "      <td>114.139999</td>\n",
              "      <td>109.999779</td>\n",
              "      <td>3591200.0</td>\n",
              "      <td>119240.0</td>\n",
              "      <td>117933.023438</td>\n",
              "      <td>48541.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20210604</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>133900</td>\n",
              "      <td>14600</td>\n",
              "      <td>14800</td>\n",
              "      <td>14550</td>\n",
              "      <td>14700</td>\n",
              "      <td>202106</td>\n",
              "      <td>1115.439941</td>\n",
              "      <td>...</td>\n",
              "      <td>367.0</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.784</td>\n",
              "      <td>1.560</td>\n",
              "      <td>114.769997</td>\n",
              "      <td>110.606934</td>\n",
              "      <td>6326000.0</td>\n",
              "      <td>119440.0</td>\n",
              "      <td>118130.835938</td>\n",
              "      <td>36730.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20210607</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>511140</td>\n",
              "      <td>14800</td>\n",
              "      <td>15550</td>\n",
              "      <td>14750</td>\n",
              "      <td>15150</td>\n",
              "      <td>202106</td>\n",
              "      <td>1109.680054</td>\n",
              "      <td>...</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.793</td>\n",
              "      <td>1.569</td>\n",
              "      <td>114.680000</td>\n",
              "      <td>110.520195</td>\n",
              "      <td>3068500.0</td>\n",
              "      <td>119655.0</td>\n",
              "      <td>118343.476562</td>\n",
              "      <td>4780.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085995</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>223590</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>8100</td>\n",
              "      <td>8230</td>\n",
              "      <td>202307</td>\n",
              "      <td>1283.880005</td>\n",
              "      <td>...</td>\n",
              "      <td>194253.0</td>\n",
              "      <td>5.255</td>\n",
              "      <td>4.195</td>\n",
              "      <td>3.969</td>\n",
              "      <td>95.620003</td>\n",
              "      <td>95.620003</td>\n",
              "      <td>5794100.0</td>\n",
              "      <td>109725.0</td>\n",
              "      <td>109725.000000</td>\n",
              "      <td>10557.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085996</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>223590</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>8100</td>\n",
              "      <td>8230</td>\n",
              "      <td>202307</td>\n",
              "      <td>1272.810059</td>\n",
              "      <td>...</td>\n",
              "      <td>194253.0</td>\n",
              "      <td>5.255</td>\n",
              "      <td>4.195</td>\n",
              "      <td>3.969</td>\n",
              "      <td>95.620003</td>\n",
              "      <td>95.620003</td>\n",
              "      <td>5794100.0</td>\n",
              "      <td>109725.0</td>\n",
              "      <td>109725.000000</td>\n",
              "      <td>10557.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085997</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>223590</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>8100</td>\n",
              "      <td>8230</td>\n",
              "      <td>202307</td>\n",
              "      <td>1272.810059</td>\n",
              "      <td>...</td>\n",
              "      <td>194253.0</td>\n",
              "      <td>5.255</td>\n",
              "      <td>4.195</td>\n",
              "      <td>3.969</td>\n",
              "      <td>95.620003</td>\n",
              "      <td>95.620003</td>\n",
              "      <td>5794100.0</td>\n",
              "      <td>109725.0</td>\n",
              "      <td>109725.000000</td>\n",
              "      <td>10557.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085998</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>223590</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>8100</td>\n",
              "      <td>8230</td>\n",
              "      <td>202307</td>\n",
              "      <td>1272.810059</td>\n",
              "      <td>...</td>\n",
              "      <td>194253.0</td>\n",
              "      <td>5.255</td>\n",
              "      <td>4.195</td>\n",
              "      <td>3.969</td>\n",
              "      <td>95.620003</td>\n",
              "      <td>95.620003</td>\n",
              "      <td>5794100.0</td>\n",
              "      <td>109725.0</td>\n",
              "      <td>109725.000000</td>\n",
              "      <td>10557.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085999</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>223590</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>8100</td>\n",
              "      <td>8230</td>\n",
              "      <td>202307</td>\n",
              "      <td>1272.810059</td>\n",
              "      <td>...</td>\n",
              "      <td>194253.0</td>\n",
              "      <td>5.255</td>\n",
              "      <td>4.195</td>\n",
              "      <td>3.969</td>\n",
              "      <td>95.620003</td>\n",
              "      <td>95.620003</td>\n",
              "      <td>5794100.0</td>\n",
              "      <td>109725.0</td>\n",
              "      <td>109725.000000</td>\n",
              "      <td>10557.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1086000 rows × 36 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-906f5f4d-4618-469c-be8d-2b399d55fc65')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-87680bf0-887f-4d11-9868-dad339a70b08\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-87680bf0-887f-4d11-9868-dad339a70b08')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-87680bf0-887f-4d11-9868-dad339a70b08 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-906f5f4d-4618-469c-be8d-2b399d55fc65 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-906f5f4d-4618-469c-be8d-2b399d55fc65');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRN_tIlAS0Qd"
      },
      "source": [
        "https://songseungwon.tistory.com/118"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93bVoMyDDA4E"
      },
      "source": [
        "**기술 지표**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2bdEOWiQe3x"
      },
      "outputs": [],
      "source": [
        "def get_technical_indicators(dataset, item_Code):\n",
        "\n",
        "  dataset = dataset[dataset['item_Code'] == item_Code]\n",
        "  O = dataset['Open']\n",
        "  H = dataset['High']\n",
        "  L = dataset['Low']\n",
        "  C = dataset['Close']\n",
        "  V = dataset['Volume']\n",
        "\n",
        "  dataset['MFI'] = ta.volume.money_flow_index(high=H, low=L, close=C, volume=V, fillna=True)\n",
        "  dataset['ADI'] = ta.volume.acc_dist_index(high=H, low=L, close=C, volume=V, fillna=True)\n",
        "  dataset['OBV'] = ta.volume.on_balance_volume(close=C, volume=V, fillna=True)\n",
        "  dataset['CMF'] = ta.volume.chaikin_money_flow(high=H, low=L, close=C, volume=V, fillna=True)\n",
        "  dataset['FI'] = ta.volume.force_index(close=C, volume=V, fillna=True)\n",
        "  dataset['EOM, EMV'] = ta.volume.ease_of_movement(high=H, low=L, volume=V, fillna=True)\n",
        "  dataset['VPT'] = ta.volume.volume_price_trend(close=C, volume=V, fillna=True)\n",
        "  dataset['NVI'] = ta.volume.negative_volume_index(close=C, volume=V, fillna=True)\n",
        "  dataset['VMAP'] = ta.volume.volume_weighted_average_price(high=H, low=L, close=C, volume=V, fillna=True)\n",
        "\n",
        "  # Volatility\n",
        "  dataset['ATR'] = ta.volatility.average_true_range(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['UB'] = ta.volatility.bollinger_hband(close=C, fillna=True)\n",
        "  dataset['AB'] = ta.volatility.bollinger_mavg(close=C, fillna=True)\n",
        "  dataset['LB'] = ta.volatility.bollinger_lband(close=C, fillna=True)\n",
        "  dataset['KCH'] = ta.volatility.keltner_channel_hband(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['KCL'] = ta.volatility.keltner_channel_lband(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['KCM'] = ta.volatility.keltner_channel_mband(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['DCH'] = ta.volatility.donchian_channel_hband(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['DCL'] = ta.volatility.donchian_channel_lband(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['DCM'] = ta.volatility.donchian_channel_mband(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['UI'] = ta.volatility.ulcer_index(close=C, fillna=True)\n",
        "\n",
        "  # Trend\n",
        "  timeperiods = [5, 6, 7, 10, 20, 21, 50, 60, 100, 120]\n",
        "  for timeperiod in timeperiods:\n",
        "    dataset[f'MA{timeperiod}'] = C.rolling(window = timeperiod).mean().values\n",
        "\n",
        "  dataset['SMA'] = ta.trend.sma_indicator(close=C, fillna=True)\n",
        "  dataset['EMA'] = ta.trend.ema_indicator(close=C, fillna=True)\n",
        "  dataset['WMA'] = ta.trend.wma_indicator(close=C, fillna=True)\n",
        "  dataset['MACD'] = ta.trend.macd(close=C, fillna=True)\n",
        "  dataset['MACD_Signal'] = ta.trend.macd_signal(close=C, fillna=True)\n",
        "  dataset['ADX'] = ta.trend.adx(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['-VI'] = ta.trend.vortex_indicator_neg(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['+VI'] = ta.trend.vortex_indicator_pos(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['TRIX'] = ta.trend.trix(close=C, fillna=True)\n",
        "  dataset['MI'] = ta.trend.mass_index(high=H, low=L, fillna=True)\n",
        "  dataset['CCI'] = ta.trend.cci(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['DPO'] = ta.trend.dpo(close=C, fillna=True)\n",
        "  dataset['KST'] = ta.trend.kst(close=C, fillna=True)\n",
        "  dataset['Ichimoku'] = ta.trend.ichimoku_a(high=H, low=L, fillna=True)\n",
        "  dataset['Parabolic SAR'] = ta.trend.psar_down(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['STC'] = ta.trend.stc(close=C, fillna=True)\n",
        "\n",
        "  # Momentum\n",
        "  dataset['RSI'] = ta.momentum.rsi(close=C, fillna=True)\n",
        "  dataset['SRSI'] = ta.momentum.stochrsi(close=C, fillna=True)\n",
        "  dataset['TSI'] = ta.momentum.tsi(close=C, fillna=True)\n",
        "  dataset['UO'] = ta.momentum.ultimate_oscillator(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['SR'] = ta.momentum.stoch(close=C, high=H, low=L, fillna=True)\n",
        "  dataset['WR'] = ta.momentum.williams_r(high=H, low=L, close=C, fillna=True)\n",
        "  dataset['AO'] = ta.momentum.awesome_oscillator(high=H, low=L, fillna=True)\n",
        "  dataset['KAMA'] = ta.momentum.kama(close=C, fillna=True)\n",
        "  dataset['ROC'] = ta.momentum.roc(close=C, fillna=True)\n",
        "  dataset['PPO'] = ta.momentum.ppo(close=C, fillna=True)\n",
        "  dataset['PVO'] = ta.momentum.pvo(volume=V, fillna=True)\n",
        "\n",
        "  # Momentum\n",
        "  #dataset['momentum'] = C - 1\n",
        "  #dataset['log_momentum'] = np.log1p(dataset['momentum'])\n",
        "\n",
        "  # Calculate Daily_Range and Mean\n",
        "  dataset['Daily_Range'] = C - O\n",
        "  dataset['Mean'] = (H + L) / 2\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E56Q4dLYDDLg"
      },
      "outputs": [],
      "source": [
        "# def get_technical_indicators(dataset, item_Code):\n",
        "\n",
        "#   dataset = dataset[dataset['item_Code'] == item_Code]\n",
        "\n",
        "#   # Moving Average\n",
        "#   numbers = [5, 7, 10, 20, 21, 50, 100]\n",
        "#   for number in numbers:\n",
        "#     dataset[f'ma{number}'] = dataset['Close'].rolling(window = number).mean().values\n",
        "\n",
        "#   # MACD\n",
        "#   dataset['ema26'] = dataset['Close'].ewm(span = 26, min_periods = 26-1, adjust = False).mean().values\n",
        "#   dataset['ema12'] = dataset['Close'].ewm(span = 12, min_periods = 12-1, adjust = False).mean().values\n",
        "#   dataset['MACD'] = (dataset['ema26'] - dataset['ema12'])\n",
        "#   dataset['MACD_Signal'] = dataset['MACD'].ewm(span = 9, min_periods = 9-1, adjust = False).mean().values\n",
        "#   dataset['MACD_Oscillator'] = dataset['MACD'] - dataset['MACD_Signal']\n",
        "\n",
        "#   # Bollinger Bands\n",
        "#   dataset['sd20'] = dataset['Close'].rolling(window = 20).std().values\n",
        "#   dataset['Upper'] = dataset['ma20'] + 2*dataset['sd20']\n",
        "#   dataset['Lower'] = dataset['ma20'] - 2*dataset['sd20']\n",
        "\n",
        "#   # Exponential moving average\n",
        "#   dataset['ema'] = dataset['Close'].ewm(com = 0.5).mean().values\n",
        "\n",
        "#   # Momentum\n",
        "#   dataset['momentum'] = dataset['Close'] - 1\n",
        "#   dataset['log_momentum'] = np.log1p(dataset['momentum'])\n",
        "\n",
        "#   # RSI\n",
        "#   dataset['u'] = np.where(dataset['Close'].diff(1) > 0, dataset['Close'].diff(1), 0)\n",
        "#   dataset['d'] = np.where(dataset['Close'].diff(1) < 0, dataset['Close'].diff(1)*(-1), 0)\n",
        "#   dataset['au'] = dataset['u'].rolling(window = 14).mean().values\n",
        "#   dataset['ad'] = dataset['d'].rolling(window = 14).mean().values\n",
        "#   dataset['rsi'] = dataset['au'] / (dataset['au'] + dataset['ad']) * 100\n",
        "#   dataset['rsi_Signal'] = dataset['rsi'].rolling(window = 9).mean().values\n",
        "\n",
        "#   # VWAP\n",
        "#   dataset['volume_price'] = dataset['Open'] * dataset['Volume']\n",
        "#   dataset['volume_price_sum'] = dataset['volume_price'].apply(lambda x: np.cumsum(x)).values\n",
        "#   dataset['volume_sum'] = dataset['Volume'].apply(lambda x: np.cumsum(x)).values\n",
        "#   dataset['vwap'] = dataset['volume_price_sum'] / dataset['volume_sum']\n",
        "#   dataset['diff'] = dataset['Open'] - dataset['vwap'] # diff : 양수 - 상승세, 음수 - 하락세\n",
        "\n",
        "#   # CCI\n",
        "#   typical_price = (dataset['High'] + dataset['Low'] + dataset['Close']) / 3\n",
        "#   sma = typical_price.rolling(window = 20).mean()\n",
        "#   mean_deviation = (typical_price - sma).abs().rolling(window = 20).mean()\n",
        "#   dataset['cci'] = (typical_price - sma) / (0.015 * mean_deviation)\n",
        "\n",
        "#   return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_dwiFSQrEe2",
        "outputId": "759da645-f1e1-4d7d-f988-4abc7626eb1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time : 872.290910243988\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "train_concat = pd.concat([get_technical_indicators(group, item_Code) for item_Code, group in train.groupby('item_Code')], axis=0)\n",
        "\n",
        "#train_concat = train.groupby('item_Code').apply(get_technical_indicators()).reset_index(drop=True)\n",
        "\n",
        "print(\"time :\", time.time() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c78ccFufLizZ"
      },
      "outputs": [],
      "source": [
        "train = train_concat.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Qr2jQfJUhkCK",
        "outputId": "7872b422-7527-479e-b729-5dd3b783594a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Date item_Code item_Name  Volume   Open   High    Low  Close  \\\n",
              "0        20210601   A000020      동화약품  114966  14700  14700  14450  14600   \n",
              "1        20210602   A000020      동화약품  109559  14700  14700  14450  14500   \n",
              "2        20210603   A000020      동화약품   96158  14550  14650  14450  14600   \n",
              "3        20210604   A000020      동화약품  133900  14600  14800  14550  14700   \n",
              "4        20210607   A000020      동화약품  511140  14800  15550  14750  15150   \n",
              "...           ...       ...       ...     ...    ...    ...    ...    ...   \n",
              "1085995  20230728   A383800     LX홀딩스  223590   8280   8300   8100   8230   \n",
              "1085996  20230728   A383800     LX홀딩스  223590   8280   8300   8100   8230   \n",
              "1085997  20230728   A383800     LX홀딩스  223590   8280   8300   8100   8230   \n",
              "1085998  20230728   A383800     LX홀딩스  223590   8280   8300   8100   8230   \n",
              "1085999  20230728   A383800     LX홀딩스  223590   8280   8300   8100   8230   \n",
              "\n",
              "         Date_ym  Close_usd_krw  ...         UO         SR         WR  \\\n",
              "0         202106    1106.979980  ...   0.000000  60.000000 -40.000000   \n",
              "1         202106    1107.209961  ...  10.000000  20.000000 -80.000000   \n",
              "2         202106    1109.880005  ...  28.571429  60.000000 -40.000000   \n",
              "3         202106    1115.439941  ...  36.842105  71.428571 -28.571429   \n",
              "4         202106    1109.680054  ...  44.444444  63.636364 -36.363636   \n",
              "...          ...            ...  ...        ...        ...        ...   \n",
              "1085995   202307    1283.880005  ...  39.848466  42.063492 -57.936508   \n",
              "1085996   202307    1272.810059  ...  42.440047  42.063492 -57.936508   \n",
              "1085997   202307    1272.810059  ...  53.296674  42.063492 -57.936508   \n",
              "1085998   202307    1272.810059  ...  54.947476  42.063492 -57.936508   \n",
              "1085999   202307    1272.810059  ...  55.720635  42.063492 -57.936508   \n",
              "\n",
              "                 AO          KAMA       ROC       PPO        PVO  Daily_Range  \\\n",
              "0          0.000000  14600.000000  0.000000  0.000000   0.000000         -100   \n",
              "1          0.000000  14599.583767  0.000000 -0.054666  -0.376490         -200   \n",
              "2          0.000000  14599.585499  0.000000 -0.042205  -1.619559           50   \n",
              "3          0.000000  14600.003458  0.000000  0.022689   0.086992          100   \n",
              "4          0.000000  14602.292725  0.000000  0.318224  22.003352          350   \n",
              "...             ...           ...       ...       ...        ...          ...   \n",
              "1085995  174.000000   8790.814142  2.746567  0.559356  24.971855          -50   \n",
              "1085996  180.147059   8788.479848  2.875000  0.641832  17.905772          -50   \n",
              "1085997  181.029412   8786.155270  4.177215  0.698807  11.845397          -50   \n",
              "1085998  181.176471   8783.840367  4.574333  0.735223   6.682143          -50   \n",
              "1085999  180.147059   8781.535100  4.707379  0.755172   2.316630          -50   \n",
              "\n",
              "            Mean  \n",
              "0        14575.0  \n",
              "1        14575.0  \n",
              "2        14550.0  \n",
              "3        14675.0  \n",
              "4        15150.0  \n",
              "...          ...  \n",
              "1085995   8200.0  \n",
              "1085996   8200.0  \n",
              "1085997   8200.0  \n",
              "1085998   8200.0  \n",
              "1085999   8200.0  \n",
              "\n",
              "[1086000 rows x 95 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-89335de7-797b-48f0-a2a4-1f218b7703b3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>item_Code</th>\n",
              "      <th>item_Name</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Date_ym</th>\n",
              "      <th>Close_usd_krw</th>\n",
              "      <th>...</th>\n",
              "      <th>UO</th>\n",
              "      <th>SR</th>\n",
              "      <th>WR</th>\n",
              "      <th>AO</th>\n",
              "      <th>KAMA</th>\n",
              "      <th>ROC</th>\n",
              "      <th>PPO</th>\n",
              "      <th>PVO</th>\n",
              "      <th>Daily_Range</th>\n",
              "      <th>Mean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>114966</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>202106</td>\n",
              "      <td>1106.979980</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>-40.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14600.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-100</td>\n",
              "      <td>14575.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20210602</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>109559</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14500</td>\n",
              "      <td>202106</td>\n",
              "      <td>1107.209961</td>\n",
              "      <td>...</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>-80.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14599.583767</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.054666</td>\n",
              "      <td>-0.376490</td>\n",
              "      <td>-200</td>\n",
              "      <td>14575.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20210603</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>96158</td>\n",
              "      <td>14550</td>\n",
              "      <td>14650</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>202106</td>\n",
              "      <td>1109.880005</td>\n",
              "      <td>...</td>\n",
              "      <td>28.571429</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>-40.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14599.585499</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.042205</td>\n",
              "      <td>-1.619559</td>\n",
              "      <td>50</td>\n",
              "      <td>14550.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20210604</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>133900</td>\n",
              "      <td>14600</td>\n",
              "      <td>14800</td>\n",
              "      <td>14550</td>\n",
              "      <td>14700</td>\n",
              "      <td>202106</td>\n",
              "      <td>1115.439941</td>\n",
              "      <td>...</td>\n",
              "      <td>36.842105</td>\n",
              "      <td>71.428571</td>\n",
              "      <td>-28.571429</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14600.003458</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022689</td>\n",
              "      <td>0.086992</td>\n",
              "      <td>100</td>\n",
              "      <td>14675.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20210607</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>511140</td>\n",
              "      <td>14800</td>\n",
              "      <td>15550</td>\n",
              "      <td>14750</td>\n",
              "      <td>15150</td>\n",
              "      <td>202106</td>\n",
              "      <td>1109.680054</td>\n",
              "      <td>...</td>\n",
              "      <td>44.444444</td>\n",
              "      <td>63.636364</td>\n",
              "      <td>-36.363636</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14602.292725</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.318224</td>\n",
              "      <td>22.003352</td>\n",
              "      <td>350</td>\n",
              "      <td>15150.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085995</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>223590</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>8100</td>\n",
              "      <td>8230</td>\n",
              "      <td>202307</td>\n",
              "      <td>1283.880005</td>\n",
              "      <td>...</td>\n",
              "      <td>39.848466</td>\n",
              "      <td>42.063492</td>\n",
              "      <td>-57.936508</td>\n",
              "      <td>174.000000</td>\n",
              "      <td>8790.814142</td>\n",
              "      <td>2.746567</td>\n",
              "      <td>0.559356</td>\n",
              "      <td>24.971855</td>\n",
              "      <td>-50</td>\n",
              "      <td>8200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085996</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>223590</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>8100</td>\n",
              "      <td>8230</td>\n",
              "      <td>202307</td>\n",
              "      <td>1272.810059</td>\n",
              "      <td>...</td>\n",
              "      <td>42.440047</td>\n",
              "      <td>42.063492</td>\n",
              "      <td>-57.936508</td>\n",
              "      <td>180.147059</td>\n",
              "      <td>8788.479848</td>\n",
              "      <td>2.875000</td>\n",
              "      <td>0.641832</td>\n",
              "      <td>17.905772</td>\n",
              "      <td>-50</td>\n",
              "      <td>8200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085997</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>223590</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>8100</td>\n",
              "      <td>8230</td>\n",
              "      <td>202307</td>\n",
              "      <td>1272.810059</td>\n",
              "      <td>...</td>\n",
              "      <td>53.296674</td>\n",
              "      <td>42.063492</td>\n",
              "      <td>-57.936508</td>\n",
              "      <td>181.029412</td>\n",
              "      <td>8786.155270</td>\n",
              "      <td>4.177215</td>\n",
              "      <td>0.698807</td>\n",
              "      <td>11.845397</td>\n",
              "      <td>-50</td>\n",
              "      <td>8200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085998</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>223590</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>8100</td>\n",
              "      <td>8230</td>\n",
              "      <td>202307</td>\n",
              "      <td>1272.810059</td>\n",
              "      <td>...</td>\n",
              "      <td>54.947476</td>\n",
              "      <td>42.063492</td>\n",
              "      <td>-57.936508</td>\n",
              "      <td>181.176471</td>\n",
              "      <td>8783.840367</td>\n",
              "      <td>4.574333</td>\n",
              "      <td>0.735223</td>\n",
              "      <td>6.682143</td>\n",
              "      <td>-50</td>\n",
              "      <td>8200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085999</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>223590</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>8100</td>\n",
              "      <td>8230</td>\n",
              "      <td>202307</td>\n",
              "      <td>1272.810059</td>\n",
              "      <td>...</td>\n",
              "      <td>55.720635</td>\n",
              "      <td>42.063492</td>\n",
              "      <td>-57.936508</td>\n",
              "      <td>180.147059</td>\n",
              "      <td>8781.535100</td>\n",
              "      <td>4.707379</td>\n",
              "      <td>0.755172</td>\n",
              "      <td>2.316630</td>\n",
              "      <td>-50</td>\n",
              "      <td>8200.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1086000 rows × 95 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89335de7-797b-48f0-a2a4-1f218b7703b3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-e6272a8d-3c31-4d9f-bea7-62038d4a584d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e6272a8d-3c31-4d9f-bea7-62038d4a584d')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-e6272a8d-3c31-4d9f-bea7-62038d4a584d button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-89335de7-797b-48f0-a2a4-1f218b7703b3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-89335de7-797b-48f0-a2a4-1f218b7703b3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kh_da_dbGAH",
        "outputId": "efd6fb9c-2b7c-45ed-e87a-d44f0f8bc0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1086000, 95)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.isin([np.nan]).sum()[train.isin([np.nan]).sum() > 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4DRx7wvOsQP",
        "outputId": "862f46f6-bb68-4a1d-a7d7-cc9ce2a41861"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Close_kospi            4000\n",
              "Volume_kospi           4000\n",
              "Close_kosdaq           6000\n",
              "Volume_kosdaq          6000\n",
              "Close_dji             38000\n",
              "Volume_dji            38000\n",
              "Close_ixic            38000\n",
              "Volume_ixic           38000\n",
              "Close_sp500           38000\n",
              "Volume_sp500          38000\n",
              "Close_gold            38000\n",
              "Volume_gold           38000\n",
              "Close_irx             38000\n",
              "Close_fvx             38000\n",
              "Close_tnx             38000\n",
              "Close_us_bond         38000\n",
              "Adj Close_us_bond     38000\n",
              "Volume_us_bond        38000\n",
              "Close_kr_bond         32000\n",
              "Adj Close_kr_bond     32000\n",
              "Volume_kr_bond        32000\n",
              "MA5                    8000\n",
              "MA6                   10000\n",
              "MA7                   12000\n",
              "MA10                  18000\n",
              "MA20                  38000\n",
              "MA21                  40000\n",
              "MA50                  98000\n",
              "MA60                 118000\n",
              "MA100                198000\n",
              "MA120                238000\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.isin([np.inf]).sum()[train.isin([np.inf]).sum() > 0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF9W5KZ0Rl9p",
        "outputId": "9d60968a-a340-4d8f-c5bb-b2119f34690a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Series([], dtype: int64)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train = train.replace(-np.inf, 0)\n",
        "#train.shape\n",
        "#train = train.replace(np.inf, np.nan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfRgLjuibU7Q",
        "outputId": "5c649651-5be6-4d01-b53d-2b9dab5327a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1086000, 95)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-2NijgGDD2Y"
      },
      "outputs": [],
      "source": [
        "train.to_parquet('train.parquet',index=False)\n",
        "#train = pd.read_parquet('train.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1VS7LekI240"
      },
      "outputs": [],
      "source": [
        "def plot_technical_indicators(dataset, item_Code, last_days):\n",
        "  plt.figure(figsize = (25, 10))\n",
        "  itemset = dataset[dataset['item_Code'] == item_Code]\n",
        "  item_name = (itemset['item_Name'].unique())[0]\n",
        "  print(item_name)\n",
        "  itemset_ = itemset.iloc[-last_days:]\n",
        "\n",
        "  item_range = range(0, itemset.shape[0])\n",
        "  item_range_ = list(item_range)\n",
        "\n",
        "  plt.subplot(2, 1, 1)\n",
        "  plt.plot(itemset_['Close'], label = 'close', color = 'k')\n",
        "  plt.plot(itemset_['MA5'], label = 'MA5', color = 'r', linestyle = '--')\n",
        "  plt.plot(itemset_['MA7'], label = 'MA7', color = 'orange', linestyle = '--')\n",
        "  plt.plot(itemset_['MA10'], label = 'MA10', color = 'y', linestyle = '--')\n",
        "  plt.plot(itemset_['MA20'], label = 'MA20', color = 'g', linestyle = '--')\n",
        "  plt.plot(itemset_['MA21'], label = 'MA21', color = 'b', linestyle = '--')\n",
        "  plt.plot(itemset_['MA50'], label = 'MA50', color = 'c', linestyle = '--')\n",
        "  plt.plot(itemset_['MA100'], label = 'MA100', color = 'm', linestyle = '--')\n",
        "  plt.plot(itemset_['UB'], label = 'upper_band', color = 'brown')\n",
        "  plt.plot(itemset_['LB'], label = 'lower_band', color = 'brown')\n",
        "  plt.fill_between(item_range_, itemset['LB'], itemset['UB'], alpha = 0.3)\n",
        "  plt.title(f'Technical indicators for {item_Code} ({item_name}) - last {last_days} days.')\n",
        "  plt.ylabel('KRW')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(2, 1, 2)\n",
        "  plt.plot(itemset_['MACD'], label = 'macd', linestyle = '-.')\n",
        "  plt.hlines(itemset['MACD'].max(), itemset.shape[0] - last_days, itemset.shape[0], colors = 'g', linestyle = '--')\n",
        "  plt.hlines(itemset['MACD'].min(), itemset.shape[0] - last_days, itemset.shape[0], colors = 'g', linestyle = '--')\n",
        "  plt.plot(itemset_['log_momentum'], label = 'momentum', color = 'b', linestyle = '-')\n",
        "  plt.title('MACD, momentum')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "Cr7p4ds5Np-A",
        "outputId": "da9903fd-9820-4dbc-a379-bd33ab0b8833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "동화약품\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2500x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_technical_indicators(train, 'A000020', 400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpQBVBC1DDfR"
      },
      "source": [
        "**펀더멘털 분석**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0NJLV_uDFaq"
      },
      "outputs": [],
      "source": [
        "# 뉴스 크롤링, 감성 분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqDVNNv6DFuM"
      },
      "source": [
        "**노이즈 제거 (푸리에 변환, 웨이블릿 변환, AutoEncoder)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbsN4D8zDHh0"
      },
      "outputs": [],
      "source": [
        "# 푸리에 변환\n",
        "def get_Fourier_Transform(dataset, topN = 2):\n",
        "  fft = np.fft.fft(dataset['Close'])\n",
        "  fft[topN:-topN] = 0\n",
        "  ifft = np.fft.ifft(fft)\n",
        "  return ifft\n",
        "\n",
        "train['fft30'] = get_Fourier_Transform(train, 30)\n",
        "train['fft30'] = train['fft30'].astype('float')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "fGBpNC_QYfVp",
        "outputId": "45d06965-26dd-4e3a-b50a-b121ba53f608"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2500x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize = (25, 10))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(train['fft30'], label = 'fft30', color = 'g', linestyle = '--')\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(train['Close'], label = 'close', color = 'k')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7_CJjPVaKTa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "56638e06-dd66-4663-e738-4d7da2fb82a3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-0a2808d66e1a>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mwt_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_Wavelet_Transform_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwavelet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'db4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mstacked_wt_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwt_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wt_db4'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_wt_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mstacked_wt_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3978\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3979\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3980\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3982\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4172\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m         \"\"\"\n\u001b[0;32m-> 4174\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4176\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4915\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4916\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    572\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (1104000) does not match length of index (1102000)"
          ]
        }
      ],
      "source": [
        "# 웨이블릿 변환\n",
        "def get_Wavelet_Transform(dataset, wavelet = 'db5', threshold = 0.63):\n",
        "  signal = dataset['Close'].values\n",
        "  thershold = threshold * np.nanmax(signal)\n",
        "  coeff = pywt.wavedec(signal, wavelet, mode = 'per')\n",
        "  coeff[1:] = (pywt.threshold(i, value = threshold, mode = 'soft') for i in coeff[1:])\n",
        "  reconstructed_signal = pywt.waverec(coeff, wavelet, mode = 'per')\n",
        "  return reconstructed_signal\n",
        "\n",
        "def get_Wavelet_Transform_v2(dataset, wavelet = 'db4', level = 2):\n",
        "  signal = dataset['Close'].values\n",
        "  coeffs = pywt.wavedec(signal, wavelet, level = level)\n",
        "  threshold = np.sqrt(2 * np.log(len(dataset))) * np.std(coeffs[-level])\n",
        "  coeffs[1:] = (pywt.threshold(c, threshold) for c in coeffs[1:])\n",
        "  denoised_data = pywt.waverec(coeffs, wavelet)\n",
        "  return denoised_data\n",
        "\n",
        "def get_Wavelet_Transform_v3(dataset, wavelet = 'haar', level = 2, declevel = 2):\n",
        "  signal = dataset['Close'].values\n",
        "  coeff = pywt.wavedec(signal, wavelet, mode = 'periodization', level = declevel, axis = 0)\n",
        "  sigma = mad(coeff[-level])\n",
        "  uthresh = sigma * np.sqrt(2 * np.log(len(signal)))\n",
        "  coeff[1:] = (pywt.threshold(i, value = uthresh, mode = 'hard') for i in coeff[1:])\n",
        "  y = pywt.waverec(coeff, wavelet, mode = 'periodization', axis = 0)\n",
        "  return y\n",
        "\n",
        "def get_Wavelet_Transform_v4(dataset, wavelet = 'db4', level = 1, declevel = 2, title = None):\n",
        "  signal = dataset['Close'].values\n",
        "  coeff = pywt.wavedec(signal, wavelet, mode = 'per', level = declevel)\n",
        "  sigma = mad(coeff[-level])\n",
        "  uthresh = sigma * np.sqrt(2 * np.log(len(signal)))\n",
        "  coeff[1:] = (pywt.threshold(i, value = uthresh, mode = 'soft') for i in coeff[1:])\n",
        "  y = pywt.waverec(coeff, wavelet, mode = 'per')\n",
        "  return y\n",
        "\n",
        "stacked_wt_data = []\n",
        "for item_Code in train['item_Code'].unique().tolist():\n",
        "  data = train[train['item_Code'] == item_Code]\n",
        "  wt_data = get_Wavelet_Transform_v2(data, wavelet = 'db4', level = 2)\n",
        "  stacked_wt_data.append(wt_data)\n",
        "train['wt_db4'] = np.concatenate(stacked_wt_data, axis = 0)\n",
        "\n",
        "stacked_wt_data = []\n",
        "for item_Code in train['item_Code'].unique().tolist():\n",
        "  data = train[train['item_Code'] == item_Code]\n",
        "  wt_data = get_Wavelet_Transform(data, wavelet = 'db5')\n",
        "  stacked_wt_data.append(wt_data)\n",
        "train['wt_db5'] = np.concatenate(stacked_wt_data, axis = 0)\n",
        "\n",
        "stacked_wt_data = []\n",
        "for item_Code in train['item_Code'].unique().tolist():\n",
        "  data = train[train['item_Code'] == item_Code]\n",
        "  wt_data = get_Wavelet_Transform(data, wavelet = 'db7')\n",
        "  stacked_wt_data.append(wt_data)\n",
        "train['wt_db7'] = np.concatenate(stacked_wt_data, axis = 0)\n",
        "\n",
        "stacked_wt_data = []\n",
        "for item_Code in train['item_Code'].unique().tolist():\n",
        "  data = train[train['item_Code'] == item_Code]\n",
        "  wt_data = get_Wavelet_Transform(data, wavelet = 'db9')\n",
        "  stacked_wt_data.append(wt_data)\n",
        "train['wt_db9'] = np.concatenate(stacked_wt_data, axis = 0)\n",
        "\n",
        "stacked_wt_data = []\n",
        "for item_Code in train['item_Code'].unique().tolist():\n",
        "  data = train[train['item_Code'] == item_Code]\n",
        "  wt_data = get_Wavelet_Transform_v3(data)\n",
        "  stacked_wt_data.append(wt_data)\n",
        "train['wt_haar_hard'] = np.concatenate(stacked_wt_data, axis = 0)\n",
        "\n",
        "stacked_wt_data = []\n",
        "for item_Code in train['item_Code'].unique().tolist():\n",
        "  data = train[train['item_Code'] == item_Code]\n",
        "  wt_data = get_Wavelet_Transform_v4(data)\n",
        "  stacked_wt_data.append(wt_data)\n",
        "train['wt_db4_soft'] = np.concatenate(stacked_wt_data, axis = 0)\n",
        "train['wt_db4_soft'] = train['wt_db4_soft'].ffill().bfill()\n",
        "\n",
        "\n",
        "# train['wt_db4'] = get_Wavelet_Transform_v2(train, wavelet = 'db4', level = 2)\n",
        "# train['wt_db5'] = get_Wavelet_Transform(train, wavelet = 'db5')\n",
        "# train['wt_db7'] = get_Wavelet_Transform(train, wavelet = 'db7')\n",
        "# train['wt_db9'] = get_Wavelet_Transform(train, wavelet = 'db9')\n",
        "# train['wt_haar_hard'] = get_Wavelet_Transform_v3(train)\n",
        "# train['wt_db4_soft'] = get_Wavelet_Transform_v4(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKluSwhqbeZ_"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (25, 10))\n",
        "plt.subplot(5, 1, 1)\n",
        "plt.plot(train['wt_db4'], label = 'wt_db4', color = 'g', linestyle = '--')\n",
        "plt.subplot(5, 1, 2)\n",
        "plt.plot(train['wt_db5'], label = 'wt_db5', color = 'b', linestyle = '--')\n",
        "plt.subplot(5, 1, 3)\n",
        "plt.plot(train['Close'], label = 'close', color = 'k')\n",
        "plt.subplot(5, 1, 4)\n",
        "plt.plot(train['wt_haar_hard'], label = 'wt_haar_hard', color = 'r')\n",
        "plt.subplot(5, 1, 5)\n",
        "plt.plot(train['wt_db4_soft'], label = 'wt_db4_soft', color = 'y')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml4ae3cGd1oh"
      },
      "outputs": [],
      "source": [
        "def MAPE(y_test, y_pred):\n",
        "\treturn np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "def MAE(y_test, y_pred):\n",
        "  return np.mean(np.abs(y_test - y_pred))\n",
        "\n",
        "def RMSE(y_test, y_pred):\n",
        "  MSE = mean_squared_error(y_test, y_pred)\n",
        "  return np.sqrt(MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yrw4hGmpsXNF"
      },
      "outputs": [],
      "source": [
        "# LSTM-AutoEncoder\n",
        "class TimeDistributed(nn.Module):\n",
        "    def __init__(self, module):\n",
        "      super(TimeDistributed, self).__init__()\n",
        "      self.module = module\n",
        "\n",
        "    def forward(self, x):\n",
        "      if len(x.size()) <= 2:\n",
        "        return self.module(x)\n",
        "      x_reshape = x.contiguous().view(-1, x.size(-1))\n",
        "      y = self.module(x_reshape)\n",
        "      if len(x.size()) == 3:\n",
        "        y = y.contiguous().view(x.size(0), -1, y.size(-1))\n",
        "      return y\n",
        "\n",
        "class LSTM_AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(LSTM_AutoEncoder, self).__init__()\n",
        "      self.encoder = nn.LSTM(\n",
        "          input_size = 1,\n",
        "          hidden_size = 16,\n",
        "          dropout = 0.25,\n",
        "          num_layers = 2,\n",
        "          bias = True,\n",
        "          batch_first = True,\n",
        "          bidirectional = True,\n",
        "      )\n",
        "      self.decoder = nn.LSTM(\n",
        "          input_size = 32,\n",
        "          hidden_size = 16,\n",
        "          dropout = 0.25,\n",
        "          num_layers = 2,\n",
        "          bias = True,\n",
        "          batch_first = True,\n",
        "          bidirectional = True,\n",
        "      )\n",
        "\n",
        "      self.fc = TimeDistributed(nn.Linear(32, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "      h0, (h_n, c_n) = self.encoder(x)\n",
        "      h0, (h_n, c_n) = self.decoder(h0[:, -1:, :].repeat(1, x.size(1), 1))\n",
        "      out = self.fc(h0)\n",
        "      return out\n",
        "\n",
        "# CAE\n",
        "class CAE(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(CAE, self).__init__()\n",
        "      self.encoder = nn.Sequential(\n",
        "          nn.Conv1d(1, 16, kernel_size = 3, stride = 1, padding = 1),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv1d(16, 8, kernel_size = 3, stride = 1, padding = 1),\n",
        "          nn.ReLU(),\n",
        "      )\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.ConvTranspose1d(8, 16, kernel_size = 3, stride = 1, padding = 1),\n",
        "          nn.ReLU(),\n",
        "          nn.ConvTranspose1d(16, 1, kernel_size = 3, stride = 1, padding = 1),\n",
        "          #nn.Sigmoid(),\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      encoded = self.encoder(x)\n",
        "      decoded = self.decoder(encoded)\n",
        "      return decoded\n",
        "\n",
        "# DAE\n",
        "class DAE(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "      super(DAE, self).__init__()\n",
        "      self.encoder = nn.Linear(input_size, hidden_size)\n",
        "      self.decoder = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "      encoded = self.encoder(x)\n",
        "      decoded = self.decoder(encoded)\n",
        "      return decoded\n",
        "\n",
        "# DDAE\n",
        "class DDAE(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super(DDAE, self).__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size//2),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size//2, hidden_size),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.Linear(hidden_size, hidden_size//2),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size//2, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, input_size),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "# AE\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "      super(AutoEncoder, self).__init__()\n",
        "      self.encoder = nn.Sequential(\n",
        "          nn.Linear(input_size, 64),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(64, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(128, 512),\n",
        "      )\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.Linear(512, 128),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(128, 64),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(64, input_size),\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      encoded = self.encoder(x)\n",
        "      decoded = self.decoder(encoded)\n",
        "      return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIFtlnzHqXty"
      },
      "outputs": [],
      "source": [
        "stacked_denoised_data = []\n",
        "\n",
        "for item_Code in train['item_Code'].unique().tolist():\n",
        "\n",
        "  print('-' * 20)\n",
        "  print(f'item_Code: {item_Code}')\n",
        "  print('-' * 20)\n",
        "\n",
        "  data = train[train['item_Code'] == item_Code]\n",
        "  data = data.loc[:, 'Close'].values\n",
        "  data = torch.tensor(data, dtype = torch.float32).unsqueeze(0) # AE, DAE, DDAE\n",
        "  #data = torch.tensor(data, dtype = torch.float32).unsqueeze(0).unsqueeze(0) # CAE\n",
        "  #data = torch.tensor(data, dtype = torch.float32).unsqueeze(0).unsqueeze(-1) # or .view(1, -1, 1) # LSTM-AutoEncoder\n",
        "  dataloader = DataLoader(data, batch_size = 32, shuffle = False)\n",
        "\n",
        "  input_size = data.shape[1]\n",
        "  model = AutoEncoder(input_size)\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "  '''\n",
        "  Training and Inference\n",
        "  '''\n",
        "  model.train()\n",
        "  num_epochs = 200\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "    train_loss = 0.0\n",
        "    for batch_data in dataloader:\n",
        "      inputs = batch_data\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, data)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()\n",
        "    avg_loss = train_loss / len(dataloader)\n",
        "    print(f'Epoch[{epoch+1}/{num_epochs} Loss: {avg_loss}]')\n",
        "\n",
        "  denoised_data = []\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    outputs = model(data).squeeze()\n",
        "    denoised_data.append(outputs)\n",
        "\n",
        "  '''\n",
        "  No Training and Inference\n",
        "  '''\n",
        "  # denoised_data = []\n",
        "  # with torch.no_grad():\n",
        "  #   for batch_data in dataloader:\n",
        "  #     inputs = batch_data\n",
        "  #     outputs = model(inputs)\n",
        "  #     denoised_data.append(outputs)\n",
        "\n",
        "  denoised_data = torch.cat(denoised_data, dim = 0)\n",
        "  stacked_denoised_data.append(denoised_data)\n",
        "stacked_denoised_data = torch.cat(stacked_denoised_data, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZN-F454DLLd",
        "outputId": "c661f388-b6f9-409b-f898-2100ddfdefbe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         14600\n",
              "1         14500\n",
              "2         14600\n",
              "3         14700\n",
              "4         15150\n",
              "          ...  \n",
              "987995     8330\n",
              "987996     8300\n",
              "987997     8310\n",
              "987998     8280\n",
              "987999     8290\n",
              "Name: Close, Length: 988000, dtype: int64"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['Close']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMEIKRcxIg01",
        "outputId": "50f3a2dc-6e57-4743-b599-9ecaa5031137"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14600.        , 14238.49078365, 14441.31491337, ...,\n",
              "        8230.        ,  8230.        ,  8230.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "stacked_denoised_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgEkUa9zH7GD"
      },
      "outputs": [],
      "source": [
        "np.save('stacked_denoised_data_autoencoder.npy', stacked_denoised_data)\n",
        "np.savetxt('stacked_denoised_data_autoencoder.txt', stacked_denoised_data)\n",
        "\n",
        "np.load('stacked_denoised_data_autoencoder.npy')\n",
        "np.loadtxt('stacked_denoised_data_autoencoder.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj7X5eiTMwmM"
      },
      "outputs": [],
      "source": [
        "train['AE_close'] = stacked_denoised_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "fbfDhZTyJTkU",
        "outputId": "ce6d762d-ede4-4620-85ba-6c5d0b84da13"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre> ARIMA(0,1,1)(0,0,0)[0]          </pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ARIMA</label><div class=\"sk-toggleable__content\"><pre> ARIMA(0,1,1)(0,0,0)[0]          </pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "ARIMA(order=(0, 1, 1), scoring_args={}, suppress_warnings=True,\n",
              "      with_intercept=False)"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#model_fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCi0jvZeCfAv",
        "outputId": "d9917ab3-2065-40d7-f22a-81cdf555aad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "item_Code: A000020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A000990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001380\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001510\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001750\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A001940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002380\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A002990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003380\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A003960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004380\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A004990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005380\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005750\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A005990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006580\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "item_Code: A006620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A006980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A007980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A008970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A009970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A010960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A011930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012510\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012600\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "item_Code: A012610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012750\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A012860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A013030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A013120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A013310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A013360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A013520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A013570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A013580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A013700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A013720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A013810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A013870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A013890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A013990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A014990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A015230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A015260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A015360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A015540\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "item_Code: A015590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A015710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A015750\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A015760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A015860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A015890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016380\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A016880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017510\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A017960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A018000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A018120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A018250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A018260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A018290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A018310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A018470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A018500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A018620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A018670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A018680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A018700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A018880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A019010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A019170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A019180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A019210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A019440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A019540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A019550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A019570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A019590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A019660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A019680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A019770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A019990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A020000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A020120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A020150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A020560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A020710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A020760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A021050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A021080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A021240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A021320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A021650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A021820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A021880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A022100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A022220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A023960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A024950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025750\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A025980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A026040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A026150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A026890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A026940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A026960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A027050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A027360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A027410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A027580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A027710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A027740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A027830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A027970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A028050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A028080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A028100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A028260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A028300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A028670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A029460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A029480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A029530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A029780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A029960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A030000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A030190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A030200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A030210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A030350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A030520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A030530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A030610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A030960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A031310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A031330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A031430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A031440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A031510\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A031820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A031860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A031980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032750\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A032980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A033920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A034020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A034120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A034220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A034230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A034300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A034310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A034590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A034730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A034810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A034830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A034940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A034950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035510\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A035900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A036930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A037030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A037070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A037230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A037270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A037330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A037350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A037370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A037440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A037460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A037560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A037710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A037760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A037950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A038950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A039860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A040160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A040300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A040350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A040420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A040610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A040910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A041020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A041190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A041440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A041460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A041510\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A041520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A041590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A041650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A041830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A041910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A041920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A041930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A041960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A042000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A042040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A042110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A042370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A042420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A042500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A042510\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A042520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A042600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A042660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A042670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A042700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A042940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A043090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A043150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A043220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A043260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A043340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A043370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A043590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A043610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A043650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A043910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A044060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A044340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A044380\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A044450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A044490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A044820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A044960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A045060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A045100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A045300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A045390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A045520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A045660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A045970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A046070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A046120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A046140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A046210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A046310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A046390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A046440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A046890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A046940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A047040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A047050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A047080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A047310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A047400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A047560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A047770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A047810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A047820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A047920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A048260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A048410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A048430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A048530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A048550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A048770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A048870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A048910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A049960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A050110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A050120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A050860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A050890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A050960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A051160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A051360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A051370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A051380\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A051490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A051500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A051600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A051630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A051780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A051900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A051910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A051980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A052900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A053980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A054950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A055490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A055550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A056080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A056090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A056190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A056360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A056700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A056730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A057030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A057050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A057540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A057680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A057880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A058110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A058400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A058430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A058470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A058610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A058630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A058650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A058730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A058820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A058850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A058860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A058970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A059090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A059100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A059120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A059210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A059270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060380\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A060980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A061040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A061250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A061970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A062860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A063080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A063160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A063170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A063440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A063570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A064090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A064240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A064260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A064290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A064350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A064480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A064520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A064550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A064760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A064800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A064820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A064850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A064960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065510\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065560\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "item_Code: A065650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A065950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A066130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A066310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A066360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A066410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A066570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A066590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A066620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A066670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A066700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A066900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A066910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A066970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A066980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A067990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A068050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A068240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A068270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A068290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A068330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A068400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A068760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A068790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A068930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A069080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A069110\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "item_Code: A069260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A069410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A069460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A069510\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A069540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A069620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A069640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A069730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A069920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A069960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A070960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A071050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A071090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A071200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A071280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A071320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A071460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A071670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A071840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A071970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A072020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A072130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A072470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A072710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A072770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A072870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A072990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A073010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A073110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A073240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A073490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A073560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A073570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A073640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A074430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A074600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A074610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A075180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A075580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A075970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A076080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A076610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A077360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A077500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A077970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A078940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A079160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A079170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A079370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A079430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A079550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A079810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A079940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A079960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A079970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A079980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A080000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A080010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A080160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A080220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A080420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A080470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A080520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A080580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A080720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A081000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A081150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A081580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A081660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A082210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A082270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A082640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A082660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A082740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A082800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A082850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A082920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A083310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A083420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A083450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A083470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A083500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A083550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A083650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A083660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A083790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A083930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A084010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A084110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A084180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A084370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A084650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A084670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A084680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A084690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A084730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A084850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A084990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A085310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A085370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A085620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A085660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A085670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A085810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A085910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A086040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A086280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A086390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A086450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A086520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A086670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A086710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A086790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A086820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A086890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A086900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A086960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A086980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A087010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A087260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A087600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A088130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A088290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A088350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A088390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A088800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A088910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A089010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A089030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A089140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A089150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A089230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A089470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A089590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A089600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A089790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A089850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A089890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A089970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A089980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A090080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A090150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A090350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A090360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A090370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A090410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A090430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A090460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A090470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A090710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A090850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A091090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A091120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A091340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A091580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A091590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A091700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A091810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A091990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A092040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A092070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A092130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A092190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A092200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A092220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A092230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A092300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A092440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A092460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A092730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A092780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A092870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A093050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A093190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A093230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A093240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A093320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A093370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A093520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A093640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A093920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A094170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A094280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A094360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A094480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A094820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A094840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A094850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A094860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A094940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A094970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A095190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A095270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A095340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A095500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A095570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A095610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A095660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A095700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A095720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A095910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A096040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A096240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A096350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A096530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A096610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A096630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A096640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A096690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A096760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A096770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A097230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A097520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A097780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A097800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A097870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A097950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A098120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A098460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A099190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A099220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A099320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A099440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A099520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A099750\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A100030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A100090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A100120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A100130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A100220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A100250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A100590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A100660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A100700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A100790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A100840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A101140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A101160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A101170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A101240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A101330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A101360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A101390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A101490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A101530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A101670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A101730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A101930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A102120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A102260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A102280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A102460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A102710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A102940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A103140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A103590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A103840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A104040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A104200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A104460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A104480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A104540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A104620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A104700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A104830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A105330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A105550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A105560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A105630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A105740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A105840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A106080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A106190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A107590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A108230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A108320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A108380\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A108490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A108670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A108860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A109070\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "item_Code: A109080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A109610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A109740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A109820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A109860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A109960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A110790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A110990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A111110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A111710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A111770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A112040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A112610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A113810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A114090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A114190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A114450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A114630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A114810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A115160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A115180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A115310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A115390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A115440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A115450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A115480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A115500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A115610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A115960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A117580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A117730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A118000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A118990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A119500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A119610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A119650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A119830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A119850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A119860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A120030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A120110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A120240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A121440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A121600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A121800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A121850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A122310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A122350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A122450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A122640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A122690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A122870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A122900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A122990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A123010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A123040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A123410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A123420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A123570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A123690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A123700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A123750\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A123860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A123890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A124500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A124560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A125210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A126340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A126560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A126600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A126640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A126700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A126880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A127120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A127710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A128540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A128660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A128820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A128940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A129260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A129890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A130500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A130580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A130660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A131030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A131090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A131100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A131220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A131290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A131370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A131400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A131760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A131970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A133750\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A133820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A134380\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A134580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A134790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A136480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A136490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A136510\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A136540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A137400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A137950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A138040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A138070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A138080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A138490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A138580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A138610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A138930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A139050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A139130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A139480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A139670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A140070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A140410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A140520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A140670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A140860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A141000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A141080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A142210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A142280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A142760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A143160\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A143210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A143240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A143540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A144510\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A144960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A145020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A145210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A145720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A145990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A147760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A147830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A148150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A148250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A149950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A149980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A150840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A150900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A151860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A151910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A153460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A153490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A153710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A155650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A155660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A156100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A158430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A159580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A159910\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A160550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A160600\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "item_Code: A160980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A161000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A161390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A161580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A161890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A163560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A163730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A164060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A166090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A166480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A168330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A169330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A170030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A170790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A170900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A170920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A171010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A171090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A171120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A173130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A173940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A174900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A175140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A175250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A175330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A177350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A177830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A178320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A178780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A178920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A179290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A179900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A180400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A180640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A181340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A181710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A182360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A182400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A183190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A183300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A183490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A184230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A185490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A185750\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A186230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A187220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A187270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A187420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A187870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A189300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A189330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A189690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A189860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A189980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A190510\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A190650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A191420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A192080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A192250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A192400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A192410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A192440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A192650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A192820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A193250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A194370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A194480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A194700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A195500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A195870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A195990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A196170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A196300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A196450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A196490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A197140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A198440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A199800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A199820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A200130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A200230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A200350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A200470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A200670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A200710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A200780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A200880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A201490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A203400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A203450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A203650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A203690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A204020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A204270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A204320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A204620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A204840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A205100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A205470\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A205500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A206400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A206560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A206640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A206650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A207760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A207940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A208140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A208340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A208370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A208640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A210540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A210980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A211050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A211270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A212560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A213420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A213500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A214870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A215000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A215090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A215100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A215200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A215360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A215600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A215790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A216050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A216080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A217190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A217270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A217330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A217500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A217620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A217730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A217820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A218150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A218410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A219130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A219420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A219550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A220100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A220180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A220260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A221840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A221980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A222040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A222080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A222110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A222420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A222800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A222810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A222980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A223250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A223310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A224110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A225190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A225220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A225530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A225570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A226320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A226330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A226340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A226400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A226440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A226950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A227100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A227840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A227950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A228340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A228670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A228760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A228850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A229000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A229640\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A230240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A230360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A230980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A232140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A232680\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A234080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A234100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A234300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A234340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A234690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A234920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A235980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A236200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A236810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A237690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A237820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A237880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A238090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A238120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A238200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A238490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A239340\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A239610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A239890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A240810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A241520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A241560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A241590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A241690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A241710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A241770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A241790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A241820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A241840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A242040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A243070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A243840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A244460\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A244920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A245620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A246690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A246710\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A246720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A246960\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A247540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A247660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A248070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A248170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A249420\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A250000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A250060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A251270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A251370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A251630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A251970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A252500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A252990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A253450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A253590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A253840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A254120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A255220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A255440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A256150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A256630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A256840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A256940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A257370\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------\n",
            "item_Code: A258610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A258830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A259630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A260660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A260930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A260970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A261200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A262260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A262840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A263020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A263050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A263540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A263600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A263690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A263700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A263720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A263750\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A263770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A263800\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A263810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A263860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A263920\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A264450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A264660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A264850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A264900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A265520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A265560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A265740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A267250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A267260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A267270\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A267290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A267320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A267790\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A267850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A267980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A268280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A268600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A269620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A270520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A270660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A270870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A271560\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A271980\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A272110\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A272210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A272290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A272450\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A272550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A273060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A274090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A276730\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A277070\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A277410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A277810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A277880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A278280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A278650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A279600\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A280360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A281740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A281820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A282330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A282690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A282880\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A284620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A284740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A285130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A285490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A286750\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A286940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A287410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A288330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A288620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A289010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A289080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A289220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A290120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A290380\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A290520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A290550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A290650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A290670\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A290690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A290720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A290740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A291230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A291650\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A293480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A293490\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A293580\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A293780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A294090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A294140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A294570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A294630\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A294870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A297090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A297570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A297890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A298000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A298020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A298040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A298050\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A298060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A298380\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A298540\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A298690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A299030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A299170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A299660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A299900\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A300080\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A300120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A300720\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A301300\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A302430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A302440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A302550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A303030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A303360\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A304100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A304840\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A305090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A306040\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A306200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A306620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A307180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A307280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A307750\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A307870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A307930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A307950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A308100\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A308170\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A309930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A310200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A311390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A311690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A312610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A313760\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A314130\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A314930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A316140\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A317120\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A317240\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A317330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A317400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A317530\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A317690\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A317770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A317830\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A317850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A317870\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A318000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A318010\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A318020\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A318410\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A319400\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A319660\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A320000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A321260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A321550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A322000\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A322180\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A322310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A322510\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A322780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A323280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A323990\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A326030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A327260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A330350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A330860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A331520\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A332290\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A332370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A332570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A333430\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A333620\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A334970\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A335810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A335890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A336060\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A336260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A336370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A336570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A337930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A338220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A339770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A339950\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A340440\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A340570\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A340930\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A344820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A347700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A347740\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A347770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A347860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A347890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A348030\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A348150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A348210\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A348350\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A351320\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A351330\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A352480\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A352700\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A352770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A352820\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A352940\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A353190\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A353200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A353810\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A354200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A355150\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A356860\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A356890\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A357230\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A357550\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A357780\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A359090\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A361390\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A361610\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A363250\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A363260\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A363280\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A365590\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A368770\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A369370\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A373200\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A375500\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A378850\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A383220\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A383310\n",
            "--------------------\n",
            "--------------------\n",
            "item_Code: A383800\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Arima\n",
        "stacked_denoised_data = []\n",
        "\n",
        "for item_Code in train['item_Code'].unique().tolist():\n",
        "\n",
        "  print('-' * 20)\n",
        "  print(f'item_Code: {item_Code}')\n",
        "  print('-' * 20)\n",
        "\n",
        "  data = train[train['item_Code'] == item_Code]\n",
        "  data['Date'] = pd.to_datetime(data['Date'], format='%Y%m%d')\n",
        "  data.set_index('Date', inplace = True)\n",
        "  data.index = data.index.to_period('D')\n",
        "  tc = data['Close']\n",
        "\n",
        "  model = ARIMA(tc, order = (0, 1, 1)) # (5, 1, 0) (2, 1, 2)\n",
        "  model_fit = model.fit()\n",
        "\n",
        "  #print(model_fit.summary())\n",
        "\n",
        "  output = model_fit.predict()\n",
        "  output[0] = tc[0]\n",
        "  #print(output.shape)\n",
        "  #print(tc)\n",
        "  #print(output)\n",
        "\n",
        "  stacked_denoised_data.append(output)\n",
        "stacked_denoised_data = np.concatenate(stacked_denoised_data, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWDvJZZYLexE",
        "outputId": "eefb7449-2496-4ace-cc8c-100f334ca175"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          14600\n",
              "1          14500\n",
              "2          14600\n",
              "3          14700\n",
              "4          15150\n",
              "           ...  \n",
              "1085995     8230\n",
              "1085996     8230\n",
              "1085997     8230\n",
              "1085998     8230\n",
              "1085999     8230\n",
              "Name: Close, Length: 1086000, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "train['Close']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e--Dc-wTCtSf",
        "outputId": "92963780-b707-41e2-bafe-e5179b841290"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14600.        , 14234.06712611, 14440.36208249, ...,\n",
              "        8229.99999687,  8230.00000018,  8229.99999999])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "stacked_denoised_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QhKvgi6LX7G",
        "outputId": "59fd2021-156f-48e5-e639-d3fa79b65db2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14600.        , 14234.06712611, 14440.36208249, ...,\n",
              "        8229.99999687,  8230.00000018,  8229.99999999])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "np.save('stacked_denoised_arima.npy', stacked_denoised_data)\n",
        "np.savetxt('stacked_denoised_arima.txt', stacked_denoised_data)\n",
        "\n",
        "np.load('stacked_denoised_arima.npy')\n",
        "np.loadtxt('stacked_denoised_arima.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqN70oS8Ld4n"
      },
      "outputs": [],
      "source": [
        "train['ARIMA_close'] = stacked_denoised_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDOBFlZliBEO",
        "outputId": "096db1a3-ff1c-4550-f9c8-11e8faad6a8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE\n",
            "fft30: 64456.13379253148\n",
            "wt_db4: 1148.9728500551983\n",
            "wt_db5: 0.6136788319744778\n",
            "wt_db7: 0.6130982811551873\n",
            "wt_db9: 0.6037773511208163\n",
            "AE_close: 173.43731123254327\n",
            "wt_haar_hard: 1122.859398767858\n",
            "wt_db4_soft: 929.2606039371385\n",
            "MAE\n",
            "fft30: 27780.62126965052\n",
            "wt_db4: 321.37102786554976\n",
            "wt_db5: 0.5026797986721024\n",
            "wt_db7: 0.498427978389635\n",
            "wt_db9: 0.48884582060307546\n",
            "AE_close: 12.168059314974888\n",
            "wt_haar_hard: 329.115214068826\n",
            "wt_db4_soft: 283.4337018863936\n",
            "MAPE\n",
            "fft30: 496.51851793241696\n",
            "wt_db4: 1.3684707972213275\n",
            "wt_db5: 0.011082395480888237\n",
            "wt_db7: 0.011010036971094558\n",
            "wt_db9: 0.010796483376951944\n",
            "AE_close: 0.052729876307533535\n",
            "wt_haar_hard: 1.3804305108751287\n",
            "wt_db4_soft: 1.214271959707792\n"
          ]
        }
      ],
      "source": [
        "metrics = ['fft30', 'wt_db4', 'wt_db5', 'wt_db7', 'wt_db9', 'AE_close', 'wt_haar_hard', 'wt_db4_soft']\n",
        "rmses = [RMSE(train['Close'], train['fft30']), RMSE(train['Close'], train['wt_db4']), RMSE(train['Close'], train['wt_db5']), RMSE(train['Close'], train['wt_db7']), RMSE(train['Close'], train['wt_db9']), RMSE(train['Close'], train['AE_close']), RMSE(train['Close'], train['wt_haar_hard']), RMSE(train['Close'], train['wt_db4_soft'])]\n",
        "maes = [MAE(train['Close'], train['fft30']), MAE(train['Close'], train['wt_db4']), MAE(train['Close'], train['wt_db5']), MAE(train['Close'], train['wt_db7']), MAE(train['Close'], train['wt_db9']), MAE(train['Close'], train['AE_close']), MAE(train['Close'], train['wt_haar_hard']), MAE(train['Close'], train['wt_db4_soft'])]\n",
        "mapes = [MAPE(train['Close'], train['fft30']), MAPE(train['Close'], train['wt_db4']), MAPE(train['Close'], train['wt_db5']), MAPE(train['Close'], train['wt_db7']), MAPE(train['Close'], train['wt_db9']), MAPE(train['Close'], train['AE_close']), MAPE(train['Close'], train['wt_haar_hard']), MAPE(train['Close'], train['wt_db4_soft'])]\n",
        "print('RMSE')\n",
        "print(f'{metrics[0]}:', rmses[0])\n",
        "print(f'{metrics[1]}:', rmses[1])\n",
        "print(f'{metrics[2]}:', rmses[2])\n",
        "print(f'{metrics[3]}:', rmses[3])\n",
        "print(f'{metrics[4]}:', rmses[4])\n",
        "print(f'{metrics[-3]}:', rmses[-3])\n",
        "print(f'{metrics[-2]}:', rmses[-2])\n",
        "print(f'{metrics[-1]}:', rmses[-1])\n",
        "\n",
        "print('MAE')\n",
        "print(f'{metrics[0]}:', maes[0])\n",
        "print(f'{metrics[1]}:', maes[1])\n",
        "print(f'{metrics[2]}:', maes[2])\n",
        "print(f'{metrics[3]}:', maes[3])\n",
        "print(f'{metrics[4]}:', maes[4])\n",
        "print(f'{metrics[-3]}:', maes[-3])\n",
        "print(f'{metrics[-2]}:', maes[-2])\n",
        "print(f'{metrics[-1]}:', maes[-1])\n",
        "\n",
        "print('MAPE')\n",
        "print(f'{metrics[0]}:', mapes[0])\n",
        "print(f'{metrics[1]}:', mapes[1])\n",
        "print(f'{metrics[2]}:', mapes[2])\n",
        "print(f'{metrics[3]}:', mapes[3])\n",
        "print(f'{metrics[4]}:', mapes[4])\n",
        "print(f'{metrics[-3]}:', mapes[-3])\n",
        "print(f'{metrics[-2]}:', mapes[-2])\n",
        "print(f'{metrics[-1]}:', mapes[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbUTHwsGLrlG",
        "outputId": "d018cccb-31c4-46a8-d8a1-99c1205710b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE\n",
            "ARIMA_close: 2608.2184106590034\n",
            "MAE\n",
            "ARIMA_close: 471.5799166365837\n",
            "MAPE\n",
            "ARIMA_close: inf\n"
          ]
        }
      ],
      "source": [
        "metrics = ['ARIMA_close']\n",
        "rmses = [RMSE(train['Close'], train['ARIMA_close'])]\n",
        "maes = [MAE(train['Close'], train['ARIMA_close'])]\n",
        "mapes = [MAPE(train['Close'], train['ARIMA_close'])]\n",
        "print('RMSE')\n",
        "print(f'{metrics[-1]}:', rmses[-1])\n",
        "\n",
        "print('MAE')\n",
        "print(f'{metrics[-1]}:', maes[-1])\n",
        "\n",
        "print('MAPE')\n",
        "print(f'{metrics[-1]}:', mapes[-1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train['ARIMA_close'] = np.round(train['ARIMA_close'])"
      ],
      "metadata": {
        "id": "yfylgtgDYez5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PM67Y2j1dW-Y",
        "outputId": "0b8f2f02-d725-42e1-ae61-579957bf7265"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'wt_db9'"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp = np.sum([rmses,maes,mapes], axis = 0)\n",
        "metrics[temp.argmin()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zIhbvjBmdPm"
      },
      "source": [
        "# **기능 중요도**\n",
        "\n",
        "- 상관자산, 기술지표, 기본 분석, 푸리에, 웨이블릿, 아리마, 오토인코더 등"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Trrri04Ugo-V"
      },
      "outputs": [],
      "source": [
        "train.drop(['fft30', 'wt_db4', 'wt_db5', 'wt_db7', 'AE_close'], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "dtaBVcrYjz0E",
        "outputId": "d9b1470d-39ad-498b-8169-6e118bca9110"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c6b60c55-3d50-478a-b634-7455c292f41d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>item_Code</th>\n",
              "      <th>item_Name</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>ma5</th>\n",
              "      <th>ma7</th>\n",
              "      <th>...</th>\n",
              "      <th>au</th>\n",
              "      <th>ad</th>\n",
              "      <th>rsi</th>\n",
              "      <th>rsi_Signal</th>\n",
              "      <th>volume_price</th>\n",
              "      <th>volume_price_sum</th>\n",
              "      <th>volume_sum</th>\n",
              "      <th>vwap</th>\n",
              "      <th>diff</th>\n",
              "      <th>wt_db9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>114966</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1690000200</td>\n",
              "      <td>1690000200</td>\n",
              "      <td>114966</td>\n",
              "      <td>14700.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14599.063047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20210602</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>109559</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1610517300</td>\n",
              "      <td>3300517500</td>\n",
              "      <td>224525</td>\n",
              "      <td>14700.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14499.344630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20210603</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>96158</td>\n",
              "      <td>14550</td>\n",
              "      <td>14650</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1399098900</td>\n",
              "      <td>4699616400</td>\n",
              "      <td>320683</td>\n",
              "      <td>14655.021938</td>\n",
              "      <td>-105.021938</td>\n",
              "      <td>14599.757525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20210604</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>133900</td>\n",
              "      <td>14600</td>\n",
              "      <td>14800</td>\n",
              "      <td>14550</td>\n",
              "      <td>14700</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1954940000</td>\n",
              "      <td>6654556400</td>\n",
              "      <td>454583</td>\n",
              "      <td>14638.814914</td>\n",
              "      <td>-38.814914</td>\n",
              "      <td>14699.270109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20210607</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>511140</td>\n",
              "      <td>14800</td>\n",
              "      <td>15550</td>\n",
              "      <td>14750</td>\n",
              "      <td>15150</td>\n",
              "      <td>14710.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7564872000</td>\n",
              "      <td>14219428400</td>\n",
              "      <td>965723</td>\n",
              "      <td>14724.127312</td>\n",
              "      <td>75.872688</td>\n",
              "      <td>15148.996570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987995</th>\n",
              "      <td>20230523</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>150364</td>\n",
              "      <td>8390</td>\n",
              "      <td>8390</td>\n",
              "      <td>8310</td>\n",
              "      <td>8330</td>\n",
              "      <td>8354.0</td>\n",
              "      <td>8354.285714</td>\n",
              "      <td>...</td>\n",
              "      <td>10.714286</td>\n",
              "      <td>25.714286</td>\n",
              "      <td>29.411765</td>\n",
              "      <td>33.866817</td>\n",
              "      <td>1261553960</td>\n",
              "      <td>1425993825510</td>\n",
              "      <td>144805856</td>\n",
              "      <td>9847.625399</td>\n",
              "      <td>-1457.625399</td>\n",
              "      <td>8331.201210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987996</th>\n",
              "      <td>20230524</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>122457</td>\n",
              "      <td>8310</td>\n",
              "      <td>8340</td>\n",
              "      <td>8280</td>\n",
              "      <td>8300</td>\n",
              "      <td>8338.0</td>\n",
              "      <td>8347.142857</td>\n",
              "      <td>...</td>\n",
              "      <td>9.285714</td>\n",
              "      <td>27.857143</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>32.477928</td>\n",
              "      <td>1017617670</td>\n",
              "      <td>1427011443180</td>\n",
              "      <td>144928313</td>\n",
              "      <td>9846.326185</td>\n",
              "      <td>-1536.326185</td>\n",
              "      <td>8300.986750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987997</th>\n",
              "      <td>20230525</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>84241</td>\n",
              "      <td>8300</td>\n",
              "      <td>8310</td>\n",
              "      <td>8270</td>\n",
              "      <td>8310</td>\n",
              "      <td>8330.0</td>\n",
              "      <td>8340.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>7.857143</td>\n",
              "      <td>27.857143</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>30.922372</td>\n",
              "      <td>699200300</td>\n",
              "      <td>1427710643480</td>\n",
              "      <td>145012554</td>\n",
              "      <td>9845.427890</td>\n",
              "      <td>-1545.427890</td>\n",
              "      <td>8310.174317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987998</th>\n",
              "      <td>20230526</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>126681</td>\n",
              "      <td>8300</td>\n",
              "      <td>8310</td>\n",
              "      <td>8270</td>\n",
              "      <td>8280</td>\n",
              "      <td>8316.0</td>\n",
              "      <td>8325.714286</td>\n",
              "      <td>...</td>\n",
              "      <td>3.571429</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>10.638298</td>\n",
              "      <td>28.400702</td>\n",
              "      <td>1051452300</td>\n",
              "      <td>1428762095780</td>\n",
              "      <td>145139235</td>\n",
              "      <td>9844.079003</td>\n",
              "      <td>-1544.079003</td>\n",
              "      <td>8281.043452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987999</th>\n",
              "      <td>20230530</td>\n",
              "      <td>A383800</td>\n",
              "      <td>LX홀딩스</td>\n",
              "      <td>70489</td>\n",
              "      <td>8300</td>\n",
              "      <td>8300</td>\n",
              "      <td>8270</td>\n",
              "      <td>8290</td>\n",
              "      <td>8302.0</td>\n",
              "      <td>8317.142857</td>\n",
              "      <td>...</td>\n",
              "      <td>4.285714</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>14.634146</td>\n",
              "      <td>26.043490</td>\n",
              "      <td>585058700</td>\n",
              "      <td>1429347154480</td>\n",
              "      <td>145209724</td>\n",
              "      <td>9843.329462</td>\n",
              "      <td>-1543.329462</td>\n",
              "      <td>8291.496903</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>988000 rows × 38 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6b60c55-3d50-478a-b634-7455c292f41d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c6b60c55-3d50-478a-b634-7455c292f41d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c6b60c55-3d50-478a-b634-7455c292f41d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            Date item_Code item_Name  Volume   Open   High    Low  Close  \\\n",
              "0       20210601   A000020      동화약품  114966  14700  14700  14450  14600   \n",
              "1       20210602   A000020      동화약품  109559  14700  14700  14450  14500   \n",
              "2       20210603   A000020      동화약품   96158  14550  14650  14450  14600   \n",
              "3       20210604   A000020      동화약품  133900  14600  14800  14550  14700   \n",
              "4       20210607   A000020      동화약품  511140  14800  15550  14750  15150   \n",
              "...          ...       ...       ...     ...    ...    ...    ...    ...   \n",
              "987995  20230523   A383800     LX홀딩스  150364   8390   8390   8310   8330   \n",
              "987996  20230524   A383800     LX홀딩스  122457   8310   8340   8280   8300   \n",
              "987997  20230525   A383800     LX홀딩스   84241   8300   8310   8270   8310   \n",
              "987998  20230526   A383800     LX홀딩스  126681   8300   8310   8270   8280   \n",
              "987999  20230530   A383800     LX홀딩스   70489   8300   8300   8270   8290   \n",
              "\n",
              "            ma5          ma7  ...         au         ad        rsi  \\\n",
              "0           NaN          NaN  ...        NaN        NaN        NaN   \n",
              "1           NaN          NaN  ...        NaN        NaN        NaN   \n",
              "2           NaN          NaN  ...        NaN        NaN        NaN   \n",
              "3           NaN          NaN  ...        NaN        NaN        NaN   \n",
              "4       14710.0          NaN  ...        NaN        NaN        NaN   \n",
              "...         ...          ...  ...        ...        ...        ...   \n",
              "987995   8354.0  8354.285714  ...  10.714286  25.714286  29.411765   \n",
              "987996   8338.0  8347.142857  ...   9.285714  27.857143  25.000000   \n",
              "987997   8330.0  8340.000000  ...   7.857143  27.857143  22.000000   \n",
              "987998   8316.0  8325.714286  ...   3.571429  30.000000  10.638298   \n",
              "987999   8302.0  8317.142857  ...   4.285714  25.000000  14.634146   \n",
              "\n",
              "        rsi_Signal  volume_price  volume_price_sum  volume_sum          vwap  \\\n",
              "0              NaN    1690000200        1690000200      114966  14700.000000   \n",
              "1              NaN    1610517300        3300517500      224525  14700.000000   \n",
              "2              NaN    1399098900        4699616400      320683  14655.021938   \n",
              "3              NaN    1954940000        6654556400      454583  14638.814914   \n",
              "4              NaN    7564872000       14219428400      965723  14724.127312   \n",
              "...            ...           ...               ...         ...           ...   \n",
              "987995   33.866817    1261553960     1425993825510   144805856   9847.625399   \n",
              "987996   32.477928    1017617670     1427011443180   144928313   9846.326185   \n",
              "987997   30.922372     699200300     1427710643480   145012554   9845.427890   \n",
              "987998   28.400702    1051452300     1428762095780   145139235   9844.079003   \n",
              "987999   26.043490     585058700     1429347154480   145209724   9843.329462   \n",
              "\n",
              "               diff        wt_db9  \n",
              "0          0.000000  14599.063047  \n",
              "1          0.000000  14499.344630  \n",
              "2       -105.021938  14599.757525  \n",
              "3        -38.814914  14699.270109  \n",
              "4         75.872688  15148.996570  \n",
              "...             ...           ...  \n",
              "987995 -1457.625399   8331.201210  \n",
              "987996 -1536.326185   8300.986750  \n",
              "987997 -1545.427890   8310.174317  \n",
              "987998 -1544.079003   8281.043452  \n",
              "987999 -1543.329462   8291.496903  \n",
              "\n",
              "[988000 rows x 38 columns]"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKA41Hkvjlbx"
      },
      "outputs": [],
      "source": [
        "train['NASDAQCOM'] = pd.to_numeric(train['NASDAQCOM'], errors='coerce')\n",
        "train['BAMLH0A0HYM2EY'] = pd.to_numeric(train['BAMLH0A0HYM2EY'], errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AVDvn50JdJI7"
      },
      "outputs": [],
      "source": [
        "def train_test_split(dataset, close_name):\n",
        "  dataset = dataset.copy()\n",
        "  y = dataset[close_name]\n",
        "  X = dataset.drop([close_name], axis = 1)\n",
        "  train_samples = int(np.ceil(0.8 * X.shape[0])) #int(X.shape[0] * 0.75)\n",
        "\n",
        "  X_train = X.iloc[:train_samples]\n",
        "  X_test = X.iloc[train_samples:]\n",
        "  y_train = y.iloc[:train_samples]\n",
        "  y_test = y.iloc[train_samples:]\n",
        "\n",
        "  return (X_train, y_train), (X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "Sxw4VYxkCaO2",
        "outputId": "49df69bc-c493-44a6-9d18-83d62caf5f27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Date item_Code item_Name  Volume   Open   High    Low  Close  Date_ym  \\\n",
              "0  20210601   A000020      동화약품  114966  14700  14700  14450  14600   202106   \n",
              "1  20210602   A000020      동화약품  109559  14700  14700  14450  14500   202106   \n",
              "2  20210603   A000020      동화약품   96158  14550  14650  14450  14600   202106   \n",
              "3  20210604   A000020      동화약품  133900  14600  14800  14550  14700   202106   \n",
              "4  20210607   A000020      동화약품  511140  14800  15550  14750  15150   202106   \n",
              "\n",
              "   Close_usd_krw  ...         SR         WR   AO          KAMA  ROC       PPO  \\\n",
              "0    1106.979980  ...  60.000000 -40.000000  0.0  14600.000000  0.0  0.000000   \n",
              "1    1107.209961  ...  20.000000 -80.000000  0.0  14599.583767  0.0 -0.054666   \n",
              "2    1109.880005  ...  60.000000 -40.000000  0.0  14599.585499  0.0 -0.042205   \n",
              "3    1115.439941  ...  71.428571 -28.571429  0.0  14600.003458  0.0  0.022689   \n",
              "4    1109.680054  ...  63.636364 -36.363636  0.0  14602.292725  0.0  0.318224   \n",
              "\n",
              "         PVO  Daily_Range     Mean   ARIMA_close  \n",
              "0   0.000000         -100  14575.0  14600.000000  \n",
              "1  -0.376490         -200  14575.0  14234.067126  \n",
              "2  -1.619559           50  14550.0  14440.362082  \n",
              "3   0.086992          100  14675.0  14562.525683  \n",
              "4  22.003352          350  15150.0  14667.648856  \n",
              "\n",
              "[5 rows x 96 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-a9a0465f-7020-4953-b63e-30bc4de9fc52\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>item_Code</th>\n",
              "      <th>item_Name</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Date_ym</th>\n",
              "      <th>Close_usd_krw</th>\n",
              "      <th>...</th>\n",
              "      <th>SR</th>\n",
              "      <th>WR</th>\n",
              "      <th>AO</th>\n",
              "      <th>KAMA</th>\n",
              "      <th>ROC</th>\n",
              "      <th>PPO</th>\n",
              "      <th>PVO</th>\n",
              "      <th>Daily_Range</th>\n",
              "      <th>Mean</th>\n",
              "      <th>ARIMA_close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>114966</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>202106</td>\n",
              "      <td>1106.979980</td>\n",
              "      <td>...</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>-40.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14600.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-100</td>\n",
              "      <td>14575.0</td>\n",
              "      <td>14600.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20210602</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>109559</td>\n",
              "      <td>14700</td>\n",
              "      <td>14700</td>\n",
              "      <td>14450</td>\n",
              "      <td>14500</td>\n",
              "      <td>202106</td>\n",
              "      <td>1107.209961</td>\n",
              "      <td>...</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>-80.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14599.583767</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.054666</td>\n",
              "      <td>-0.376490</td>\n",
              "      <td>-200</td>\n",
              "      <td>14575.0</td>\n",
              "      <td>14234.067126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20210603</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>96158</td>\n",
              "      <td>14550</td>\n",
              "      <td>14650</td>\n",
              "      <td>14450</td>\n",
              "      <td>14600</td>\n",
              "      <td>202106</td>\n",
              "      <td>1109.880005</td>\n",
              "      <td>...</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>-40.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14599.585499</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.042205</td>\n",
              "      <td>-1.619559</td>\n",
              "      <td>50</td>\n",
              "      <td>14550.0</td>\n",
              "      <td>14440.362082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20210604</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>133900</td>\n",
              "      <td>14600</td>\n",
              "      <td>14800</td>\n",
              "      <td>14550</td>\n",
              "      <td>14700</td>\n",
              "      <td>202106</td>\n",
              "      <td>1115.439941</td>\n",
              "      <td>...</td>\n",
              "      <td>71.428571</td>\n",
              "      <td>-28.571429</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14600.003458</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.022689</td>\n",
              "      <td>0.086992</td>\n",
              "      <td>100</td>\n",
              "      <td>14675.0</td>\n",
              "      <td>14562.525683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20210607</td>\n",
              "      <td>A000020</td>\n",
              "      <td>동화약품</td>\n",
              "      <td>511140</td>\n",
              "      <td>14800</td>\n",
              "      <td>15550</td>\n",
              "      <td>14750</td>\n",
              "      <td>15150</td>\n",
              "      <td>202106</td>\n",
              "      <td>1109.680054</td>\n",
              "      <td>...</td>\n",
              "      <td>63.636364</td>\n",
              "      <td>-36.363636</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14602.292725</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.318224</td>\n",
              "      <td>22.003352</td>\n",
              "      <td>350</td>\n",
              "      <td>15150.0</td>\n",
              "      <td>14667.648856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 96 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a9a0465f-7020-4953-b63e-30bc4de9fc52')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-659bea55-632d-480c-928c-dd9a92faff6e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-659bea55-632d-480c-928c-dd9a92faff6e')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-659bea55-632d-480c-928c-dd9a92faff6e button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a9a0465f-7020-4953-b63e-30bc4de9fc52 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a9a0465f-7020-4953-b63e-30bc4de9fc52');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG5FOQ26Av4b"
      },
      "outputs": [],
      "source": [
        "train.to_parquet('full_train.parquet')\n",
        "#train = pd.read_parquet('full_train.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWN8WQq_hxH8"
      },
      "outputs": [],
      "source": [
        "fi_list = []\n",
        "\n",
        "for item_Code in train['item_Code'].unique().tolist():\n",
        "\n",
        "  print('-' * 20)\n",
        "  print(f'item_Code: {item_Code}')\n",
        "  print('-' * 20)\n",
        "\n",
        "  item_set = train[train['item_Code'] == item_Code]\n",
        "  item_set = item_set.drop(['Date', 'item_Code', 'item_Name', 'ARIMA_close'], axis = 1)\n",
        "  (X_train_fi, y_train_fi), (X_test_fi, y_test_fi) = train_test_split(item_set)\n",
        "  regressor = xgb.XGBRegressor() # gamma=0.0,n_estimators=150,base_score=0.7,colsample_bytree=1,learning_rate=0.05\n",
        "  regressor = regressor.fit(X_train_fi, y_train_fi, eval_set = [(X_train_fi, y_train_fi), (X_test_fi, y_test_fi)], verbose = False)\n",
        "\n",
        "  #fi_mean = np.mean(regressor.feature_importances_)\n",
        "  #print(dict(zip(item_set.columns, regressor.feature_importances_)))\n",
        "  fi = X_train_fi.columns[np.array(regressor.feature_importances_) > 0.0]\n",
        "  print('len:', len(fi))\n",
        "  print(fi)\n",
        "  fi_list.append(fi)\n",
        "\n",
        "  # eval_result = regressor.evals_result()\n",
        "  # training_rounds = range(len(eval_result['validation_0']['rmse']))\n",
        "\n",
        "  # fig = plt.figure(figsize=(15,7))\n",
        "\n",
        "  # plt.subplot(2, 1, 1)\n",
        "  # plt.scatter(x=training_rounds,y=eval_result['validation_0']['rmse'],label='Training Error')\n",
        "  # plt.scatter(x=training_rounds,y=eval_result['validation_1']['rmse'],label='Validation Error')\n",
        "  # plt.xlabel('Iterations')\n",
        "  # plt.ylabel('RMSE')\n",
        "  # plt.title('Training Vs Validation Error')\n",
        "  # plt.legend()\n",
        "\n",
        "  # plt.subplot(2, 1, 2)\n",
        "  # plt.xticks(rotation='vertical')\n",
        "  # plt.bar([i for i in range(len(regressor.feature_importances_))], regressor.feature_importances_.tolist(), tick_label=X_test_fi.columns)\n",
        "  # plt.title('Figure 6: Feature importance of the technical indicators.')\n",
        "  # plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc-ZHPK5s2uO"
      },
      "source": [
        "# Stacked AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kzzhe66Ms2e2"
      },
      "outputs": [],
      "source": [
        "class Stacked_Autoencoder(nn.Module):\n",
        "    def __init__(self, n_in, n_hidden = 10):\n",
        "      super(Stacked_Autoencoder, self).__init__()\n",
        "      self.n_in = n_in\n",
        "      self.n_hidden = n_hidden\n",
        "      self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "      self.encoder = nn.Sequential(\n",
        "          nn.Linear(self.n_in, self.n_hidden),\n",
        "          nn.Sigmoid()\n",
        "      )\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.Linear(self.n_hidden, self.n_in),\n",
        "          #nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      encoder = self.encoder(x)\n",
        "      decoder = self.decoder(encoder)\n",
        "      return decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJgHl89bS2Yz"
      },
      "outputs": [],
      "source": [
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * math.pow(x, 3))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL7Kopj2WHb0"
      },
      "outputs": [],
      "source": [
        "class Stacked_VAE(nn.Module):\n",
        "    def __init__(self, n_in = 784, n_hidden = 400, n_latent = 2):\n",
        "      super(Stacked_VAE, self).__init__()\n",
        "      self.n_in = n_in\n",
        "      self.n_latent = n_latent\n",
        "      self.n_hidden = n_hidden\n",
        "\n",
        "      self.encoder = nn.Sequential(\n",
        "        nn.Linear(n_in, n_hidden),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(n_hidden, n_hidden),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(n_hidden, n_latent * 2)\n",
        "      )\n",
        "\n",
        "      self.decoder = nn.Sequential(\n",
        "          nn.Linear(n_latent, n_hidden),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(n_hidden, n_hidden),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(n_hidden, n_in),\n",
        "          nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "    def encoded(self, x):\n",
        "      h = self.encoder(x)\n",
        "      mu, log_var = torch.chunk(h, 2, dim = 1)\n",
        "      return mu, log_var\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "      std = torch.exp(0.5 * log_var)\n",
        "      eps = torch.randn_like(std)\n",
        "      z = mu + eps * std\n",
        "      return z\n",
        "\n",
        "    def decoded(self, z):\n",
        "      return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "      mu, log_var = self.encoded(x)\n",
        "      z = self.reparameterize(mu, log_var)\n",
        "      y = self.decoded(z)\n",
        "\n",
        "      return y, mu, log_var\n",
        "\n",
        "def vae_loss(recon_x, x, mu, log_var):\n",
        "  BCE =  F.binary_cross_entropy(recon_x, x, reduction='sum') #nn.MSELoss()(recon_x, x) #F.mse_loss(recon_x, x, reduction='sum') # binary_cross_entropy\n",
        "  KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "  return BCE + KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozUp7A53_-vY"
      },
      "outputs": [],
      "source": [
        "class Stacked_VAE(nn.Module):\n",
        "    def __init__(self, n_in=784, n_hidden=400, n_latent=2, n_layers=1):\n",
        "      super(Stacked_VAE, self).__init__()\n",
        "      self.soft_zero = 1e-10\n",
        "      self.n_latent = n_latent\n",
        "      self.n_in = n_in\n",
        "      self.mu = None\n",
        "\n",
        "      encoder_layers = []\n",
        "      for i in range(n_layers):\n",
        "        in_features = n_in if i == 0 else n_hidden\n",
        "        out_features = n_latent *2 if i == n_layers - 1 else n_hidden\n",
        "        encoder_layers.append(nn.Linear(in_features, out_features))\n",
        "        if i < n_layers - 1:\n",
        "          encoder_layers.append(nn.ReLU())\n",
        "      #encoder_layers.append(nn.ReLU())\n",
        "      self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "      decoder_layers = []\n",
        "      for i in range(n_layers):\n",
        "        in_features = n_latent if i == 0 else n_hidden\n",
        "        out_features = n_in if i == n_layers - 1 else n_hidden\n",
        "        decoder_layers.append(nn.Linear(in_features, out_features))\n",
        "        if i < n_layers - 1:\n",
        "          decoder_layers.append(nn.ReLU())\n",
        "      decoder_layers.append(nn.Sigmoid())\n",
        "      self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    def encoded(self, x):\n",
        "      h = self.encoder(x)\n",
        "      mu_lv = torch.split(h, self.n_latent, dim=1)\n",
        "      return mu_lv[0], mu_lv[1]\n",
        "\n",
        "    def decoded(self, z):\n",
        "      return self.decoder(z)\n",
        "\n",
        "    def reparameterize(self, mu, lv):\n",
        "      eps = torch.randn_like(mu)\n",
        "      z = mu + torch.exp(0.5 * lv) * eps\n",
        "      return z\n",
        "\n",
        "    def forward(self, x):\n",
        "      mu, lv = self.encoded(x)\n",
        "      z = self.reparameterize(mu, lv)\n",
        "      y = self.decoded(z)\n",
        "\n",
        "      KL = 0.5 * torch.sum(1 + lv - mu * mu - lv.exp(), dim=1)\n",
        "      logloss = torch.sum(x * torch.log(y + self.soft_zero) + (1 - x) * torch.log(1 - y + self.soft_zero), dim=1)\n",
        "      loss = -logloss - KL\n",
        "\n",
        "      weight_loss = loss.unsqueeze(1)\n",
        "\n",
        "      return y, weight_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSTJ1dw2PNJ6",
        "outputId": "d340c9cd-a66d-45bf-906d-96ace6ad6ad7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1086000, 96)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knBqNTj7z7lZ"
      },
      "outputs": [],
      "source": [
        "train['Close2'] = train['Close'] # close or use arima close\n",
        "train.drop(['Close'], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHEir3YdCeG7"
      },
      "source": [
        "standard scaling\n",
        "close2\n",
        "n_epoch = 150\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "n_hidden = 32\n",
        "n_latent = 10\n",
        "n_layers = 3\n",
        "close2\n",
        "Stacked_VAE(n_in = feats_train.shape[1], n_hidden = n_hidden, n_latent = n_latent, n_layers = n_layers) - stacked_vae_filter_data\n",
        "\n",
        "standard scaling\n",
        "close2\n",
        "n_epoch = 150\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "n_hidden = 32\n",
        "n_latent = 10\n",
        "n_layers = 3\n",
        "close2\n",
        "Stacked_VAE(n_in = feats_train.shape[1], n_hidden = n_hidden, n_latent = n_latent) - stacked_vae_filter_data1\n",
        "\n",
        "standard scaling\n",
        "close2\n",
        "n_epoch = 150\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "n_hidden = 32\n",
        "n_latent = 10\n",
        "n_layers = 3\n",
        "close2\n",
        "Stacked_VAE(n_in = feats_train.shape[1], n_hidden = n_hidden, n_latent = n_latent, n_layers = n_layers) - stacked_vae_filter_data2\n",
        "\n",
        "standard scaling\n",
        "close2\n",
        "n_epoch = 150\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "n_hidden = 32\n",
        "n_latent = 10\n",
        "n_layers = 3\n",
        "close2\n",
        "Stacked_VAE(n_in = feats_train.shape[1], n_hidden = n_hidden, n_latent = n_latent) - stacked_vae_filter_data3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "UnBua0hdMWJk",
        "outputId": "b8a37610-dff4-415e-c5d5-a55d194bb770"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Date item_Code item_Name  Volume  Open  High   Low  Date_ym  \\\n",
              "836763  20210601   A180400      DXVX       0     0     0     0   202106   \n",
              "836764  20210602   A180400      DXVX       0     0     0     0   202106   \n",
              "836765  20210603   A180400      DXVX       0     0     0     0   202106   \n",
              "836766  20210604   A180400      DXVX       0     0     0     0   202106   \n",
              "836767  20210607   A180400      DXVX       0     0     0     0   202106   \n",
              "...          ...       ...       ...     ...   ...   ...   ...      ...   \n",
              "837301  20230728   A180400      DXVX  106884  7000  7150  6930   202307   \n",
              "837302  20230728   A180400      DXVX  106884  7000  7150  6930   202307   \n",
              "837303  20230728   A180400      DXVX  106884  7000  7150  6930   202307   \n",
              "837304  20230728   A180400      DXVX  106884  7000  7150  6930   202307   \n",
              "837305  20230728   A180400      DXVX  106884  7000  7150  6930   202307   \n",
              "\n",
              "        Close_usd_krw  Close_usd_eur  ...         WR          AO         KAMA  \\\n",
              "836763    1106.979980       0.817470  ... -50.000000    0.000000  7740.000000   \n",
              "836764    1107.209961       0.818480  ... -50.000000    0.000000  7740.000000   \n",
              "836765    1109.880005       0.819010  ... -50.000000    0.000000  7740.000000   \n",
              "836766    1115.439941       0.824198  ... -50.000000    0.000000  7740.000000   \n",
              "836767    1109.680054       0.821800  ... -50.000000    0.000000  7740.000000   \n",
              "...               ...            ...  ...        ...         ...          ...   \n",
              "837301    1283.880005       0.907500  ... -68.421053 -630.147059  7066.721941   \n",
              "837302    1272.810059       0.910850  ... -68.421053 -580.441176  7066.985325   \n",
              "837303    1272.810059       0.910850  ... -68.421053 -541.470588  7067.247613   \n",
              "837304    1272.810059       0.907500  ... -78.448276 -501.764706  7067.508809   \n",
              "837305    1272.810059       0.907500  ... -78.448276 -464.705882  7067.768918   \n",
              "\n",
              "             ROC       PPO        PVO  Daily_Range    Mean  ARIMA_close  \\\n",
              "836763  0.000000  0.000000   0.000000         7740     0.0  7740.000000   \n",
              "836764  0.000000  0.000000   0.000000         7740     0.0  7716.064017   \n",
              "836765  0.000000  0.000000   0.000000         7740     0.0  7736.440496   \n",
              "836766  0.000000  0.000000   0.000000         7740     0.0  7739.458953   \n",
              "836767  0.000000  0.000000   0.000000         7740     0.0  7739.917719   \n",
              "...          ...       ...        ...          ...     ...          ...   \n",
              "837301  0.140449 -3.016026 -17.197935          130  7040.0  7129.588733   \n",
              "837302 -1.246537 -2.895164 -19.672918          130  7040.0  7129.937455   \n",
              "837303 -0.972222 -2.768100 -21.663651          130  7040.0  7129.990488   \n",
              "837304 -8.354756 -2.637507 -23.222156          130  7040.0  7129.998553   \n",
              "837305 -3.908356 -2.505538 -24.396561          130  7040.0  7129.999780   \n",
              "\n",
              "        Close2  \n",
              "836763    7740  \n",
              "836764    7740  \n",
              "836765    7740  \n",
              "836766    7740  \n",
              "836767    7740  \n",
              "...        ...  \n",
              "837301    7130  \n",
              "837302    7130  \n",
              "837303    7130  \n",
              "837304    7130  \n",
              "837305    7130  \n",
              "\n",
              "[543 rows x 96 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-f6659f48-1126-4581-82b2-66385cc888ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>item_Code</th>\n",
              "      <th>item_Name</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Date_ym</th>\n",
              "      <th>Close_usd_krw</th>\n",
              "      <th>Close_usd_eur</th>\n",
              "      <th>...</th>\n",
              "      <th>WR</th>\n",
              "      <th>AO</th>\n",
              "      <th>KAMA</th>\n",
              "      <th>ROC</th>\n",
              "      <th>PPO</th>\n",
              "      <th>PVO</th>\n",
              "      <th>Daily_Range</th>\n",
              "      <th>Mean</th>\n",
              "      <th>ARIMA_close</th>\n",
              "      <th>Close2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>836763</th>\n",
              "      <td>20210601</td>\n",
              "      <td>A180400</td>\n",
              "      <td>DXVX</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>202106</td>\n",
              "      <td>1106.979980</td>\n",
              "      <td>0.817470</td>\n",
              "      <td>...</td>\n",
              "      <td>-50.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7740.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7740</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7740.000000</td>\n",
              "      <td>7740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836764</th>\n",
              "      <td>20210602</td>\n",
              "      <td>A180400</td>\n",
              "      <td>DXVX</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>202106</td>\n",
              "      <td>1107.209961</td>\n",
              "      <td>0.818480</td>\n",
              "      <td>...</td>\n",
              "      <td>-50.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7740.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7740</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7716.064017</td>\n",
              "      <td>7740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836765</th>\n",
              "      <td>20210603</td>\n",
              "      <td>A180400</td>\n",
              "      <td>DXVX</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>202106</td>\n",
              "      <td>1109.880005</td>\n",
              "      <td>0.819010</td>\n",
              "      <td>...</td>\n",
              "      <td>-50.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7740.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7740</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7736.440496</td>\n",
              "      <td>7740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836766</th>\n",
              "      <td>20210604</td>\n",
              "      <td>A180400</td>\n",
              "      <td>DXVX</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>202106</td>\n",
              "      <td>1115.439941</td>\n",
              "      <td>0.824198</td>\n",
              "      <td>...</td>\n",
              "      <td>-50.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7740.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7740</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7739.458953</td>\n",
              "      <td>7740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836767</th>\n",
              "      <td>20210607</td>\n",
              "      <td>A180400</td>\n",
              "      <td>DXVX</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>202106</td>\n",
              "      <td>1109.680054</td>\n",
              "      <td>0.821800</td>\n",
              "      <td>...</td>\n",
              "      <td>-50.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7740.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7740</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7739.917719</td>\n",
              "      <td>7740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>837301</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A180400</td>\n",
              "      <td>DXVX</td>\n",
              "      <td>106884</td>\n",
              "      <td>7000</td>\n",
              "      <td>7150</td>\n",
              "      <td>6930</td>\n",
              "      <td>202307</td>\n",
              "      <td>1283.880005</td>\n",
              "      <td>0.907500</td>\n",
              "      <td>...</td>\n",
              "      <td>-68.421053</td>\n",
              "      <td>-630.147059</td>\n",
              "      <td>7066.721941</td>\n",
              "      <td>0.140449</td>\n",
              "      <td>-3.016026</td>\n",
              "      <td>-17.197935</td>\n",
              "      <td>130</td>\n",
              "      <td>7040.0</td>\n",
              "      <td>7129.588733</td>\n",
              "      <td>7130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>837302</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A180400</td>\n",
              "      <td>DXVX</td>\n",
              "      <td>106884</td>\n",
              "      <td>7000</td>\n",
              "      <td>7150</td>\n",
              "      <td>6930</td>\n",
              "      <td>202307</td>\n",
              "      <td>1272.810059</td>\n",
              "      <td>0.910850</td>\n",
              "      <td>...</td>\n",
              "      <td>-68.421053</td>\n",
              "      <td>-580.441176</td>\n",
              "      <td>7066.985325</td>\n",
              "      <td>-1.246537</td>\n",
              "      <td>-2.895164</td>\n",
              "      <td>-19.672918</td>\n",
              "      <td>130</td>\n",
              "      <td>7040.0</td>\n",
              "      <td>7129.937455</td>\n",
              "      <td>7130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>837303</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A180400</td>\n",
              "      <td>DXVX</td>\n",
              "      <td>106884</td>\n",
              "      <td>7000</td>\n",
              "      <td>7150</td>\n",
              "      <td>6930</td>\n",
              "      <td>202307</td>\n",
              "      <td>1272.810059</td>\n",
              "      <td>0.910850</td>\n",
              "      <td>...</td>\n",
              "      <td>-68.421053</td>\n",
              "      <td>-541.470588</td>\n",
              "      <td>7067.247613</td>\n",
              "      <td>-0.972222</td>\n",
              "      <td>-2.768100</td>\n",
              "      <td>-21.663651</td>\n",
              "      <td>130</td>\n",
              "      <td>7040.0</td>\n",
              "      <td>7129.990488</td>\n",
              "      <td>7130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>837304</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A180400</td>\n",
              "      <td>DXVX</td>\n",
              "      <td>106884</td>\n",
              "      <td>7000</td>\n",
              "      <td>7150</td>\n",
              "      <td>6930</td>\n",
              "      <td>202307</td>\n",
              "      <td>1272.810059</td>\n",
              "      <td>0.907500</td>\n",
              "      <td>...</td>\n",
              "      <td>-78.448276</td>\n",
              "      <td>-501.764706</td>\n",
              "      <td>7067.508809</td>\n",
              "      <td>-8.354756</td>\n",
              "      <td>-2.637507</td>\n",
              "      <td>-23.222156</td>\n",
              "      <td>130</td>\n",
              "      <td>7040.0</td>\n",
              "      <td>7129.998553</td>\n",
              "      <td>7130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>837305</th>\n",
              "      <td>20230728</td>\n",
              "      <td>A180400</td>\n",
              "      <td>DXVX</td>\n",
              "      <td>106884</td>\n",
              "      <td>7000</td>\n",
              "      <td>7150</td>\n",
              "      <td>6930</td>\n",
              "      <td>202307</td>\n",
              "      <td>1272.810059</td>\n",
              "      <td>0.907500</td>\n",
              "      <td>...</td>\n",
              "      <td>-78.448276</td>\n",
              "      <td>-464.705882</td>\n",
              "      <td>7067.768918</td>\n",
              "      <td>-3.908356</td>\n",
              "      <td>-2.505538</td>\n",
              "      <td>-24.396561</td>\n",
              "      <td>130</td>\n",
              "      <td>7040.0</td>\n",
              "      <td>7129.999780</td>\n",
              "      <td>7130</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>543 rows × 96 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f6659f48-1126-4581-82b2-66385cc888ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-b4d04ef3-9c15-4200-b465-1796f4c6c879\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b4d04ef3-9c15-4200-b465-1796f4c6c879')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-b4d04ef3-9c15-4200-b465-1796f4c6c879 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f6659f48-1126-4581-82b2-66385cc888ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f6659f48-1126-4581-82b2-66385cc888ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "train[train['item_Code']=='A180400']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f13iBh54yIwa",
        "outputId": "cddd7b65-a7cd-4b4d-ef52-0f4f1ba527df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.9158, Validation loss 60.0967\n",
            "Epoch 50, Training loss 50.0078, Validation loss 53.9283\n",
            "Epoch 100, Training loss 48.9794, Validation loss 53.2060\n",
            "Epoch 150, Training loss 48.3028, Validation loss 52.6490\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1643, item_Code: A215200\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8971, Validation loss 61.1882\n",
            "Epoch 50, Training loss 53.6756, Validation loss 60.7807\n",
            "Epoch 100, Training loss 52.8672, Validation loss 58.9305\n",
            "Epoch 150, Training loss 52.4704, Validation loss 58.4295\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1644, item_Code: A215360\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.4283, Validation loss 60.1056\n",
            "Epoch 50, Training loss 48.9076, Validation loss 55.1179\n",
            "Epoch 100, Training loss 48.2708, Validation loss 55.1156\n",
            "Epoch 150, Training loss 47.6060, Validation loss 54.5780\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1645, item_Code: A215600\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 78)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 54.0382, Validation loss 54.1893\n",
            "Epoch 50, Training loss 35.7331, Validation loss 51.1929\n",
            "Epoch 100, Training loss 35.2044, Validation loss 51.7403\n",
            "Epoch 150, Training loss 34.3683, Validation loss 50.5905\n",
            "torch.Size([543, 77])\n",
            "torch.Size([543, 78])\n",
            "--------------------\n",
            "count: 1646, item_Code: A215790\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.8800, Validation loss 61.6390\n",
            "Epoch 50, Training loss 49.9880, Validation loss 53.0512\n",
            "Epoch 100, Training loss 49.1776, Validation loss 52.9590\n",
            "Epoch 150, Training loss 48.4700, Validation loss 52.3603\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1647, item_Code: A216050\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.5720, Validation loss 60.0883\n",
            "Epoch 50, Training loss 46.5754, Validation loss 46.7084\n",
            "Epoch 100, Training loss 45.5006, Validation loss 46.3860\n",
            "Epoch 150, Training loss 45.0871, Validation loss 45.7302\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1648, item_Code: A216080\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.3752, Validation loss 58.5610\n",
            "Epoch 50, Training loss 47.2092, Validation loss 56.3897\n",
            "Epoch 100, Training loss 46.8182, Validation loss 55.9449\n",
            "Epoch 150, Training loss 45.9820, Validation loss 55.9088\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1649, item_Code: A217190\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.6908, Validation loss 62.1039\n",
            "Epoch 50, Training loss 49.9292, Validation loss 65.3839\n",
            "Epoch 100, Training loss 48.8968, Validation loss 65.2154\n",
            "Epoch 150, Training loss 48.6875, Validation loss 65.5562\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1650, item_Code: A217270\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.1897, Validation loss 60.6531\n",
            "Epoch 50, Training loss 49.7243, Validation loss 46.7445\n",
            "Epoch 100, Training loss 48.9984, Validation loss 45.5736\n",
            "Epoch 150, Training loss 48.8382, Validation loss 45.4219\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1651, item_Code: A217330\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.7052, Validation loss 62.1831\n",
            "Epoch 50, Training loss 47.0014, Validation loss 47.3826\n",
            "Epoch 100, Training loss 46.1275, Validation loss 45.9964\n",
            "Epoch 150, Training loss 45.1891, Validation loss 45.2753\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1652, item_Code: A217500\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 63.0249, Validation loss 62.5659\n",
            "Epoch 50, Training loss 52.4615, Validation loss 59.0127\n",
            "Epoch 100, Training loss 51.2929, Validation loss 58.0003\n",
            "Epoch 150, Training loss 50.5244, Validation loss 57.9880\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1653, item_Code: A217620\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.3681, Validation loss 61.9820\n",
            "Epoch 50, Training loss 52.7886, Validation loss 54.4181\n",
            "Epoch 100, Training loss 52.1322, Validation loss 54.1079\n",
            "Epoch 150, Training loss 51.6568, Validation loss 53.7075\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1654, item_Code: A217730\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.4769, Validation loss 59.4668\n",
            "Epoch 50, Training loss 46.9588, Validation loss 45.9255\n",
            "Epoch 100, Training loss 46.0496, Validation loss 45.3683\n",
            "Epoch 150, Training loss 45.4183, Validation loss 45.1745\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1655, item_Code: A217820\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.0611, Validation loss 61.7968\n",
            "Epoch 50, Training loss 51.2148, Validation loss 57.5065\n",
            "Epoch 100, Training loss 50.2754, Validation loss 57.3472\n",
            "Epoch 150, Training loss 49.9467, Validation loss 56.9174\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1656, item_Code: A218150\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.7241, Validation loss 59.3982\n",
            "Epoch 50, Training loss 48.0525, Validation loss 53.6908\n",
            "Epoch 100, Training loss 47.1622, Validation loss 53.2220\n",
            "Epoch 150, Training loss 46.7499, Validation loss 53.0466\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1657, item_Code: A218410\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.5643, Validation loss 61.2352\n",
            "Epoch 50, Training loss 50.7824, Validation loss 51.1543\n",
            "Epoch 100, Training loss 50.0752, Validation loss 50.7361\n",
            "Epoch 150, Training loss 49.4118, Validation loss 50.2729\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1658, item_Code: A219130\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.5674, Validation loss 62.0173\n",
            "Epoch 50, Training loss 48.2148, Validation loss 60.7100\n",
            "Epoch 100, Training loss 47.7336, Validation loss 60.2817\n",
            "Epoch 150, Training loss 47.3584, Validation loss 59.5904\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1659, item_Code: A219420\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.0771, Validation loss 62.0574\n",
            "Epoch 50, Training loss 52.1568, Validation loss 54.9544\n",
            "Epoch 100, Training loss 51.6130, Validation loss 54.5748\n",
            "Epoch 150, Training loss 50.5925, Validation loss 54.2397\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1660, item_Code: A219550\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.2045, Validation loss 59.6650\n",
            "Epoch 50, Training loss 47.3438, Validation loss 45.7197\n",
            "Epoch 100, Training loss 46.5215, Validation loss 44.6998\n",
            "Epoch 150, Training loss 46.2139, Validation loss 44.5511\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1661, item_Code: A220100\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3368, Validation loss 60.2747\n",
            "Epoch 50, Training loss 51.7796, Validation loss 53.1125\n",
            "Epoch 100, Training loss 51.3216, Validation loss 51.8680\n",
            "Epoch 150, Training loss 50.9027, Validation loss 50.0535\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1662, item_Code: A220180\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0710, Validation loss 60.4361\n",
            "Epoch 50, Training loss 51.1473, Validation loss 56.2122\n",
            "Epoch 100, Training loss 50.5210, Validation loss 56.0612\n",
            "Epoch 150, Training loss 49.9678, Validation loss 56.0195\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1663, item_Code: A220260\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.2687, Validation loss 59.8126\n",
            "Epoch 50, Training loss 49.5945, Validation loss 52.4367\n",
            "Epoch 100, Training loss 48.7689, Validation loss 51.9552\n",
            "Epoch 150, Training loss 48.1228, Validation loss 51.8303\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1664, item_Code: A221840\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0242, Validation loss 60.8487\n",
            "Epoch 50, Training loss 50.0990, Validation loss 50.2020\n",
            "Epoch 100, Training loss 49.2150, Validation loss 49.0999\n",
            "Epoch 150, Training loss 48.5307, Validation loss 48.6335\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1665, item_Code: A221980\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8746, Validation loss 60.8364\n",
            "Epoch 50, Training loss 50.4271, Validation loss 49.2084\n",
            "Epoch 100, Training loss 49.4086, Validation loss 48.3291\n",
            "Epoch 150, Training loss 49.3298, Validation loss 47.7691\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1666, item_Code: A222040\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.4663, Validation loss 60.1822\n",
            "Epoch 50, Training loss 47.4657, Validation loss 47.4220\n",
            "Epoch 100, Training loss 46.0350, Validation loss 46.4266\n",
            "Epoch 150, Training loss 45.3985, Validation loss 45.9389\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1667, item_Code: A222080\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7287, Validation loss 60.8064\n",
            "Epoch 50, Training loss 52.3229, Validation loss 54.9245\n",
            "Epoch 100, Training loss 51.3227, Validation loss 55.3069\n",
            "Epoch 150, Training loss 50.9256, Validation loss 55.1781\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1668, item_Code: A222110\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.9632, Validation loss 60.0107\n",
            "Epoch 50, Training loss 50.4265, Validation loss 52.1373\n",
            "Epoch 100, Training loss 49.5970, Validation loss 51.1102\n",
            "Epoch 150, Training loss 49.3385, Validation loss 50.8484\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1669, item_Code: A222420\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.9725, Validation loss 62.1714\n",
            "Epoch 50, Training loss 53.2188, Validation loss 60.1900\n",
            "Epoch 100, Training loss 52.5303, Validation loss 59.6449\n",
            "Epoch 150, Training loss 52.1755, Validation loss 59.5591\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1670, item_Code: A222800\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.4112, Validation loss 60.9721\n",
            "Epoch 50, Training loss 50.4296, Validation loss 54.0757\n",
            "Epoch 100, Training loss 49.7751, Validation loss 53.4764\n",
            "Epoch 150, Training loss 49.1883, Validation loss 53.2559\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1671, item_Code: A222810\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7959, Validation loss 60.8195\n",
            "Epoch 50, Training loss 46.5970, Validation loss 59.7787\n",
            "Epoch 100, Training loss 46.3243, Validation loss 59.0085\n",
            "Epoch 150, Training loss 45.7427, Validation loss 58.5423\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1672, item_Code: A222980\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.6600, Validation loss 60.4057\n",
            "Epoch 50, Training loss 49.3075, Validation loss 44.1265\n",
            "Epoch 100, Training loss 48.2775, Validation loss 43.6690\n",
            "Epoch 150, Training loss 47.9584, Validation loss 43.6789\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1673, item_Code: A223250\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.9343, Validation loss 60.0240\n",
            "Epoch 50, Training loss 51.1699, Validation loss 53.7269\n",
            "Epoch 100, Training loss 50.0159, Validation loss 54.0734\n",
            "Epoch 150, Training loss 49.5554, Validation loss 53.2806\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1674, item_Code: A223310\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 1)\n",
            "Series([], dtype: int64)\n",
            "A223310 is zero\n",
            "--------------------\n",
            "count: 1675, item_Code: A224110\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.9731, Validation loss 60.2409\n",
            "Epoch 50, Training loss 46.6507, Validation loss 47.4873\n",
            "Epoch 100, Training loss 45.6363, Validation loss 46.7144\n",
            "Epoch 150, Training loss 45.3078, Validation loss 46.6561\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1676, item_Code: A225190\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0766, Validation loss 60.2355\n",
            "Epoch 50, Training loss 51.4369, Validation loss 51.4017\n",
            "Epoch 100, Training loss 50.6827, Validation loss 51.3054\n",
            "Epoch 150, Training loss 50.1889, Validation loss 51.1273\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1677, item_Code: A225220\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.1518, Validation loss 60.0264\n",
            "Epoch 50, Training loss 50.4564, Validation loss 56.1715\n",
            "Epoch 100, Training loss 49.8703, Validation loss 54.7039\n",
            "Epoch 150, Training loss 49.7762, Validation loss 54.3624\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1678, item_Code: A225530\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.2686, Validation loss 60.3543\n",
            "Epoch 50, Training loss 50.5411, Validation loss 51.0671\n",
            "Epoch 100, Training loss 49.9773, Validation loss 50.8433\n",
            "Epoch 150, Training loss 49.1946, Validation loss 50.7383\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1679, item_Code: A225570\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.9152, Validation loss 60.7577\n",
            "Epoch 50, Training loss 51.0928, Validation loss 56.5343\n",
            "Epoch 100, Training loss 50.1207, Validation loss 55.6313\n",
            "Epoch 150, Training loss 49.5461, Validation loss 55.4772\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1680, item_Code: A226320\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.4443, Validation loss 61.4316\n",
            "Epoch 50, Training loss 51.8884, Validation loss 50.5237\n",
            "Epoch 100, Training loss 51.0127, Validation loss 49.9456\n",
            "Epoch 150, Training loss 50.7011, Validation loss 49.6684\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1681, item_Code: A226330\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.2936, Validation loss 59.9926\n",
            "Epoch 50, Training loss 50.1465, Validation loss 50.2628\n",
            "Epoch 100, Training loss 49.4556, Validation loss 49.6399\n",
            "Epoch 150, Training loss 49.1759, Validation loss 49.4565\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1682, item_Code: A226340\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7900, Validation loss 59.8947\n",
            "Epoch 50, Training loss 48.6372, Validation loss 49.1537\n",
            "Epoch 100, Training loss 47.8096, Validation loss 47.9831\n",
            "Epoch 150, Training loss 47.3376, Validation loss 47.4064\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1683, item_Code: A226400\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.8659, Validation loss 59.3380\n",
            "Epoch 50, Training loss 46.7436, Validation loss 62.9797\n",
            "Epoch 100, Training loss 46.3496, Validation loss 62.9587\n",
            "Epoch 150, Training loss 46.2018, Validation loss 63.5580\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1684, item_Code: A226440\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.5875, Validation loss 60.5735\n",
            "Epoch 50, Training loss 42.8664, Validation loss 35.6728\n",
            "Epoch 100, Training loss 42.2090, Validation loss 35.1458\n",
            "Epoch 150, Training loss 41.8162, Validation loss 34.4640\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1685, item_Code: A226950\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.9138, Validation loss 60.5957\n",
            "Epoch 50, Training loss 49.7518, Validation loss 48.4215\n",
            "Epoch 100, Training loss 49.1728, Validation loss 47.8950\n",
            "Epoch 150, Training loss 48.6757, Validation loss 47.5347\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1686, item_Code: A227100\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.5805, Validation loss 59.5403\n",
            "Epoch 50, Training loss 48.3133, Validation loss 43.2228\n",
            "Epoch 100, Training loss 47.3632, Validation loss 42.5823\n",
            "Epoch 150, Training loss 46.7750, Validation loss 42.0286\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1687, item_Code: A227840\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.2457, Validation loss 60.9354\n",
            "Epoch 50, Training loss 49.3766, Validation loss 51.9311\n",
            "Epoch 100, Training loss 48.7016, Validation loss 51.5823\n",
            "Epoch 150, Training loss 47.8391, Validation loss 51.1774\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1688, item_Code: A227950\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.5651, Validation loss 60.1684\n",
            "Epoch 50, Training loss 48.7270, Validation loss 55.5236\n",
            "Epoch 100, Training loss 47.7550, Validation loss 55.5355\n",
            "Epoch 150, Training loss 47.4910, Validation loss 55.0315\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1689, item_Code: A228340\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.2472, Validation loss 61.1204\n",
            "Epoch 50, Training loss 50.1544, Validation loss 47.6828\n",
            "Epoch 100, Training loss 49.4594, Validation loss 46.6808\n",
            "Epoch 150, Training loss 49.0387, Validation loss 46.3269\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1690, item_Code: A228670\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8796, Validation loss 60.4214\n",
            "Epoch 50, Training loss 50.0469, Validation loss 62.5670\n",
            "Epoch 100, Training loss 49.5299, Validation loss 62.8361\n",
            "Epoch 150, Training loss 49.0406, Validation loss 63.0171\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1691, item_Code: A228760\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.5902, Validation loss 60.3333\n",
            "Epoch 50, Training loss 49.6281, Validation loss 59.8906\n",
            "Epoch 100, Training loss 48.6865, Validation loss 59.6043\n",
            "Epoch 150, Training loss 48.2371, Validation loss 58.5616\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1692, item_Code: A228850\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3611, Validation loss 59.7192\n",
            "Epoch 50, Training loss 49.9295, Validation loss 54.3975\n",
            "Epoch 100, Training loss 49.3974, Validation loss 54.3759\n",
            "Epoch 150, Training loss 48.8273, Validation loss 54.3941\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1693, item_Code: A229000\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 83)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.5708, Validation loss 58.0223\n",
            "Epoch 50, Training loss 46.4822, Validation loss 46.7520\n",
            "Epoch 100, Training loss 45.3157, Validation loss 45.4010\n",
            "Epoch 150, Training loss 45.2669, Validation loss 44.7148\n",
            "torch.Size([543, 82])\n",
            "torch.Size([543, 83])\n",
            "--------------------\n",
            "count: 1694, item_Code: A229640\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.3271, Validation loss 62.5728\n",
            "Epoch 50, Training loss 51.7122, Validation loss 50.3552\n",
            "Epoch 100, Training loss 51.2712, Validation loss 49.9487\n",
            "Epoch 150, Training loss 50.9986, Validation loss 49.8233\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1695, item_Code: A230240\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.9198, Validation loss 59.9064\n",
            "Epoch 50, Training loss 51.0809, Validation loss 58.0878\n",
            "Epoch 100, Training loss 50.2218, Validation loss 57.8077\n",
            "Epoch 150, Training loss 49.3649, Validation loss 56.8061\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1696, item_Code: A230360\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.9044, Validation loss 58.7954\n",
            "Epoch 50, Training loss 50.1842, Validation loss 48.6855\n",
            "Epoch 100, Training loss 49.5273, Validation loss 48.5871\n",
            "Epoch 150, Training loss 49.1546, Validation loss 48.4546\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1697, item_Code: A230980\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.7615, Validation loss 58.4375\n",
            "Epoch 50, Training loss 48.6540, Validation loss 48.8983\n",
            "Epoch 100, Training loss 48.2778, Validation loss 48.4613\n",
            "Epoch 150, Training loss 47.6351, Validation loss 47.6656\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1698, item_Code: A232140\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.1814, Validation loss 59.9228\n",
            "Epoch 50, Training loss 47.8938, Validation loss 54.9983\n",
            "Epoch 100, Training loss 46.8570, Validation loss 55.1152\n",
            "Epoch 150, Training loss 46.1734, Validation loss 54.8218\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1699, item_Code: A232680\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.4452, Validation loss 61.5910\n",
            "Epoch 50, Training loss 50.7450, Validation loss 46.3925\n",
            "Epoch 100, Training loss 49.5195, Validation loss 45.4900\n",
            "Epoch 150, Training loss 49.1828, Validation loss 45.4643\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1700, item_Code: A234080\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3983, Validation loss 59.9963\n",
            "Epoch 50, Training loss 49.3570, Validation loss 49.2183\n",
            "Epoch 100, Training loss 48.7891, Validation loss 48.8130\n",
            "Epoch 150, Training loss 48.3232, Validation loss 48.6396\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1701, item_Code: A234100\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.1536, Validation loss 58.1960\n",
            "Epoch 50, Training loss 45.6766, Validation loss 45.4152\n",
            "Epoch 100, Training loss 45.1403, Validation loss 44.5318\n",
            "Epoch 150, Training loss 44.0530, Validation loss 44.2225\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1702, item_Code: A234300\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.3347, Validation loss 61.6571\n",
            "Epoch 50, Training loss 50.0973, Validation loss 54.9989\n",
            "Epoch 100, Training loss 49.0955, Validation loss 54.4319\n",
            "Epoch 150, Training loss 48.5874, Validation loss 54.1095\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1703, item_Code: A234340\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8995, Validation loss 61.2082\n",
            "Epoch 50, Training loss 50.9825, Validation loss 52.4876\n",
            "Epoch 100, Training loss 50.3895, Validation loss 51.7776\n",
            "Epoch 150, Training loss 49.5600, Validation loss 51.0682\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1704, item_Code: A234690\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.7739, Validation loss 60.9348\n",
            "Epoch 50, Training loss 51.0202, Validation loss 51.9875\n",
            "Epoch 100, Training loss 50.3135, Validation loss 51.5022\n",
            "Epoch 150, Training loss 49.7252, Validation loss 51.1138\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1705, item_Code: A234920\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7190, Validation loss 61.3432\n",
            "Epoch 50, Training loss 41.2097, Validation loss 81.0574\n",
            "Epoch 100, Training loss 40.7182, Validation loss 82.6561\n",
            "Epoch 150, Training loss 40.4661, Validation loss 83.7756\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1706, item_Code: A235980\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.6229, Validation loss 59.8968\n",
            "Epoch 50, Training loss 48.9450, Validation loss 46.3743\n",
            "Epoch 100, Training loss 48.1858, Validation loss 45.5145\n",
            "Epoch 150, Training loss 47.6098, Validation loss 45.3363\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1707, item_Code: A236200\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.2335, Validation loss 61.9060\n",
            "Epoch 50, Training loss 52.5680, Validation loss 55.2372\n",
            "Epoch 100, Training loss 51.3225, Validation loss 55.1790\n",
            "Epoch 150, Training loss 50.7372, Validation loss 54.9860\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1708, item_Code: A236810\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.4413, Validation loss 62.4838\n",
            "Epoch 50, Training loss 48.8775, Validation loss 47.8657\n",
            "Epoch 100, Training loss 48.4400, Validation loss 46.5923\n",
            "Epoch 150, Training loss 47.9044, Validation loss 46.3533\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1709, item_Code: A237690\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.2621, Validation loss 60.1719\n",
            "Epoch 50, Training loss 52.1151, Validation loss 49.1399\n",
            "Epoch 100, Training loss 51.0773, Validation loss 48.3066\n",
            "Epoch 150, Training loss 50.7782, Validation loss 48.1779\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1710, item_Code: A237820\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.1026, Validation loss 60.8924\n",
            "Epoch 50, Training loss 49.2405, Validation loss 52.7271\n",
            "Epoch 100, Training loss 48.7216, Validation loss 52.5958\n",
            "Epoch 150, Training loss 47.9522, Validation loss 52.2338\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1711, item_Code: A237880\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.8712, Validation loss 58.2752\n",
            "Epoch 50, Training loss 48.4083, Validation loss 56.8600\n",
            "Epoch 100, Training loss 47.8181, Validation loss 55.9984\n",
            "Epoch 150, Training loss 47.1369, Validation loss 55.6851\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1712, item_Code: A238090\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.6181, Validation loss 59.8597\n",
            "Epoch 50, Training loss 50.7372, Validation loss 53.4946\n",
            "Epoch 100, Training loss 49.9022, Validation loss 52.0093\n",
            "Epoch 150, Training loss 49.4698, Validation loss 51.1306\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1713, item_Code: A238120\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.5526, Validation loss 62.3485\n",
            "Epoch 50, Training loss 53.2762, Validation loss 55.3506\n",
            "Epoch 100, Training loss 52.6047, Validation loss 55.3625\n",
            "Epoch 150, Training loss 52.0430, Validation loss 55.0303\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1714, item_Code: A238200\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.2489, Validation loss 61.5682\n",
            "Epoch 50, Training loss 51.4206, Validation loss 50.1107\n",
            "Epoch 100, Training loss 50.6691, Validation loss 49.5991\n",
            "Epoch 150, Training loss 50.1948, Validation loss 49.6336\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1715, item_Code: A238490\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.0072, Validation loss 60.0156\n",
            "Epoch 50, Training loss 47.9046, Validation loss 52.2523\n",
            "Epoch 100, Training loss 46.5327, Validation loss 51.6414\n",
            "Epoch 150, Training loss 46.1450, Validation loss 51.1384\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1716, item_Code: A239340\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.9908, Validation loss 61.4934\n",
            "Epoch 50, Training loss 50.8017, Validation loss 49.4169\n",
            "Epoch 100, Training loss 49.6637, Validation loss 49.1790\n",
            "Epoch 150, Training loss 49.2030, Validation loss 48.8434\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1717, item_Code: A239610\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8769, Validation loss 61.2743\n",
            "Epoch 50, Training loss 48.3999, Validation loss 52.9064\n",
            "Epoch 100, Training loss 47.0914, Validation loss 51.6278\n",
            "Epoch 150, Training loss 46.4116, Validation loss 51.0160\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1718, item_Code: A239890\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8243, Validation loss 60.2967\n",
            "Epoch 50, Training loss 49.6339, Validation loss 56.4219\n",
            "Epoch 100, Training loss 49.0027, Validation loss 56.5062\n",
            "Epoch 150, Training loss 48.2249, Validation loss 56.3913\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1719, item_Code: A240810\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.7724, Validation loss 59.5663\n",
            "Epoch 50, Training loss 48.6975, Validation loss 53.4047\n",
            "Epoch 100, Training loss 47.9311, Validation loss 52.7429\n",
            "Epoch 150, Training loss 47.4919, Validation loss 52.4162\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1720, item_Code: A241520\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 91)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.8027, Validation loss 64.1698\n",
            "Epoch 50, Training loss 51.3147, Validation loss 53.6886\n",
            "Epoch 100, Training loss 50.2807, Validation loss 52.7935\n",
            "Epoch 150, Training loss 49.8821, Validation loss 52.6057\n",
            "torch.Size([543, 90])\n",
            "torch.Size([543, 91])\n",
            "--------------------\n",
            "count: 1721, item_Code: A241560\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7787, Validation loss 60.8653\n",
            "Epoch 50, Training loss 50.9201, Validation loss 64.5110\n",
            "Epoch 100, Training loss 50.5588, Validation loss 65.1563\n",
            "Epoch 150, Training loss 50.0695, Validation loss 65.7961\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1722, item_Code: A241590\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.6311, Validation loss 61.3799\n",
            "Epoch 50, Training loss 50.6023, Validation loss 51.8037\n",
            "Epoch 100, Training loss 50.0713, Validation loss 51.1893\n",
            "Epoch 150, Training loss 49.3494, Validation loss 49.5566\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1723, item_Code: A241690\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.5283, Validation loss 62.3406\n",
            "Epoch 50, Training loss 51.8783, Validation loss 48.8750\n",
            "Epoch 100, Training loss 50.7720, Validation loss 48.5394\n",
            "Epoch 150, Training loss 50.2850, Validation loss 47.9830\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1724, item_Code: A241710\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.4692, Validation loss 62.7547\n",
            "Epoch 50, Training loss 49.8434, Validation loss 61.9592\n",
            "Epoch 100, Training loss 49.2254, Validation loss 61.8579\n",
            "Epoch 150, Training loss 48.7582, Validation loss 61.7580\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1725, item_Code: A241770\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.6806, Validation loss 61.2231\n",
            "Epoch 50, Training loss 53.0000, Validation loss 56.5540\n",
            "Epoch 100, Training loss 52.0672, Validation loss 56.0429\n",
            "Epoch 150, Training loss 51.9376, Validation loss 55.5836\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1726, item_Code: A241790\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.9772, Validation loss 58.4589\n",
            "Epoch 50, Training loss 50.2383, Validation loss 53.8702\n",
            "Epoch 100, Training loss 49.3973, Validation loss 53.1711\n",
            "Epoch 150, Training loss 48.7155, Validation loss 52.8396\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1727, item_Code: A241820\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.9841, Validation loss 58.0232\n",
            "Epoch 50, Training loss 47.9129, Validation loss 44.4330\n",
            "Epoch 100, Training loss 47.1378, Validation loss 43.5007\n",
            "Epoch 150, Training loss 46.6483, Validation loss 42.9488\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1728, item_Code: A241840\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.4949, Validation loss 59.0998\n",
            "Epoch 50, Training loss 51.6183, Validation loss 50.5816\n",
            "Epoch 100, Training loss 50.9978, Validation loss 49.9664\n",
            "Epoch 150, Training loss 50.7389, Validation loss 49.5655\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1729, item_Code: A242040\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.9341, Validation loss 60.4334\n",
            "Epoch 50, Training loss 49.7109, Validation loss 57.1072\n",
            "Epoch 100, Training loss 49.1080, Validation loss 56.8470\n",
            "Epoch 150, Training loss 48.4662, Validation loss 56.3401\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1730, item_Code: A243070\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.9740, Validation loss 59.6810\n",
            "Epoch 50, Training loss 47.6589, Validation loss 46.4776\n",
            "Epoch 100, Training loss 46.3887, Validation loss 45.6983\n",
            "Epoch 150, Training loss 46.0356, Validation loss 45.1453\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1731, item_Code: A243840\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.3674, Validation loss 60.3397\n",
            "Epoch 50, Training loss 51.2812, Validation loss 50.4911\n",
            "Epoch 100, Training loss 50.3001, Validation loss 49.8880\n",
            "Epoch 150, Training loss 49.5321, Validation loss 49.6399\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1732, item_Code: A244460\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0735, Validation loss 60.7612\n",
            "Epoch 50, Training loss 51.6127, Validation loss 53.6656\n",
            "Epoch 100, Training loss 50.8718, Validation loss 52.4422\n",
            "Epoch 150, Training loss 50.6821, Validation loss 52.6654\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1733, item_Code: A244920\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.2857, Validation loss 59.4580\n",
            "Epoch 50, Training loss 48.6229, Validation loss 44.5541\n",
            "Epoch 100, Training loss 47.7108, Validation loss 43.5287\n",
            "Epoch 150, Training loss 47.4782, Validation loss 43.0739\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1734, item_Code: A245620\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.3870, Validation loss 58.5871\n",
            "Epoch 50, Training loss 48.6898, Validation loss 46.6266\n",
            "Epoch 100, Training loss 48.1411, Validation loss 45.7675\n",
            "Epoch 150, Training loss 47.4422, Validation loss 44.8406\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1735, item_Code: A246690\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.9952, Validation loss 60.6477\n",
            "Epoch 50, Training loss 51.1509, Validation loss 46.9540\n",
            "Epoch 100, Training loss 50.3831, Validation loss 45.8343\n",
            "Epoch 150, Training loss 49.6962, Validation loss 45.5680\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1736, item_Code: A246710\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.0211, Validation loss 59.6172\n",
            "Epoch 50, Training loss 48.1693, Validation loss 42.4079\n",
            "Epoch 100, Training loss 47.1392, Validation loss 41.5452\n",
            "Epoch 150, Training loss 46.4107, Validation loss 41.0490\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1737, item_Code: A246720\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8571, Validation loss 62.3702\n",
            "Epoch 50, Training loss 45.4032, Validation loss 71.5217\n",
            "Epoch 100, Training loss 44.9039, Validation loss 72.3524\n",
            "Epoch 150, Training loss 44.6182, Validation loss 73.6636\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1738, item_Code: A246960\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.6949, Validation loss 60.5717\n",
            "Epoch 50, Training loss 50.3929, Validation loss 47.4592\n",
            "Epoch 100, Training loss 49.7059, Validation loss 46.7022\n",
            "Epoch 150, Training loss 49.0890, Validation loss 45.9889\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1739, item_Code: A247540\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.9588, Validation loss 60.1377\n",
            "Epoch 50, Training loss 48.3095, Validation loss 57.9379\n",
            "Epoch 100, Training loss 47.2325, Validation loss 58.1193\n",
            "Epoch 150, Training loss 46.4842, Validation loss 57.8038\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1740, item_Code: A247660\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.6376, Validation loss 60.6534\n",
            "Epoch 50, Training loss 49.7583, Validation loss 43.0366\n",
            "Epoch 100, Training loss 48.4933, Validation loss 42.9312\n",
            "Epoch 150, Training loss 47.9558, Validation loss 42.9160\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1741, item_Code: A248070\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.2047, Validation loss 59.5788\n",
            "Epoch 50, Training loss 50.0921, Validation loss 57.1874\n",
            "Epoch 100, Training loss 48.9204, Validation loss 57.0690\n",
            "Epoch 150, Training loss 48.6057, Validation loss 56.9955\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1742, item_Code: A248170\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.3381, Validation loss 59.9085\n",
            "Epoch 50, Training loss 49.1613, Validation loss 46.9623\n",
            "Epoch 100, Training loss 48.2584, Validation loss 46.5139\n",
            "Epoch 150, Training loss 48.0041, Validation loss 45.9659\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1743, item_Code: A249420\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.1900, Validation loss 59.4011\n",
            "Epoch 50, Training loss 50.5310, Validation loss 51.8043\n",
            "Epoch 100, Training loss 49.7255, Validation loss 50.7528\n",
            "Epoch 150, Training loss 49.6448, Validation loss 50.9616\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1744, item_Code: A250000\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.5766, Validation loss 61.1366\n",
            "Epoch 50, Training loss 48.9399, Validation loss 53.0071\n",
            "Epoch 100, Training loss 48.7311, Validation loss 52.9769\n",
            "Epoch 150, Training loss 48.1100, Validation loss 52.3025\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1745, item_Code: A250060\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.5601, Validation loss 60.2922\n",
            "Epoch 50, Training loss 49.2192, Validation loss 56.2212\n",
            "Epoch 100, Training loss 48.6132, Validation loss 55.9454\n",
            "Epoch 150, Training loss 47.8302, Validation loss 55.7280\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1746, item_Code: A251270\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8505, Validation loss 61.1312\n",
            "Epoch 50, Training loss 48.1252, Validation loss 49.3881\n",
            "Epoch 100, Training loss 47.5634, Validation loss 48.6695\n",
            "Epoch 150, Training loss 46.9142, Validation loss 48.2900\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1747, item_Code: A251370\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.1041, Validation loss 62.7497\n",
            "Epoch 50, Training loss 51.0088, Validation loss 50.6758\n",
            "Epoch 100, Training loss 50.3756, Validation loss 50.0588\n",
            "Epoch 150, Training loss 49.5410, Validation loss 48.6798\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1748, item_Code: A251630\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7424, Validation loss 60.8944\n",
            "Epoch 50, Training loss 50.4656, Validation loss 60.5945\n",
            "Epoch 100, Training loss 49.8710, Validation loss 59.9892\n",
            "Epoch 150, Training loss 49.4891, Validation loss 58.8552\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1749, item_Code: A251970\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.9601, Validation loss 61.6208\n",
            "Epoch 50, Training loss 48.6750, Validation loss 57.8585\n",
            "Epoch 100, Training loss 47.8756, Validation loss 56.8923\n",
            "Epoch 150, Training loss 47.4948, Validation loss 57.4824\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1750, item_Code: A252500\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.1955, Validation loss 57.7958\n",
            "Epoch 50, Training loss 45.2931, Validation loss 41.7081\n",
            "Epoch 100, Training loss 44.3886, Validation loss 41.5096\n",
            "Epoch 150, Training loss 44.0208, Validation loss 41.0277\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1751, item_Code: A252990\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.2461, Validation loss 59.8143\n",
            "Epoch 50, Training loss 49.2220, Validation loss 53.9355\n",
            "Epoch 100, Training loss 48.5522, Validation loss 53.5412\n",
            "Epoch 150, Training loss 48.1823, Validation loss 53.2040\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1752, item_Code: A253450\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.5294, Validation loss 61.9483\n",
            "Epoch 50, Training loss 52.2072, Validation loss 60.0478\n",
            "Epoch 100, Training loss 51.3432, Validation loss 59.5910\n",
            "Epoch 150, Training loss 51.0309, Validation loss 58.5264\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1753, item_Code: A253590\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.2275, Validation loss 61.3299\n",
            "Epoch 50, Training loss 50.6856, Validation loss 56.7528\n",
            "Epoch 100, Training loss 50.0542, Validation loss 56.3394\n",
            "Epoch 150, Training loss 49.6103, Validation loss 55.9904\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1754, item_Code: A253840\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.2095, Validation loss 60.9557\n",
            "Epoch 50, Training loss 50.8061, Validation loss 46.1907\n",
            "Epoch 100, Training loss 50.1530, Validation loss 45.4699\n",
            "Epoch 150, Training loss 49.4427, Validation loss 44.5635\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1755, item_Code: A254120\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.9092, Validation loss 60.9369\n",
            "Epoch 50, Training loss 45.3240, Validation loss 61.9279\n",
            "Epoch 100, Training loss 44.8701, Validation loss 62.5089\n",
            "Epoch 150, Training loss 44.5194, Validation loss 62.6487\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1756, item_Code: A255220\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0911, Validation loss 61.2734\n",
            "Epoch 50, Training loss 49.6997, Validation loss 50.9486\n",
            "Epoch 100, Training loss 49.3484, Validation loss 50.9106\n",
            "Epoch 150, Training loss 48.6051, Validation loss 49.8499\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1757, item_Code: A255440\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.2044, Validation loss 59.7212\n",
            "Epoch 50, Training loss 48.8674, Validation loss 56.5168\n",
            "Epoch 100, Training loss 48.5430, Validation loss 56.6355\n",
            "Epoch 150, Training loss 47.5262, Validation loss 56.4804\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1758, item_Code: A256150\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.0496, Validation loss 58.9667\n",
            "Epoch 50, Training loss 47.5476, Validation loss 48.0395\n",
            "Epoch 100, Training loss 46.7621, Validation loss 47.1775\n",
            "Epoch 150, Training loss 46.2928, Validation loss 46.6610\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1759, item_Code: A256630\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.5630, Validation loss 60.1060\n",
            "Epoch 50, Training loss 48.1104, Validation loss 45.3795\n",
            "Epoch 100, Training loss 47.5045, Validation loss 45.0694\n",
            "Epoch 150, Training loss 46.8184, Validation loss 44.5716\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1760, item_Code: A256840\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.9065, Validation loss 59.4555\n",
            "Epoch 50, Training loss 48.0378, Validation loss 40.7123\n",
            "Epoch 100, Training loss 46.9906, Validation loss 39.8110\n",
            "Epoch 150, Training loss 46.5473, Validation loss 39.3949\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1761, item_Code: A256940\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.1464, Validation loss 59.5583\n",
            "Epoch 50, Training loss 47.4208, Validation loss 57.4778\n",
            "Epoch 100, Training loss 46.8426, Validation loss 57.5372\n",
            "Epoch 150, Training loss 46.1734, Validation loss 57.5486\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1762, item_Code: A257370\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 1)\n",
            "Series([], dtype: int64)\n",
            "A257370 is zero\n",
            "--------------------\n",
            "count: 1763, item_Code: A258610\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.8585, Validation loss 59.5090\n",
            "Epoch 50, Training loss 49.5428, Validation loss 50.4628\n",
            "Epoch 100, Training loss 48.9858, Validation loss 50.3533\n",
            "Epoch 150, Training loss 48.5657, Validation loss 49.3114\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1764, item_Code: A258830\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7934, Validation loss 59.1742\n",
            "Epoch 50, Training loss 46.6820, Validation loss 42.7387\n",
            "Epoch 100, Training loss 45.8738, Validation loss 42.4246\n",
            "Epoch 150, Training loss 45.4903, Validation loss 42.2277\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1765, item_Code: A259630\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0374, Validation loss 60.1186\n",
            "Epoch 50, Training loss 51.4567, Validation loss 51.3906\n",
            "Epoch 100, Training loss 50.7930, Validation loss 51.1023\n",
            "Epoch 150, Training loss 50.4196, Validation loss 50.3467\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1766, item_Code: A260660\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.5568, Validation loss 61.2163\n",
            "Epoch 50, Training loss 49.3751, Validation loss 48.3063\n",
            "Epoch 100, Training loss 48.4334, Validation loss 47.5404\n",
            "Epoch 150, Training loss 48.0971, Validation loss 47.6190\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1767, item_Code: A260930\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.6483, Validation loss 63.1569\n",
            "Epoch 50, Training loss 51.3729, Validation loss 50.0217\n",
            "Epoch 100, Training loss 50.5859, Validation loss 49.1639\n",
            "Epoch 150, Training loss 49.8776, Validation loss 48.8584\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1768, item_Code: A260970\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.6781, Validation loss 58.9488\n",
            "Epoch 50, Training loss 47.3747, Validation loss 44.0314\n",
            "Epoch 100, Training loss 46.4310, Validation loss 44.1058\n",
            "Epoch 150, Training loss 45.9651, Validation loss 43.2190\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1769, item_Code: A261200\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.1997, Validation loss 60.1175\n",
            "Epoch 50, Training loss 47.3178, Validation loss 56.9213\n",
            "Epoch 100, Training loss 46.4698, Validation loss 56.7882\n",
            "Epoch 150, Training loss 45.8193, Validation loss 56.6296\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1770, item_Code: A262260\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7278, Validation loss 60.3544\n",
            "Epoch 50, Training loss 46.8858, Validation loss 52.1782\n",
            "Epoch 100, Training loss 45.9278, Validation loss 51.5948\n",
            "Epoch 150, Training loss 45.8043, Validation loss 51.7045\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1771, item_Code: A262840\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8468, Validation loss 59.5619\n",
            "Epoch 50, Training loss 49.7738, Validation loss 48.3194\n",
            "Epoch 100, Training loss 48.8636, Validation loss 46.9365\n",
            "Epoch 150, Training loss 48.4377, Validation loss 46.7090\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1772, item_Code: A263020\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.1145, Validation loss 61.9401\n",
            "Epoch 50, Training loss 45.9623, Validation loss 44.4693\n",
            "Epoch 100, Training loss 45.1926, Validation loss 44.0888\n",
            "Epoch 150, Training loss 45.0600, Validation loss 43.9136\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1773, item_Code: A263050\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0052, Validation loss 60.6613\n",
            "Epoch 50, Training loss 48.5806, Validation loss 43.7978\n",
            "Epoch 100, Training loss 48.0126, Validation loss 43.1984\n",
            "Epoch 150, Training loss 47.1891, Validation loss 42.2350\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1774, item_Code: A263540\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 4)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 2.3617, Validation loss 2.2455\n",
            "Epoch 50, Training loss 2.1067, Validation loss 2.2831\n",
            "Epoch 100, Training loss 2.1010, Validation loss 2.3109\n",
            "Epoch 150, Training loss 2.1025, Validation loss 2.3086\n",
            "torch.Size([543, 3])\n",
            "torch.Size([543, 4])\n",
            "--------------------\n",
            "count: 1775, item_Code: A263600\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.1665, Validation loss 61.7903\n",
            "Epoch 50, Training loss 50.9064, Validation loss 68.2734\n",
            "Epoch 100, Training loss 50.4729, Validation loss 68.0418\n",
            "Epoch 150, Training loss 49.7974, Validation loss 67.2367\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1776, item_Code: A263690\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.7969, Validation loss 60.6235\n",
            "Epoch 50, Training loss 51.0917, Validation loss 57.8744\n",
            "Epoch 100, Training loss 50.3667, Validation loss 58.2371\n",
            "Epoch 150, Training loss 49.6098, Validation loss 57.1523\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1777, item_Code: A263700\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.6836, Validation loss 61.9801\n",
            "Epoch 50, Training loss 52.6089, Validation loss 55.0899\n",
            "Epoch 100, Training loss 51.3899, Validation loss 54.6272\n",
            "Epoch 150, Training loss 50.9977, Validation loss 54.1108\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1778, item_Code: A263720\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.7673, Validation loss 59.1208\n",
            "Epoch 50, Training loss 47.3898, Validation loss 49.3491\n",
            "Epoch 100, Training loss 46.5476, Validation loss 48.8945\n",
            "Epoch 150, Training loss 45.6756, Validation loss 48.2959\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1779, item_Code: A263750\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.7848, Validation loss 61.1218\n",
            "Epoch 50, Training loss 50.9435, Validation loss 47.3002\n",
            "Epoch 100, Training loss 49.7528, Validation loss 46.5800\n",
            "Epoch 150, Training loss 49.2821, Validation loss 46.1294\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1780, item_Code: A263770\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.9387, Validation loss 60.0582\n",
            "Epoch 50, Training loss 48.0981, Validation loss 54.6676\n",
            "Epoch 100, Training loss 47.3741, Validation loss 54.1094\n",
            "Epoch 150, Training loss 46.7671, Validation loss 53.8487\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1781, item_Code: A263800\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.3509, Validation loss 62.1423\n",
            "Epoch 50, Training loss 53.3121, Validation loss 58.6981\n",
            "Epoch 100, Training loss 52.5695, Validation loss 57.9797\n",
            "Epoch 150, Training loss 52.2535, Validation loss 57.7505\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1782, item_Code: A263810\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.3570, Validation loss 59.6366\n",
            "Epoch 50, Training loss 47.3602, Validation loss 60.5308\n",
            "Epoch 100, Training loss 46.7107, Validation loss 61.1551\n",
            "Epoch 150, Training loss 46.3264, Validation loss 61.3586\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1783, item_Code: A263860\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.5432, Validation loss 61.3997\n",
            "Epoch 50, Training loss 49.3749, Validation loss 62.7832\n",
            "Epoch 100, Training loss 48.8077, Validation loss 61.2212\n",
            "Epoch 150, Training loss 48.4406, Validation loss 60.4965\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1784, item_Code: A263920\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 78)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 53.0404, Validation loss 54.7192\n",
            "Epoch 50, Training loss 31.9125, Validation loss 45.9707\n",
            "Epoch 100, Training loss 30.4120, Validation loss 45.3676\n",
            "Epoch 150, Training loss 29.6343, Validation loss 44.6583\n",
            "torch.Size([543, 77])\n",
            "torch.Size([543, 78])\n",
            "--------------------\n",
            "count: 1785, item_Code: A264450\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.8707, Validation loss 62.5825\n",
            "Epoch 50, Training loss 50.8401, Validation loss 50.6694\n",
            "Epoch 100, Training loss 50.5987, Validation loss 49.7749\n",
            "Epoch 150, Training loss 50.0021, Validation loss 49.3256\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1786, item_Code: A264660\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.3805, Validation loss 59.0887\n",
            "Epoch 50, Training loss 49.4531, Validation loss 55.3180\n",
            "Epoch 100, Training loss 49.1607, Validation loss 55.7175\n",
            "Epoch 150, Training loss 48.8209, Validation loss 55.4745\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1787, item_Code: A264850\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.4267, Validation loss 61.5457\n",
            "Epoch 50, Training loss 52.0679, Validation loss 51.3322\n",
            "Epoch 100, Training loss 51.0951, Validation loss 50.7291\n",
            "Epoch 150, Training loss 50.6720, Validation loss 50.3821\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1788, item_Code: A264900\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.8147, Validation loss 60.9486\n",
            "Epoch 50, Training loss 49.0642, Validation loss 46.1626\n",
            "Epoch 100, Training loss 48.2034, Validation loss 45.7450\n",
            "Epoch 150, Training loss 47.4883, Validation loss 45.1883\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1789, item_Code: A265520\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8107, Validation loss 60.0210\n",
            "Epoch 50, Training loss 50.0872, Validation loss 53.9590\n",
            "Epoch 100, Training loss 48.8198, Validation loss 53.4736\n",
            "Epoch 150, Training loss 48.1853, Validation loss 53.6684\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1790, item_Code: A265560\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 91)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 63.0823, Validation loss 62.5676\n",
            "Epoch 50, Training loss 51.7929, Validation loss 49.1691\n",
            "Epoch 100, Training loss 50.6348, Validation loss 48.6361\n",
            "Epoch 150, Training loss 50.2575, Validation loss 48.5312\n",
            "torch.Size([543, 90])\n",
            "torch.Size([543, 91])\n",
            "--------------------\n",
            "count: 1791, item_Code: A265740\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.5791, Validation loss 60.7476\n",
            "Epoch 50, Training loss 49.2568, Validation loss 47.1483\n",
            "Epoch 100, Training loss 48.2389, Validation loss 46.0631\n",
            "Epoch 150, Training loss 47.7287, Validation loss 45.3539\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1792, item_Code: A267250\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.1393, Validation loss 60.7587\n",
            "Epoch 50, Training loss 52.5611, Validation loss 56.2508\n",
            "Epoch 100, Training loss 51.9451, Validation loss 56.2942\n",
            "Epoch 150, Training loss 51.2598, Validation loss 55.9400\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1793, item_Code: A267260\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8693, Validation loss 61.3663\n",
            "Epoch 50, Training loss 47.2552, Validation loss 64.0516\n",
            "Epoch 100, Training loss 46.8504, Validation loss 63.6259\n",
            "Epoch 150, Training loss 46.2112, Validation loss 62.5872\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1794, item_Code: A267270\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.7715, Validation loss 59.8218\n",
            "Epoch 50, Training loss 48.9819, Validation loss 60.1497\n",
            "Epoch 100, Training loss 48.5219, Validation loss 58.5538\n",
            "Epoch 150, Training loss 48.3791, Validation loss 58.7282\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1795, item_Code: A267290\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.8680, Validation loss 61.4621\n",
            "Epoch 50, Training loss 54.4469, Validation loss 56.2751\n",
            "Epoch 100, Training loss 53.7497, Validation loss 54.6551\n",
            "Epoch 150, Training loss 53.2100, Validation loss 54.3187\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1796, item_Code: A267320\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.9964, Validation loss 61.4739\n",
            "Epoch 50, Training loss 50.6656, Validation loss 53.3754\n",
            "Epoch 100, Training loss 49.9732, Validation loss 53.0235\n",
            "Epoch 150, Training loss 49.3262, Validation loss 52.6415\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1797, item_Code: A267790\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.8500, Validation loss 58.6571\n",
            "Epoch 50, Training loss 49.9607, Validation loss 54.1433\n",
            "Epoch 100, Training loss 48.9366, Validation loss 54.5295\n",
            "Epoch 150, Training loss 48.5006, Validation loss 54.2909\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1798, item_Code: A267850\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3050, Validation loss 59.5981\n",
            "Epoch 50, Training loss 49.6532, Validation loss 51.9207\n",
            "Epoch 100, Training loss 48.6777, Validation loss 51.7421\n",
            "Epoch 150, Training loss 48.1271, Validation loss 51.6959\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1799, item_Code: A267980\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.9684, Validation loss 57.7230\n",
            "Epoch 50, Training loss 48.7535, Validation loss 50.5630\n",
            "Epoch 100, Training loss 48.1019, Validation loss 49.9977\n",
            "Epoch 150, Training loss 47.6376, Validation loss 49.2304\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1800, item_Code: A268280\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7849, Validation loss 61.2701\n",
            "Epoch 50, Training loss 52.2497, Validation loss 54.8239\n",
            "Epoch 100, Training loss 51.3652, Validation loss 51.9874\n",
            "Epoch 150, Training loss 50.8429, Validation loss 51.3793\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1801, item_Code: A268600\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 83)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.7205, Validation loss 57.7903\n",
            "Epoch 50, Training loss 46.3909, Validation loss 45.6234\n",
            "Epoch 100, Training loss 45.9292, Validation loss 45.0518\n",
            "Epoch 150, Training loss 45.0755, Validation loss 44.4342\n",
            "torch.Size([543, 82])\n",
            "torch.Size([543, 83])\n",
            "--------------------\n",
            "count: 1802, item_Code: A269620\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.9875, Validation loss 57.5918\n",
            "Epoch 50, Training loss 39.7177, Validation loss 31.9429\n",
            "Epoch 100, Training loss 38.7071, Validation loss 31.2570\n",
            "Epoch 150, Training loss 38.0680, Validation loss 31.3208\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1803, item_Code: A270520\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.1079, Validation loss 62.0047\n",
            "Epoch 50, Training loss 42.7168, Validation loss 77.4013\n",
            "Epoch 100, Training loss 42.1557, Validation loss 78.2263\n",
            "Epoch 150, Training loss 41.9844, Validation loss 78.4908\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1804, item_Code: A270660\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 91)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 63.1318, Validation loss 63.2437\n",
            "Epoch 50, Training loss 52.8701, Validation loss 49.2109\n",
            "Epoch 100, Training loss 52.0614, Validation loss 48.5121\n",
            "Epoch 150, Training loss 51.4271, Validation loss 48.0457\n",
            "torch.Size([543, 90])\n",
            "torch.Size([543, 91])\n",
            "--------------------\n",
            "count: 1805, item_Code: A270870\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 83)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.0498, Validation loss 58.2926\n",
            "Epoch 50, Training loss 46.9378, Validation loss 45.1328\n",
            "Epoch 100, Training loss 46.3273, Validation loss 44.4110\n",
            "Epoch 150, Training loss 45.4448, Validation loss 43.6529\n",
            "torch.Size([543, 82])\n",
            "torch.Size([543, 83])\n",
            "--------------------\n",
            "count: 1806, item_Code: A271560\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.0242, Validation loss 61.3562\n",
            "Epoch 50, Training loss 53.5552, Validation loss 59.0416\n",
            "Epoch 100, Training loss 52.7545, Validation loss 58.4845\n",
            "Epoch 150, Training loss 52.4144, Validation loss 57.2559\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1807, item_Code: A271980\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.0145, Validation loss 58.5888\n",
            "Epoch 50, Training loss 47.0801, Validation loss 44.5160\n",
            "Epoch 100, Training loss 46.6826, Validation loss 43.5126\n",
            "Epoch 150, Training loss 46.0897, Validation loss 43.3395\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1808, item_Code: A272110\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.2964, Validation loss 61.8092\n",
            "Epoch 50, Training loss 49.7269, Validation loss 70.2032\n",
            "Epoch 100, Training loss 49.3661, Validation loss 70.6913\n",
            "Epoch 150, Training loss 48.9134, Validation loss 69.8117\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1809, item_Code: A272210\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.0509, Validation loss 61.7188\n",
            "Epoch 50, Training loss 51.2813, Validation loss 56.8906\n",
            "Epoch 100, Training loss 50.6229, Validation loss 56.8934\n",
            "Epoch 150, Training loss 50.4089, Validation loss 56.8906\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1810, item_Code: A272290\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.3083, Validation loss 57.9165\n",
            "Epoch 50, Training loss 47.6311, Validation loss 51.0833\n",
            "Epoch 100, Training loss 46.7303, Validation loss 51.4335\n",
            "Epoch 150, Training loss 46.2092, Validation loss 50.5037\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1811, item_Code: A272450\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.9274, Validation loss 62.1872\n",
            "Epoch 50, Training loss 51.3692, Validation loss 54.2245\n",
            "Epoch 100, Training loss 50.2669, Validation loss 54.1819\n",
            "Epoch 150, Training loss 49.6779, Validation loss 53.5887\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1812, item_Code: A272550\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.2687, Validation loss 61.3416\n",
            "Epoch 50, Training loss 49.9197, Validation loss 47.8400\n",
            "Epoch 100, Training loss 48.7920, Validation loss 47.0778\n",
            "Epoch 150, Training loss 48.5296, Validation loss 46.8773\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1813, item_Code: A273060\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.4123, Validation loss 61.8757\n",
            "Epoch 50, Training loss 50.2772, Validation loss 55.7924\n",
            "Epoch 100, Training loss 49.6502, Validation loss 55.4072\n",
            "Epoch 150, Training loss 49.1827, Validation loss 55.1382\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1814, item_Code: A274090\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.3865, Validation loss 60.4319\n",
            "Epoch 50, Training loss 52.8683, Validation loss 59.2019\n",
            "Epoch 100, Training loss 52.2431, Validation loss 58.3415\n",
            "Epoch 150, Training loss 51.6856, Validation loss 57.9051\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1815, item_Code: A276730\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.6742, Validation loss 59.0843\n",
            "Epoch 50, Training loss 49.4130, Validation loss 45.7557\n",
            "Epoch 100, Training loss 48.8501, Validation loss 45.0021\n",
            "Epoch 150, Training loss 48.1527, Validation loss 44.5302\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1816, item_Code: A277070\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3265, Validation loss 60.2309\n",
            "Epoch 50, Training loss 48.5966, Validation loss 59.6400\n",
            "Epoch 100, Training loss 48.0849, Validation loss 59.8459\n",
            "Epoch 150, Training loss 47.5841, Validation loss 60.0672\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1817, item_Code: A277410\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.8054, Validation loss 59.2912\n",
            "Epoch 50, Training loss 48.6326, Validation loss 53.8381\n",
            "Epoch 100, Training loss 47.9983, Validation loss 54.1834\n",
            "Epoch 150, Training loss 47.4154, Validation loss 53.8422\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1818, item_Code: A277810\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.3816, Validation loss 62.0008\n",
            "Epoch 50, Training loss 42.0463, Validation loss 72.7741\n",
            "Epoch 100, Training loss 41.3477, Validation loss 69.5889\n",
            "Epoch 150, Training loss 40.9938, Validation loss 66.4850\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1819, item_Code: A277880\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3965, Validation loss 60.5988\n",
            "Epoch 50, Training loss 51.0192, Validation loss 59.4677\n",
            "Epoch 100, Training loss 50.1457, Validation loss 58.1388\n",
            "Epoch 150, Training loss 49.6527, Validation loss 57.7537\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1820, item_Code: A278280\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.4550, Validation loss 62.1601\n",
            "Epoch 50, Training loss 54.0537, Validation loss 55.6507\n",
            "Epoch 100, Training loss 53.1372, Validation loss 55.3060\n",
            "Epoch 150, Training loss 52.7946, Validation loss 54.7922\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1821, item_Code: A278650\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.0202, Validation loss 58.3671\n",
            "Epoch 50, Training loss 48.1777, Validation loss 42.6626\n",
            "Epoch 100, Training loss 47.6428, Validation loss 42.0650\n",
            "Epoch 150, Training loss 47.3600, Validation loss 41.2658\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1822, item_Code: A279600\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0128, Validation loss 60.7664\n",
            "Epoch 50, Training loss 51.2432, Validation loss 57.0141\n",
            "Epoch 100, Training loss 50.2917, Validation loss 57.0111\n",
            "Epoch 150, Training loss 49.6457, Validation loss 56.7043\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1823, item_Code: A280360\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.1789, Validation loss 61.4181\n",
            "Epoch 50, Training loss 53.6654, Validation loss 56.7072\n",
            "Epoch 100, Training loss 53.2427, Validation loss 56.3676\n",
            "Epoch 150, Training loss 52.6706, Validation loss 55.3514\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1824, item_Code: A281740\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7420, Validation loss 60.9929\n",
            "Epoch 50, Training loss 39.8464, Validation loss 88.5314\n",
            "Epoch 100, Training loss 39.4507, Validation loss 82.9054\n",
            "Epoch 150, Training loss 39.0272, Validation loss 81.3606\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1825, item_Code: A281820\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.2746, Validation loss 57.6083\n",
            "Epoch 50, Training loss 48.3262, Validation loss 53.8819\n",
            "Epoch 100, Training loss 47.5959, Validation loss 53.3474\n",
            "Epoch 150, Training loss 47.1794, Validation loss 53.2717\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1826, item_Code: A282330\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.2451, Validation loss 60.3476\n",
            "Epoch 50, Training loss 52.7967, Validation loss 55.9496\n",
            "Epoch 100, Training loss 51.9163, Validation loss 55.6832\n",
            "Epoch 150, Training loss 51.4309, Validation loss 55.9243\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1827, item_Code: A282690\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.7892, Validation loss 61.5054\n",
            "Epoch 50, Training loss 53.5903, Validation loss 55.3549\n",
            "Epoch 100, Training loss 52.3556, Validation loss 55.4496\n",
            "Epoch 150, Training loss 52.1111, Validation loss 55.4743\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1828, item_Code: A282880\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.2829, Validation loss 61.8987\n",
            "Epoch 50, Training loss 50.0058, Validation loss 60.7805\n",
            "Epoch 100, Training loss 49.4563, Validation loss 60.8799\n",
            "Epoch 150, Training loss 49.2414, Validation loss 60.8424\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1829, item_Code: A284620\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.4744, Validation loss 60.4306\n",
            "Epoch 50, Training loss 50.1871, Validation loss 55.0755\n",
            "Epoch 100, Training loss 49.6072, Validation loss 54.6274\n",
            "Epoch 150, Training loss 49.1557, Validation loss 54.3192\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1830, item_Code: A284740\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7899, Validation loss 60.6997\n",
            "Epoch 50, Training loss 50.9947, Validation loss 50.0031\n",
            "Epoch 100, Training loss 50.4080, Validation loss 49.8518\n",
            "Epoch 150, Training loss 49.7623, Validation loss 49.3813\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1831, item_Code: A285130\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.5676, Validation loss 57.7740\n",
            "Epoch 50, Training loss 46.2229, Validation loss 40.9647\n",
            "Epoch 100, Training loss 45.1516, Validation loss 40.3846\n",
            "Epoch 150, Training loss 44.7581, Validation loss 40.0490\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1832, item_Code: A285490\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.0206, Validation loss 62.0582\n",
            "Epoch 50, Training loss 49.4774, Validation loss 59.4454\n",
            "Epoch 100, Training loss 48.6236, Validation loss 59.7622\n",
            "Epoch 150, Training loss 48.1621, Validation loss 59.8843\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1833, item_Code: A286750\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 83)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.1900, Validation loss 58.5822\n",
            "Epoch 50, Training loss 44.2444, Validation loss 45.8325\n",
            "Epoch 100, Training loss 43.6418, Validation loss 45.6943\n",
            "Epoch 150, Training loss 43.3562, Validation loss 45.8305\n",
            "torch.Size([543, 82])\n",
            "torch.Size([543, 83])\n",
            "--------------------\n",
            "count: 1834, item_Code: A286940\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.2341, Validation loss 59.8971\n",
            "Epoch 50, Training loss 49.1528, Validation loss 53.7321\n",
            "Epoch 100, Training loss 48.4459, Validation loss 53.7619\n",
            "Epoch 150, Training loss 47.7299, Validation loss 53.6868\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1835, item_Code: A287410\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.6128, Validation loss 60.6959\n",
            "Epoch 50, Training loss 52.1014, Validation loss 58.6237\n",
            "Epoch 100, Training loss 51.3698, Validation loss 56.9238\n",
            "Epoch 150, Training loss 50.7663, Validation loss 56.2294\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1836, item_Code: A288330\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7642, Validation loss 62.0957\n",
            "Epoch 50, Training loss 50.7941, Validation loss 59.7577\n",
            "Epoch 100, Training loss 50.3446, Validation loss 59.7759\n",
            "Epoch 150, Training loss 49.8890, Validation loss 58.6224\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1837, item_Code: A288620\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7347, Validation loss 60.3520\n",
            "Epoch 50, Training loss 50.5878, Validation loss 51.1721\n",
            "Epoch 100, Training loss 50.1894, Validation loss 50.8782\n",
            "Epoch 150, Training loss 49.4331, Validation loss 49.8764\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1838, item_Code: A289010\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.1980, Validation loss 59.0165\n",
            "Epoch 50, Training loss 47.5988, Validation loss 49.0205\n",
            "Epoch 100, Training loss 46.9640, Validation loss 48.6341\n",
            "Epoch 150, Training loss 46.3812, Validation loss 48.5240\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1839, item_Code: A289080\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8246, Validation loss 60.3961\n",
            "Epoch 50, Training loss 50.3242, Validation loss 47.2988\n",
            "Epoch 100, Training loss 49.5480, Validation loss 46.9698\n",
            "Epoch 150, Training loss 48.7319, Validation loss 46.0504\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1840, item_Code: A289220\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3353, Validation loss 59.2906\n",
            "Epoch 50, Training loss 49.2572, Validation loss 43.4853\n",
            "Epoch 100, Training loss 48.1271, Validation loss 42.8189\n",
            "Epoch 150, Training loss 47.6134, Validation loss 42.2936\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1841, item_Code: A290120\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3544, Validation loss 60.1066\n",
            "Epoch 50, Training loss 50.6164, Validation loss 50.1707\n",
            "Epoch 100, Training loss 50.0235, Validation loss 50.0906\n",
            "Epoch 150, Training loss 49.2864, Validation loss 49.8738\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1842, item_Code: A290380\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 83)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.7716, Validation loss 58.7768\n",
            "Epoch 50, Training loss 47.9865, Validation loss 50.9895\n",
            "Epoch 100, Training loss 47.2044, Validation loss 49.6361\n",
            "Epoch 150, Training loss 46.9696, Validation loss 49.6146\n",
            "torch.Size([543, 82])\n",
            "torch.Size([543, 83])\n",
            "--------------------\n",
            "count: 1843, item_Code: A290520\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 83)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.3583, Validation loss 57.8616\n",
            "Epoch 50, Training loss 45.1186, Validation loss 50.5656\n",
            "Epoch 100, Training loss 44.4286, Validation loss 50.0113\n",
            "Epoch 150, Training loss 43.6741, Validation loss 49.7788\n",
            "torch.Size([543, 82])\n",
            "torch.Size([543, 83])\n",
            "--------------------\n",
            "count: 1844, item_Code: A290550\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.2114, Validation loss 58.7283\n",
            "Epoch 50, Training loss 48.2619, Validation loss 47.1896\n",
            "Epoch 100, Training loss 47.5900, Validation loss 47.1743\n",
            "Epoch 150, Training loss 46.9388, Validation loss 46.6920\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1845, item_Code: A290650\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.3156, Validation loss 59.1936\n",
            "Epoch 50, Training loss 50.9943, Validation loss 57.6819\n",
            "Epoch 100, Training loss 49.9773, Validation loss 57.6074\n",
            "Epoch 150, Training loss 49.4114, Validation loss 57.1987\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1846, item_Code: A290670\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3917, Validation loss 60.3661\n",
            "Epoch 50, Training loss 49.1253, Validation loss 55.3769\n",
            "Epoch 100, Training loss 48.4949, Validation loss 54.5142\n",
            "Epoch 150, Training loss 47.8673, Validation loss 53.9454\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1847, item_Code: A290690\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.7300, Validation loss 57.7557\n",
            "Epoch 50, Training loss 44.4145, Validation loss 57.2248\n",
            "Epoch 100, Training loss 43.8393, Validation loss 57.7244\n",
            "Epoch 150, Training loss 43.4218, Validation loss 58.2506\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1848, item_Code: A290720\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.5239, Validation loss 60.4882\n",
            "Epoch 50, Training loss 51.7120, Validation loss 49.5091\n",
            "Epoch 100, Training loss 51.1596, Validation loss 48.1434\n",
            "Epoch 150, Training loss 50.7016, Validation loss 47.3002\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1849, item_Code: A290740\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.1307, Validation loss 60.4505\n",
            "Epoch 50, Training loss 49.1430, Validation loss 55.8225\n",
            "Epoch 100, Training loss 48.2161, Validation loss 54.9861\n",
            "Epoch 150, Training loss 47.4309, Validation loss 54.6768\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1850, item_Code: A291230\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3531, Validation loss 60.3349\n",
            "Epoch 50, Training loss 52.3696, Validation loss 49.9140\n",
            "Epoch 100, Training loss 51.2564, Validation loss 49.9070\n",
            "Epoch 150, Training loss 50.6772, Validation loss 49.4700\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1851, item_Code: A291650\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7796, Validation loss 60.8852\n",
            "Epoch 50, Training loss 50.5422, Validation loss 49.3138\n",
            "Epoch 100, Training loss 49.8464, Validation loss 48.3978\n",
            "Epoch 150, Training loss 49.2321, Validation loss 47.2716\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1852, item_Code: A293480\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.9329, Validation loss 58.3102\n",
            "Epoch 50, Training loss 47.8506, Validation loss 48.0680\n",
            "Epoch 100, Training loss 46.8092, Validation loss 46.5786\n",
            "Epoch 150, Training loss 46.5782, Validation loss 46.2280\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1853, item_Code: A293490\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.2460, Validation loss 61.2555\n",
            "Epoch 50, Training loss 52.4825, Validation loss 49.8428\n",
            "Epoch 100, Training loss 51.5067, Validation loss 49.1595\n",
            "Epoch 150, Training loss 51.2880, Validation loss 48.7790\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1854, item_Code: A293580\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.5239, Validation loss 59.5830\n",
            "Epoch 50, Training loss 48.7039, Validation loss 60.3016\n",
            "Epoch 100, Training loss 48.0942, Validation loss 59.0923\n",
            "Epoch 150, Training loss 47.8049, Validation loss 58.5895\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1855, item_Code: A293780\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 81)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 55.6191, Validation loss 55.4870\n",
            "Epoch 50, Training loss 44.0931, Validation loss 40.0436\n",
            "Epoch 100, Training loss 43.3786, Validation loss 39.2256\n",
            "Epoch 150, Training loss 42.9372, Validation loss 38.5866\n",
            "torch.Size([543, 80])\n",
            "torch.Size([543, 81])\n",
            "--------------------\n",
            "count: 1856, item_Code: A294090\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.6907, Validation loss 60.3569\n",
            "Epoch 50, Training loss 46.1203, Validation loss 55.8165\n",
            "Epoch 100, Training loss 45.2586, Validation loss 55.8695\n",
            "Epoch 150, Training loss 44.8209, Validation loss 55.3180\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1857, item_Code: A294140\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3257, Validation loss 60.3360\n",
            "Epoch 50, Training loss 47.1139, Validation loss 60.3858\n",
            "Epoch 100, Training loss 46.6606, Validation loss 61.1947\n",
            "Epoch 150, Training loss 46.4401, Validation loss 61.3055\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1858, item_Code: A294570\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.7908, Validation loss 59.1836\n",
            "Epoch 50, Training loss 48.5658, Validation loss 46.4264\n",
            "Epoch 100, Training loss 48.4104, Validation loss 46.0344\n",
            "Epoch 150, Training loss 47.9983, Validation loss 45.7982\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1859, item_Code: A294630\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.5232, Validation loss 58.4633\n",
            "Epoch 50, Training loss 46.3673, Validation loss 55.6887\n",
            "Epoch 100, Training loss 45.7054, Validation loss 55.9164\n",
            "Epoch 150, Training loss 45.0544, Validation loss 55.9525\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1860, item_Code: A294870\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3890, Validation loss 60.0132\n",
            "Epoch 50, Training loss 45.4940, Validation loss 43.3301\n",
            "Epoch 100, Training loss 44.5505, Validation loss 42.9308\n",
            "Epoch 150, Training loss 43.4266, Validation loss 42.2457\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1861, item_Code: A297090\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7401, Validation loss 60.7144\n",
            "Epoch 50, Training loss 48.5241, Validation loss 51.3657\n",
            "Epoch 100, Training loss 47.5329, Validation loss 50.7613\n",
            "Epoch 150, Training loss 46.9127, Validation loss 50.4718\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1862, item_Code: A297570\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0171, Validation loss 60.8267\n",
            "Epoch 50, Training loss 47.0529, Validation loss 42.9831\n",
            "Epoch 100, Training loss 46.4444, Validation loss 42.5056\n",
            "Epoch 150, Training loss 45.6936, Validation loss 42.0070\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1863, item_Code: A297890\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.2862, Validation loss 61.1788\n",
            "Epoch 50, Training loss 51.4405, Validation loss 55.7851\n",
            "Epoch 100, Training loss 50.5918, Validation loss 55.5567\n",
            "Epoch 150, Training loss 50.3967, Validation loss 55.3344\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1864, item_Code: A298000\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 82)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 56.6059, Validation loss 56.6053\n",
            "Epoch 50, Training loss 47.1854, Validation loss 44.5556\n",
            "Epoch 100, Training loss 46.7834, Validation loss 44.0590\n",
            "Epoch 150, Training loss 46.5949, Validation loss 44.0342\n",
            "torch.Size([543, 81])\n",
            "torch.Size([543, 82])\n",
            "--------------------\n",
            "count: 1865, item_Code: A298020\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.1583, Validation loss 57.3085\n",
            "Epoch 50, Training loss 47.5028, Validation loss 50.2431\n",
            "Epoch 100, Training loss 46.4591, Validation loss 49.6483\n",
            "Epoch 150, Training loss 45.9100, Validation loss 49.5070\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1866, item_Code: A298040\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8562, Validation loss 61.6297\n",
            "Epoch 50, Training loss 49.7288, Validation loss 63.2177\n",
            "Epoch 100, Training loss 49.0010, Validation loss 62.4270\n",
            "Epoch 150, Training loss 48.5100, Validation loss 61.3789\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1867, item_Code: A298050\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.4827, Validation loss 57.5937\n",
            "Epoch 50, Training loss 49.3172, Validation loss 52.3882\n",
            "Epoch 100, Training loss 48.2428, Validation loss 52.1115\n",
            "Epoch 150, Training loss 47.7844, Validation loss 52.2675\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1868, item_Code: A298060\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 83)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.2769, Validation loss 56.7325\n",
            "Epoch 50, Training loss 46.9011, Validation loss 48.1825\n",
            "Epoch 100, Training loss 46.0033, Validation loss 47.3248\n",
            "Epoch 150, Training loss 45.4444, Validation loss 47.1067\n",
            "torch.Size([543, 82])\n",
            "torch.Size([543, 83])\n",
            "--------------------\n",
            "count: 1869, item_Code: A298380\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.1896, Validation loss 59.7910\n",
            "Epoch 50, Training loss 50.4574, Validation loss 51.7155\n",
            "Epoch 100, Training loss 50.0661, Validation loss 51.2410\n",
            "Epoch 150, Training loss 49.2911, Validation loss 50.9400\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1870, item_Code: A298540\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.1223, Validation loss 61.3990\n",
            "Epoch 50, Training loss 50.3879, Validation loss 47.6343\n",
            "Epoch 100, Training loss 49.3146, Validation loss 47.8687\n",
            "Epoch 150, Training loss 48.8147, Validation loss 47.3754\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1871, item_Code: A298690\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.4060, Validation loss 59.8902\n",
            "Epoch 50, Training loss 51.2990, Validation loss 54.1624\n",
            "Epoch 100, Training loss 50.6164, Validation loss 53.0221\n",
            "Epoch 150, Training loss 50.0215, Validation loss 52.6295\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1872, item_Code: A299030\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.9843, Validation loss 61.4325\n",
            "Epoch 50, Training loss 46.2594, Validation loss 62.2009\n",
            "Epoch 100, Training loss 45.8590, Validation loss 62.2960\n",
            "Epoch 150, Training loss 45.5612, Validation loss 62.3152\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1873, item_Code: A299170\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8023, Validation loss 61.0326\n",
            "Epoch 50, Training loss 50.5750, Validation loss 53.5386\n",
            "Epoch 100, Training loss 50.0712, Validation loss 53.5460\n",
            "Epoch 150, Training loss 49.2117, Validation loss 53.3812\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1874, item_Code: A299660\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.7018, Validation loss 57.9061\n",
            "Epoch 50, Training loss 45.3887, Validation loss 39.0583\n",
            "Epoch 100, Training loss 44.6454, Validation loss 39.0012\n",
            "Epoch 150, Training loss 43.8789, Validation loss 37.8477\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1875, item_Code: A299900\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.5517, Validation loss 60.7006\n",
            "Epoch 50, Training loss 52.5808, Validation loss 53.8367\n",
            "Epoch 100, Training loss 52.0695, Validation loss 54.0792\n",
            "Epoch 150, Training loss 51.8524, Validation loss 53.8137\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1876, item_Code: A300080\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.2883, Validation loss 60.8526\n",
            "Epoch 50, Training loss 52.7769, Validation loss 58.5713\n",
            "Epoch 100, Training loss 51.6506, Validation loss 58.3948\n",
            "Epoch 150, Training loss 51.1984, Validation loss 58.4298\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1877, item_Code: A300120\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.4710, Validation loss 60.2537\n",
            "Epoch 50, Training loss 48.4045, Validation loss 50.3992\n",
            "Epoch 100, Training loss 47.7039, Validation loss 50.5304\n",
            "Epoch 150, Training loss 47.3840, Validation loss 50.0805\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1878, item_Code: A300720\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.5299, Validation loss 58.2758\n",
            "Epoch 50, Training loss 41.2689, Validation loss 37.9858\n",
            "Epoch 100, Training loss 40.1813, Validation loss 36.9183\n",
            "Epoch 150, Training loss 39.7054, Validation loss 36.7024\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1879, item_Code: A301300\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.7051, Validation loss 59.9698\n",
            "Epoch 50, Training loss 52.0688, Validation loss 51.9485\n",
            "Epoch 100, Training loss 51.3189, Validation loss 51.6356\n",
            "Epoch 150, Training loss 50.8304, Validation loss 50.4801\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1880, item_Code: A302430\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.8155, Validation loss 59.4539\n",
            "Epoch 50, Training loss 50.3946, Validation loss 56.7258\n",
            "Epoch 100, Training loss 49.4770, Validation loss 55.8364\n",
            "Epoch 150, Training loss 48.7131, Validation loss 55.6400\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1881, item_Code: A302440\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.4318, Validation loss 59.8251\n",
            "Epoch 50, Training loss 48.0654, Validation loss 42.1774\n",
            "Epoch 100, Training loss 47.1812, Validation loss 41.8136\n",
            "Epoch 150, Training loss 46.8049, Validation loss 41.5723\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1882, item_Code: A302550\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.6381, Validation loss 59.9607\n",
            "Epoch 50, Training loss 47.6029, Validation loss 49.1543\n",
            "Epoch 100, Training loss 47.0728, Validation loss 48.9580\n",
            "Epoch 150, Training loss 46.1344, Validation loss 47.7052\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1883, item_Code: A303030\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.2453, Validation loss 60.3876\n",
            "Epoch 50, Training loss 50.5994, Validation loss 50.5679\n",
            "Epoch 100, Training loss 50.1248, Validation loss 50.4263\n",
            "Epoch 150, Training loss 49.1158, Validation loss 49.6602\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1884, item_Code: A303360\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.5991, Validation loss 61.9745\n",
            "Epoch 50, Training loss 50.3924, Validation loss 57.7251\n",
            "Epoch 100, Training loss 50.0016, Validation loss 57.8426\n",
            "Epoch 150, Training loss 49.6087, Validation loss 57.7799\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1885, item_Code: A304100\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.9098, Validation loss 58.6424\n",
            "Epoch 50, Training loss 47.1747, Validation loss 60.6111\n",
            "Epoch 100, Training loss 46.1529, Validation loss 58.4398\n",
            "Epoch 150, Training loss 45.6157, Validation loss 57.2464\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1886, item_Code: A304840\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.6504, Validation loss 60.5334\n",
            "Epoch 50, Training loss 49.8493, Validation loss 50.7471\n",
            "Epoch 100, Training loss 48.8829, Validation loss 50.2039\n",
            "Epoch 150, Training loss 48.2683, Validation loss 50.2103\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1887, item_Code: A305090\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.1272, Validation loss 57.2143\n",
            "Epoch 50, Training loss 46.2621, Validation loss 49.5367\n",
            "Epoch 100, Training loss 45.4085, Validation loss 48.9698\n",
            "Epoch 150, Training loss 44.7692, Validation loss 48.2670\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1888, item_Code: A306040\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7891, Validation loss 60.6885\n",
            "Epoch 50, Training loss 52.0574, Validation loss 53.2580\n",
            "Epoch 100, Training loss 51.2297, Validation loss 52.0188\n",
            "Epoch 150, Training loss 50.9800, Validation loss 51.7698\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1889, item_Code: A306200\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.1149, Validation loss 61.5778\n",
            "Epoch 50, Training loss 50.9890, Validation loss 57.8015\n",
            "Epoch 100, Training loss 49.6216, Validation loss 57.0486\n",
            "Epoch 150, Training loss 49.4208, Validation loss 57.1730\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1890, item_Code: A306620\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.1603, Validation loss 59.2104\n",
            "Epoch 50, Training loss 51.3125, Validation loss 52.7151\n",
            "Epoch 100, Training loss 50.8096, Validation loss 51.8092\n",
            "Epoch 150, Training loss 50.3934, Validation loss 51.8242\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1891, item_Code: A307180\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.2097, Validation loss 59.6724\n",
            "Epoch 50, Training loss 49.5742, Validation loss 52.1086\n",
            "Epoch 100, Training loss 48.6434, Validation loss 51.9437\n",
            "Epoch 150, Training loss 48.0487, Validation loss 51.9555\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1892, item_Code: A307280\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 83)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.3541, Validation loss 56.8816\n",
            "Epoch 50, Training loss 47.7582, Validation loss 53.2668\n",
            "Epoch 100, Training loss 47.3260, Validation loss 53.4298\n",
            "Epoch 150, Training loss 46.7168, Validation loss 52.8521\n",
            "torch.Size([543, 82])\n",
            "torch.Size([543, 83])\n",
            "--------------------\n",
            "count: 1893, item_Code: A307750\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.1629, Validation loss 61.0406\n",
            "Epoch 50, Training loss 52.2298, Validation loss 48.9596\n",
            "Epoch 100, Training loss 51.3595, Validation loss 48.8294\n",
            "Epoch 150, Training loss 50.8582, Validation loss 48.6766\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1894, item_Code: A307870\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.5797, Validation loss 60.5291\n",
            "Epoch 50, Training loss 49.0246, Validation loss 53.2220\n",
            "Epoch 100, Training loss 48.3602, Validation loss 52.8650\n",
            "Epoch 150, Training loss 47.9748, Validation loss 52.7117\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1895, item_Code: A307930\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8367, Validation loss 61.0632\n",
            "Epoch 50, Training loss 50.8256, Validation loss 54.6160\n",
            "Epoch 100, Training loss 50.2611, Validation loss 54.4625\n",
            "Epoch 150, Training loss 49.6389, Validation loss 53.9158\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1896, item_Code: A307950\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.8964, Validation loss 61.9067\n",
            "Epoch 50, Training loss 54.0154, Validation loss 57.6303\n",
            "Epoch 100, Training loss 53.5628, Validation loss 57.5561\n",
            "Epoch 150, Training loss 53.0353, Validation loss 56.8485\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1897, item_Code: A308100\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.9624, Validation loss 59.2386\n",
            "Epoch 50, Training loss 47.9690, Validation loss 49.1053\n",
            "Epoch 100, Training loss 47.0224, Validation loss 48.4362\n",
            "Epoch 150, Training loss 46.5038, Validation loss 47.8368\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1898, item_Code: A308170\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.9522, Validation loss 60.2666\n",
            "Epoch 50, Training loss 49.4010, Validation loss 50.7922\n",
            "Epoch 100, Training loss 48.5517, Validation loss 51.1079\n",
            "Epoch 150, Training loss 47.8400, Validation loss 50.5197\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1899, item_Code: A309930\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.5150, Validation loss 59.9346\n",
            "Epoch 50, Training loss 49.3575, Validation loss 56.1465\n",
            "Epoch 100, Training loss 48.5560, Validation loss 55.9974\n",
            "Epoch 150, Training loss 47.8437, Validation loss 55.5904\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1900, item_Code: A310200\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.3492, Validation loss 61.4855\n",
            "Epoch 50, Training loss 53.8615, Validation loss 57.4383\n",
            "Epoch 100, Training loss 52.7679, Validation loss 57.2558\n",
            "Epoch 150, Training loss 52.5312, Validation loss 57.1390\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1901, item_Code: A311390\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.9371, Validation loss 60.5952\n",
            "Epoch 50, Training loss 51.2491, Validation loss 48.0737\n",
            "Epoch 100, Training loss 50.6715, Validation loss 48.0959\n",
            "Epoch 150, Training loss 50.3972, Validation loss 47.9010\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1902, item_Code: A311690\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.8563, Validation loss 61.7856\n",
            "Epoch 50, Training loss 53.0201, Validation loss 54.6676\n",
            "Epoch 100, Training loss 52.5183, Validation loss 54.4555\n",
            "Epoch 150, Training loss 52.2132, Validation loss 54.4297\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1903, item_Code: A312610\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.6845, Validation loss 59.4956\n",
            "Epoch 50, Training loss 46.9596, Validation loss 50.3327\n",
            "Epoch 100, Training loss 46.6103, Validation loss 50.0669\n",
            "Epoch 150, Training loss 46.1309, Validation loss 49.6461\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1904, item_Code: A313760\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.3992, Validation loss 62.2323\n",
            "Epoch 50, Training loss 51.4942, Validation loss 52.3800\n",
            "Epoch 100, Training loss 50.8702, Validation loss 51.6948\n",
            "Epoch 150, Training loss 49.9448, Validation loss 51.2811\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1905, item_Code: A314130\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.6253, Validation loss 62.7349\n",
            "Epoch 50, Training loss 50.8946, Validation loss 50.5859\n",
            "Epoch 100, Training loss 49.6550, Validation loss 49.4401\n",
            "Epoch 150, Training loss 49.6809, Validation loss 49.1938\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1906, item_Code: A314930\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.0670, Validation loss 59.0762\n",
            "Epoch 50, Training loss 50.2644, Validation loss 58.4639\n",
            "Epoch 100, Training loss 49.5635, Validation loss 58.7060\n",
            "Epoch 150, Training loss 48.9855, Validation loss 58.8404\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1907, item_Code: A316140\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.7914, Validation loss 58.8856\n",
            "Epoch 50, Training loss 49.9142, Validation loss 51.0383\n",
            "Epoch 100, Training loss 49.1002, Validation loss 50.8179\n",
            "Epoch 150, Training loss 48.4965, Validation loss 51.0010\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1908, item_Code: A317120\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 83)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.6452, Validation loss 57.4372\n",
            "Epoch 50, Training loss 46.8347, Validation loss 46.7304\n",
            "Epoch 100, Training loss 46.2024, Validation loss 46.3374\n",
            "Epoch 150, Training loss 45.8641, Validation loss 45.8602\n",
            "torch.Size([543, 82])\n",
            "torch.Size([543, 83])\n",
            "--------------------\n",
            "count: 1909, item_Code: A317240\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.6099, Validation loss 61.8624\n",
            "Epoch 50, Training loss 50.7975, Validation loss 53.5632\n",
            "Epoch 100, Training loss 50.4785, Validation loss 53.4763\n",
            "Epoch 150, Training loss 49.9845, Validation loss 52.0913\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1910, item_Code: A317330\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.2962, Validation loss 62.0238\n",
            "Epoch 50, Training loss 51.2099, Validation loss 55.2426\n",
            "Epoch 100, Training loss 50.2470, Validation loss 55.0277\n",
            "Epoch 150, Training loss 49.9466, Validation loss 55.3114\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1911, item_Code: A317400\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.5258, Validation loss 60.8215\n",
            "Epoch 50, Training loss 49.0176, Validation loss 50.1019\n",
            "Epoch 100, Training loss 48.3704, Validation loss 49.8110\n",
            "Epoch 150, Training loss 47.4557, Validation loss 49.0550\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1912, item_Code: A317530\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.6396, Validation loss 60.0240\n",
            "Epoch 50, Training loss 49.1428, Validation loss 48.4271\n",
            "Epoch 100, Training loss 48.4445, Validation loss 48.0410\n",
            "Epoch 150, Training loss 47.8698, Validation loss 47.7968\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1913, item_Code: A317690\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.4243, Validation loss 60.3581\n",
            "Epoch 50, Training loss 48.4160, Validation loss 42.9102\n",
            "Epoch 100, Training loss 46.9945, Validation loss 41.7827\n",
            "Epoch 150, Training loss 46.7279, Validation loss 41.7558\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1914, item_Code: A317770\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.2175, Validation loss 59.8679\n",
            "Epoch 50, Training loss 41.8703, Validation loss 70.2766\n",
            "Epoch 100, Training loss 41.3996, Validation loss 71.2092\n",
            "Epoch 150, Training loss 41.0063, Validation loss 73.1039\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1915, item_Code: A317830\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.6532, Validation loss 61.4960\n",
            "Epoch 50, Training loss 50.0738, Validation loss 62.4428\n",
            "Epoch 100, Training loss 49.6445, Validation loss 60.3264\n",
            "Epoch 150, Training loss 49.1048, Validation loss 59.0903\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1916, item_Code: A317850\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.2705, Validation loss 62.3786\n",
            "Epoch 50, Training loss 47.9121, Validation loss 54.3882\n",
            "Epoch 100, Training loss 46.8395, Validation loss 52.8545\n",
            "Epoch 150, Training loss 46.5850, Validation loss 52.0214\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1917, item_Code: A317870\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.4802, Validation loss 61.5812\n",
            "Epoch 50, Training loss 52.6527, Validation loss 60.6289\n",
            "Epoch 100, Training loss 51.7566, Validation loss 59.9407\n",
            "Epoch 150, Training loss 51.2610, Validation loss 59.3737\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1918, item_Code: A318000\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0323, Validation loss 61.8530\n",
            "Epoch 50, Training loss 47.7490, Validation loss 69.6467\n",
            "Epoch 100, Training loss 47.0969, Validation loss 69.1621\n",
            "Epoch 150, Training loss 46.9948, Validation loss 69.0888\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1919, item_Code: A318010\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0691, Validation loss 60.8180\n",
            "Epoch 50, Training loss 52.2077, Validation loss 52.9376\n",
            "Epoch 100, Training loss 51.2493, Validation loss 51.4783\n",
            "Epoch 150, Training loss 50.8588, Validation loss 51.1069\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1920, item_Code: A318020\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.7581, Validation loss 59.0699\n",
            "Epoch 50, Training loss 42.0994, Validation loss 35.7824\n",
            "Epoch 100, Training loss 41.0665, Validation loss 35.0633\n",
            "Epoch 150, Training loss 40.4311, Validation loss 34.6695\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1921, item_Code: A318410\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.9449, Validation loss 57.2871\n",
            "Epoch 50, Training loss 46.9004, Validation loss 46.3455\n",
            "Epoch 100, Training loss 45.9970, Validation loss 45.2574\n",
            "Epoch 150, Training loss 45.4001, Validation loss 44.7626\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1922, item_Code: A319400\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.9318, Validation loss 60.2678\n",
            "Epoch 50, Training loss 49.7195, Validation loss 59.7677\n",
            "Epoch 100, Training loss 49.0590, Validation loss 59.9735\n",
            "Epoch 150, Training loss 48.3899, Validation loss 59.1157\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1923, item_Code: A319660\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.6706, Validation loss 62.1819\n",
            "Epoch 50, Training loss 49.9354, Validation loss 52.6826\n",
            "Epoch 100, Training loss 49.2144, Validation loss 53.3556\n",
            "Epoch 150, Training loss 48.5385, Validation loss 53.2653\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1924, item_Code: A320000\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8647, Validation loss 61.1460\n",
            "Epoch 50, Training loss 47.1902, Validation loss 66.7946\n",
            "Epoch 100, Training loss 46.9203, Validation loss 67.0292\n",
            "Epoch 150, Training loss 46.4791, Validation loss 66.9616\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1925, item_Code: A321260\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.5131, Validation loss 61.3905\n",
            "Epoch 50, Training loss 51.2367, Validation loss 57.6448\n",
            "Epoch 100, Training loss 50.4345, Validation loss 57.8926\n",
            "Epoch 150, Training loss 50.0139, Validation loss 57.7460\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1926, item_Code: A321550\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.0910, Validation loss 60.5046\n",
            "Epoch 50, Training loss 51.5661, Validation loss 52.2274\n",
            "Epoch 100, Training loss 51.0229, Validation loss 51.9003\n",
            "Epoch 150, Training loss 50.4467, Validation loss 50.4662\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1927, item_Code: A322000\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.1452, Validation loss 60.1776\n",
            "Epoch 50, Training loss 47.8727, Validation loss 55.7562\n",
            "Epoch 100, Training loss 47.0678, Validation loss 55.8843\n",
            "Epoch 150, Training loss 46.3792, Validation loss 55.4037\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1928, item_Code: A322180\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.2467, Validation loss 60.6667\n",
            "Epoch 50, Training loss 50.7094, Validation loss 61.0972\n",
            "Epoch 100, Training loss 50.1885, Validation loss 61.1302\n",
            "Epoch 150, Training loss 49.2737, Validation loss 60.3462\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1929, item_Code: A322310\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.6008, Validation loss 59.8362\n",
            "Epoch 50, Training loss 48.3324, Validation loss 56.1994\n",
            "Epoch 100, Training loss 47.8955, Validation loss 55.8140\n",
            "Epoch 150, Training loss 47.3461, Validation loss 55.4093\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1930, item_Code: A322510\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.9685, Validation loss 59.0842\n",
            "Epoch 50, Training loss 40.8617, Validation loss 62.0586\n",
            "Epoch 100, Training loss 40.6479, Validation loss 61.0879\n",
            "Epoch 150, Training loss 40.1817, Validation loss 61.1860\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1931, item_Code: A322780\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.1888, Validation loss 59.6819\n",
            "Epoch 50, Training loss 51.2630, Validation loss 54.6232\n",
            "Epoch 100, Training loss 50.1557, Validation loss 54.1290\n",
            "Epoch 150, Training loss 49.9705, Validation loss 54.6321\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1932, item_Code: A323280\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.2221, Validation loss 61.9214\n",
            "Epoch 50, Training loss 53.3815, Validation loss 58.7760\n",
            "Epoch 100, Training loss 52.0461, Validation loss 58.3253\n",
            "Epoch 150, Training loss 51.7413, Validation loss 58.0633\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1933, item_Code: A323990\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.6449, Validation loss 60.6700\n",
            "Epoch 50, Training loss 49.2774, Validation loss 47.0286\n",
            "Epoch 100, Training loss 48.3891, Validation loss 46.4710\n",
            "Epoch 150, Training loss 47.8107, Validation loss 45.9916\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1934, item_Code: A326030\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.2217, Validation loss 58.7277\n",
            "Epoch 50, Training loss 48.7676, Validation loss 51.3808\n",
            "Epoch 100, Training loss 47.9320, Validation loss 50.3794\n",
            "Epoch 150, Training loss 47.2874, Validation loss 50.1130\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1935, item_Code: A327260\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3540, Validation loss 60.1661\n",
            "Epoch 50, Training loss 46.5859, Validation loss 48.7911\n",
            "Epoch 100, Training loss 45.7417, Validation loss 47.9285\n",
            "Epoch 150, Training loss 45.2032, Validation loss 47.9343\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1936, item_Code: A330350\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.9289, Validation loss 60.0315\n",
            "Epoch 50, Training loss 48.9328, Validation loss 45.2651\n",
            "Epoch 100, Training loss 48.1333, Validation loss 44.6758\n",
            "Epoch 150, Training loss 47.6560, Validation loss 44.2533\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1937, item_Code: A330860\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.7580, Validation loss 61.3599\n",
            "Epoch 50, Training loss 49.0748, Validation loss 53.5495\n",
            "Epoch 100, Training loss 48.3980, Validation loss 52.8665\n",
            "Epoch 150, Training loss 47.7277, Validation loss 52.3106\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1938, item_Code: A331520\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8809, Validation loss 61.3682\n",
            "Epoch 50, Training loss 49.1765, Validation loss 57.0170\n",
            "Epoch 100, Training loss 48.5368, Validation loss 54.8402\n",
            "Epoch 150, Training loss 47.8040, Validation loss 53.5400\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1939, item_Code: A332290\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.9593, Validation loss 61.0767\n",
            "Epoch 50, Training loss 51.6584, Validation loss 57.5247\n",
            "Epoch 100, Training loss 50.4061, Validation loss 57.2326\n",
            "Epoch 150, Training loss 49.7775, Validation loss 57.1850\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1940, item_Code: A332370\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.5181, Validation loss 59.1224\n",
            "Epoch 50, Training loss 49.2884, Validation loss 47.4728\n",
            "Epoch 100, Training loss 48.6079, Validation loss 46.9629\n",
            "Epoch 150, Training loss 48.0166, Validation loss 45.4414\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1941, item_Code: A332570\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.5793, Validation loss 58.6067\n",
            "Epoch 50, Training loss 46.8741, Validation loss 49.7576\n",
            "Epoch 100, Training loss 45.6460, Validation loss 49.1284\n",
            "Epoch 150, Training loss 45.3164, Validation loss 48.0460\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1942, item_Code: A333430\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.8317, Validation loss 61.0939\n",
            "Epoch 50, Training loss 51.5977, Validation loss 57.0977\n",
            "Epoch 100, Training loss 50.4597, Validation loss 56.4704\n",
            "Epoch 150, Training loss 50.0499, Validation loss 56.0593\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1943, item_Code: A333620\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.9938, Validation loss 61.0624\n",
            "Epoch 50, Training loss 49.4048, Validation loss 52.7033\n",
            "Epoch 100, Training loss 48.7816, Validation loss 52.3597\n",
            "Epoch 150, Training loss 48.1312, Validation loss 52.1097\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1944, item_Code: A334970\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.4236, Validation loss 59.4939\n",
            "Epoch 50, Training loss 46.9706, Validation loss 48.8379\n",
            "Epoch 100, Training loss 46.2105, Validation loss 48.3357\n",
            "Epoch 150, Training loss 45.3924, Validation loss 47.4986\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1945, item_Code: A335810\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.4757, Validation loss 59.2659\n",
            "Epoch 50, Training loss 46.6734, Validation loss 41.7610\n",
            "Epoch 100, Training loss 45.7032, Validation loss 41.1223\n",
            "Epoch 150, Training loss 45.2985, Validation loss 40.7002\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1946, item_Code: A335890\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8624, Validation loss 62.1286\n",
            "Epoch 50, Training loss 43.8359, Validation loss 75.4568\n",
            "Epoch 100, Training loss 43.1803, Validation loss 71.8255\n",
            "Epoch 150, Training loss 42.6973, Validation loss 69.2064\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1947, item_Code: A336060\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.0680, Validation loss 62.0413\n",
            "Epoch 50, Training loss 52.8233, Validation loss 57.0075\n",
            "Epoch 100, Training loss 51.5687, Validation loss 57.0450\n",
            "Epoch 150, Training loss 50.9941, Validation loss 56.8576\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1948, item_Code: A336260\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.1222, Validation loss 62.5675\n",
            "Epoch 50, Training loss 50.9794, Validation loss 51.6581\n",
            "Epoch 100, Training loss 50.3241, Validation loss 51.0994\n",
            "Epoch 150, Training loss 49.6749, Validation loss 50.4866\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1949, item_Code: A336370\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.1131, Validation loss 60.7880\n",
            "Epoch 50, Training loss 51.5820, Validation loss 49.9616\n",
            "Epoch 100, Training loss 50.7188, Validation loss 49.9213\n",
            "Epoch 150, Training loss 49.9807, Validation loss 49.6436\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1950, item_Code: A336570\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.4804, Validation loss 62.9160\n",
            "Epoch 50, Training loss 45.8116, Validation loss 68.8518\n",
            "Epoch 100, Training loss 45.0078, Validation loss 69.2778\n",
            "Epoch 150, Training loss 44.6428, Validation loss 68.7633\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1951, item_Code: A337930\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 82)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.1568, Validation loss 57.0616\n",
            "Epoch 50, Training loss 47.1723, Validation loss 44.3023\n",
            "Epoch 100, Training loss 46.2115, Validation loss 43.8364\n",
            "Epoch 150, Training loss 46.0557, Validation loss 43.1655\n",
            "torch.Size([543, 81])\n",
            "torch.Size([543, 82])\n",
            "--------------------\n",
            "count: 1952, item_Code: A338220\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.9138, Validation loss 59.6109\n",
            "Epoch 50, Training loss 45.9692, Validation loss 67.0573\n",
            "Epoch 100, Training loss 45.3802, Validation loss 66.6828\n",
            "Epoch 150, Training loss 44.7902, Validation loss 66.5313\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1953, item_Code: A339770\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.2397, Validation loss 58.1795\n",
            "Epoch 50, Training loss 48.6664, Validation loss 47.2246\n",
            "Epoch 100, Training loss 47.6969, Validation loss 46.1241\n",
            "Epoch 150, Training loss 47.4144, Validation loss 46.1201\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1954, item_Code: A339950\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.7058, Validation loss 62.7478\n",
            "Epoch 50, Training loss 50.5539, Validation loss 46.7326\n",
            "Epoch 100, Training loss 49.8785, Validation loss 46.1908\n",
            "Epoch 150, Training loss 49.2200, Validation loss 45.6325\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1955, item_Code: A340440\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.1013, Validation loss 59.7658\n",
            "Epoch 50, Training loss 50.7138, Validation loss 54.0308\n",
            "Epoch 100, Training loss 49.7957, Validation loss 53.4519\n",
            "Epoch 150, Training loss 49.2917, Validation loss 53.4685\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1956, item_Code: A340570\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.6245, Validation loss 60.9795\n",
            "Epoch 50, Training loss 48.9041, Validation loss 58.0690\n",
            "Epoch 100, Training loss 48.0604, Validation loss 58.2831\n",
            "Epoch 150, Training loss 47.5928, Validation loss 58.6675\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1957, item_Code: A340930\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.3638, Validation loss 61.0494\n",
            "Epoch 50, Training loss 52.4608, Validation loss 56.8666\n",
            "Epoch 100, Training loss 51.5578, Validation loss 56.3560\n",
            "Epoch 150, Training loss 51.1478, Validation loss 56.1100\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1958, item_Code: A344820\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.8100, Validation loss 59.8078\n",
            "Epoch 50, Training loss 51.0582, Validation loss 50.4946\n",
            "Epoch 100, Training loss 49.9454, Validation loss 50.2501\n",
            "Epoch 150, Training loss 49.5325, Validation loss 49.8918\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1959, item_Code: A347700\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.9674, Validation loss 57.6613\n",
            "Epoch 50, Training loss 46.2631, Validation loss 44.9966\n",
            "Epoch 100, Training loss 45.2032, Validation loss 43.9979\n",
            "Epoch 150, Training loss 44.8194, Validation loss 43.9322\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1960, item_Code: A347740\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.6030, Validation loss 60.5319\n",
            "Epoch 50, Training loss 46.2730, Validation loss 43.1418\n",
            "Epoch 100, Training loss 45.3739, Validation loss 42.4164\n",
            "Epoch 150, Training loss 44.5874, Validation loss 41.4305\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1961, item_Code: A347770\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.7697, Validation loss 60.0256\n",
            "Epoch 50, Training loss 49.4207, Validation loss 54.4448\n",
            "Epoch 100, Training loss 48.2739, Validation loss 53.4912\n",
            "Epoch 150, Training loss 47.7952, Validation loss 53.6598\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1962, item_Code: A347860\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.9788, Validation loss 59.1257\n",
            "Epoch 50, Training loss 48.8487, Validation loss 49.6019\n",
            "Epoch 100, Training loss 48.0493, Validation loss 48.8133\n",
            "Epoch 150, Training loss 47.5174, Validation loss 48.5870\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1963, item_Code: A347890\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.1858, Validation loss 62.1916\n",
            "Epoch 50, Training loss 52.6116, Validation loss 60.4984\n",
            "Epoch 100, Training loss 51.6222, Validation loss 59.0115\n",
            "Epoch 150, Training loss 51.1158, Validation loss 58.7008\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1964, item_Code: A348030\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 85)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.8701, Validation loss 59.2671\n",
            "Epoch 50, Training loss 48.8559, Validation loss 47.8040\n",
            "Epoch 100, Training loss 48.0848, Validation loss 47.0172\n",
            "Epoch 150, Training loss 47.2896, Validation loss 46.5467\n",
            "torch.Size([543, 84])\n",
            "torch.Size([543, 85])\n",
            "--------------------\n",
            "count: 1965, item_Code: A348150\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.2472, Validation loss 60.0259\n",
            "Epoch 50, Training loss 46.7189, Validation loss 48.3644\n",
            "Epoch 100, Training loss 46.1213, Validation loss 47.7611\n",
            "Epoch 150, Training loss 45.4582, Validation loss 47.4390\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1966, item_Code: A348210\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.3072, Validation loss 58.8919\n",
            "Epoch 50, Training loss 50.2863, Validation loss 59.1694\n",
            "Epoch 100, Training loss 49.8253, Validation loss 57.5841\n",
            "Epoch 150, Training loss 49.4615, Validation loss 57.6188\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1967, item_Code: A348350\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 83)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.4515, Validation loss 57.0498\n",
            "Epoch 50, Training loss 47.4241, Validation loss 49.1481\n",
            "Epoch 100, Training loss 46.6564, Validation loss 48.8727\n",
            "Epoch 150, Training loss 46.2974, Validation loss 48.5126\n",
            "torch.Size([543, 82])\n",
            "torch.Size([543, 83])\n",
            "--------------------\n",
            "count: 1968, item_Code: A351320\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.6851, Validation loss 61.3170\n",
            "Epoch 50, Training loss 50.1856, Validation loss 55.6905\n",
            "Epoch 100, Training loss 49.5057, Validation loss 55.2855\n",
            "Epoch 150, Training loss 49.1049, Validation loss 54.9096\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1969, item_Code: A351330\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0815, Validation loss 60.6782\n",
            "Epoch 50, Training loss 50.4360, Validation loss 53.8574\n",
            "Epoch 100, Training loss 49.6659, Validation loss 53.1197\n",
            "Epoch 150, Training loss 49.0524, Validation loss 53.1789\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1970, item_Code: A352480\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.3087, Validation loss 59.6608\n",
            "Epoch 50, Training loss 47.8123, Validation loss 61.2581\n",
            "Epoch 100, Training loss 47.4264, Validation loss 59.7485\n",
            "Epoch 150, Training loss 47.0805, Validation loss 58.8764\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1971, item_Code: A352700\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.0434, Validation loss 59.4164\n",
            "Epoch 50, Training loss 46.3334, Validation loss 42.2893\n",
            "Epoch 100, Training loss 45.4118, Validation loss 41.7337\n",
            "Epoch 150, Training loss 44.7362, Validation loss 41.1949\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1972, item_Code: A352770\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.1297, Validation loss 59.8595\n",
            "Epoch 50, Training loss 49.4994, Validation loss 51.9444\n",
            "Epoch 100, Training loss 48.8792, Validation loss 51.7498\n",
            "Epoch 150, Training loss 48.2139, Validation loss 51.0357\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1973, item_Code: A352820\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0589, Validation loss 60.5558\n",
            "Epoch 50, Training loss 51.1173, Validation loss 58.0587\n",
            "Epoch 100, Training loss 50.3418, Validation loss 57.8182\n",
            "Epoch 150, Training loss 49.8718, Validation loss 57.5031\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1974, item_Code: A352940\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.6249, Validation loss 59.8981\n",
            "Epoch 50, Training loss 48.6846, Validation loss 46.5810\n",
            "Epoch 100, Training loss 48.0638, Validation loss 46.0888\n",
            "Epoch 150, Training loss 47.3306, Validation loss 45.4449\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1975, item_Code: A353190\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.5143, Validation loss 59.6084\n",
            "Epoch 50, Training loss 47.9861, Validation loss 44.7614\n",
            "Epoch 100, Training loss 47.0171, Validation loss 43.8283\n",
            "Epoch 150, Training loss 46.5833, Validation loss 43.8131\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1976, item_Code: A353200\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 83)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 57.4366, Validation loss 57.4583\n",
            "Epoch 50, Training loss 48.1633, Validation loss 53.2110\n",
            "Epoch 100, Training loss 47.6598, Validation loss 52.0002\n",
            "Epoch 150, Training loss 47.2079, Validation loss 51.9785\n",
            "torch.Size([543, 82])\n",
            "torch.Size([543, 83])\n",
            "--------------------\n",
            "count: 1977, item_Code: A353810\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.4243, Validation loss 61.7791\n",
            "Epoch 50, Training loss 51.4523, Validation loss 47.6103\n",
            "Epoch 100, Training loss 50.8849, Validation loss 47.4122\n",
            "Epoch 150, Training loss 50.0311, Validation loss 46.2403\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1978, item_Code: A354200\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0262, Validation loss 60.8735\n",
            "Epoch 50, Training loss 49.1670, Validation loss 45.8588\n",
            "Epoch 100, Training loss 48.4381, Validation loss 45.5184\n",
            "Epoch 150, Training loss 48.0153, Validation loss 45.0238\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1979, item_Code: A355150\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.0649, Validation loss 62.0305\n",
            "Epoch 50, Training loss 47.5133, Validation loss 71.2850\n",
            "Epoch 100, Training loss 46.5980, Validation loss 69.2970\n",
            "Epoch 150, Training loss 46.3121, Validation loss 67.5695\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1980, item_Code: A356860\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.7220, Validation loss 62.9089\n",
            "Epoch 50, Training loss 51.8287, Validation loss 57.0583\n",
            "Epoch 100, Training loss 50.7724, Validation loss 55.5920\n",
            "Epoch 150, Training loss 50.4321, Validation loss 54.2515\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1981, item_Code: A356890\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.0141, Validation loss 61.3103\n",
            "Epoch 50, Training loss 50.1878, Validation loss 52.6166\n",
            "Epoch 100, Training loss 49.6902, Validation loss 52.4567\n",
            "Epoch 150, Training loss 49.0064, Validation loss 51.9491\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1982, item_Code: A357230\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 83)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.1365, Validation loss 58.1037\n",
            "Epoch 50, Training loss 46.7909, Validation loss 46.3503\n",
            "Epoch 100, Training loss 45.9473, Validation loss 45.5519\n",
            "Epoch 150, Training loss 45.2553, Validation loss 44.8242\n",
            "torch.Size([543, 82])\n",
            "torch.Size([543, 83])\n",
            "--------------------\n",
            "count: 1983, item_Code: A357550\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.6368, Validation loss 63.6192\n",
            "Epoch 50, Training loss 44.3092, Validation loss 79.9901\n",
            "Epoch 100, Training loss 43.6772, Validation loss 77.7542\n",
            "Epoch 150, Training loss 43.5286, Validation loss 77.5736\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1984, item_Code: A357780\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.5524, Validation loss 59.3992\n",
            "Epoch 50, Training loss 49.5384, Validation loss 52.2240\n",
            "Epoch 100, Training loss 48.9020, Validation loss 51.5520\n",
            "Epoch 150, Training loss 48.3552, Validation loss 50.8906\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1985, item_Code: A359090\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.4067, Validation loss 59.9384\n",
            "Epoch 50, Training loss 50.0289, Validation loss 51.9943\n",
            "Epoch 100, Training loss 48.9754, Validation loss 51.5311\n",
            "Epoch 150, Training loss 48.5984, Validation loss 50.4445\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1986, item_Code: A361390\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.1557, Validation loss 58.4927\n",
            "Epoch 50, Training loss 46.9777, Validation loss 43.2734\n",
            "Epoch 100, Training loss 46.0980, Validation loss 42.5688\n",
            "Epoch 150, Training loss 45.6265, Validation loss 42.4963\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1987, item_Code: A361610\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.6862, Validation loss 62.3444\n",
            "Epoch 50, Training loss 51.0225, Validation loss 54.6248\n",
            "Epoch 100, Training loss 49.9575, Validation loss 54.4817\n",
            "Epoch 150, Training loss 49.4394, Validation loss 53.7980\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1988, item_Code: A363250\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.6914, Validation loss 60.5570\n",
            "Epoch 50, Training loss 47.8704, Validation loss 63.5045\n",
            "Epoch 100, Training loss 47.5447, Validation loss 64.4875\n",
            "Epoch 150, Training loss 47.1073, Validation loss 64.7357\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1989, item_Code: A363260\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 60.1414, Validation loss 60.6994\n",
            "Epoch 50, Training loss 46.3471, Validation loss 52.8096\n",
            "Epoch 100, Training loss 46.1203, Validation loss 51.5117\n",
            "Epoch 150, Training loss 45.3831, Validation loss 51.4946\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1990, item_Code: A363280\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 84)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 58.1913, Validation loss 58.2924\n",
            "Epoch 50, Training loss 48.3032, Validation loss 50.1522\n",
            "Epoch 100, Training loss 47.6261, Validation loss 49.0958\n",
            "Epoch 150, Training loss 46.9957, Validation loss 48.5337\n",
            "torch.Size([543, 83])\n",
            "torch.Size([543, 84])\n",
            "--------------------\n",
            "count: 1991, item_Code: A365590\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.5046, Validation loss 58.7340\n",
            "Epoch 50, Training loss 47.2059, Validation loss 57.2756\n",
            "Epoch 100, Training loss 46.7761, Validation loss 57.4652\n",
            "Epoch 150, Training loss 45.9547, Validation loss 56.7114\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1992, item_Code: A368770\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 87)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.9778, Validation loss 59.6793\n",
            "Epoch 50, Training loss 50.1349, Validation loss 55.9274\n",
            "Epoch 100, Training loss 49.0490, Validation loss 56.0242\n",
            "Epoch 150, Training loss 48.5197, Validation loss 55.7732\n",
            "torch.Size([543, 86])\n",
            "torch.Size([543, 87])\n",
            "--------------------\n",
            "count: 1993, item_Code: A369370\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 90)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 62.4957, Validation loss 62.5912\n",
            "Epoch 50, Training loss 51.4191, Validation loss 51.0380\n",
            "Epoch 100, Training loss 50.7208, Validation loss 50.7159\n",
            "Epoch 150, Training loss 49.9995, Validation loss 51.0310\n",
            "torch.Size([543, 89])\n",
            "torch.Size([543, 90])\n",
            "--------------------\n",
            "count: 1994, item_Code: A373200\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.4410, Validation loss 60.9371\n",
            "Epoch 50, Training loss 47.7756, Validation loss 55.5678\n",
            "Epoch 100, Training loss 47.2728, Validation loss 55.2871\n",
            "Epoch 150, Training loss 46.6229, Validation loss 55.1105\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1995, item_Code: A375500\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 88)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.1649, Validation loss 61.2690\n",
            "Epoch 50, Training loss 45.8443, Validation loss 42.9459\n",
            "Epoch 100, Training loss 45.1836, Validation loss 42.6421\n",
            "Epoch 150, Training loss 44.5301, Validation loss 42.2543\n",
            "torch.Size([543, 87])\n",
            "torch.Size([543, 88])\n",
            "--------------------\n",
            "count: 1996, item_Code: A378850\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.9345, Validation loss 61.5122\n",
            "Epoch 50, Training loss 49.5228, Validation loss 54.5615\n",
            "Epoch 100, Training loss 48.8855, Validation loss 54.5283\n",
            "Epoch 150, Training loss 48.3104, Validation loss 54.1456\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1997, item_Code: A383220\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.8443, Validation loss 62.2722\n",
            "Epoch 50, Training loss 46.6792, Validation loss 44.7875\n",
            "Epoch 100, Training loss 45.9449, Validation loss 44.1253\n",
            "Epoch 150, Training loss 45.3106, Validation loss 43.7111\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n",
            "--------------------\n",
            "count: 1998, item_Code: A383310\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 86)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 59.0487, Validation loss 58.4725\n",
            "Epoch 50, Training loss 44.6375, Validation loss 48.6097\n",
            "Epoch 100, Training loss 43.8315, Validation loss 48.2344\n",
            "Epoch 150, Training loss 43.3067, Validation loss 48.2001\n",
            "torch.Size([543, 85])\n",
            "torch.Size([543, 86])\n",
            "--------------------\n",
            "count: 1999, item_Code: A383800\n",
            "--------------------\n",
            "Before feature importances (543, 92)\n",
            "Series([], dtype: int64)\n",
            "(543, 91) (543,)\n",
            "after feature importances (543, 89)\n",
            "Series([], dtype: int64)\n",
            "Epoch 0, Training loss 61.4610, Validation loss 61.8066\n",
            "Epoch 50, Training loss 52.8427, Validation loss 51.2570\n",
            "Epoch 100, Training loss 52.2695, Validation loss 50.8893\n",
            "Epoch 150, Training loss 51.7364, Validation loss 50.6545\n",
            "torch.Size([543, 88])\n",
            "torch.Size([543, 89])\n"
          ]
        }
      ],
      "source": [
        "forcated_g_concat = []\n",
        "\n",
        "n_epoch = 200 # 200\n",
        "batch_size = 64 #64\n",
        "learning_rate = 0.001 # 0.01\n",
        "\n",
        "n_hidden = 64 #32\n",
        "n_latent = 10\n",
        "n_layers = 2 #3\n",
        "\n",
        "# zero_item_Code = ['A006580',\n",
        "#   'A012600',\n",
        "#   'A015540',\n",
        "#   'A033340',\n",
        "#   'A065560',\n",
        "#   'A069110',\n",
        "#   'A078590',\n",
        "#   'A099520',\n",
        "#   'A109070',\n",
        "#   'A160600',\n",
        "#   'A178780',\n",
        "#   'A223310',\n",
        "#   'A257370',\n",
        "#   'A263540']\n",
        "\n",
        "zero_item_Code = [\n",
        "  'A006580'\n",
        "  'A012600'\n",
        "  'A015540'\n",
        "  'A033340'\n",
        "  'A065560'\n",
        "  'A069110'\n",
        "  'A078590'\n",
        "  'A099520'\n",
        "  'A109070'\n",
        "  'A160600'\n",
        "  'A178780'\n",
        "  'A257370'\n",
        "  'A263540'\n",
        "  ]\n",
        "\n",
        "stacked_vae_filter_data = []\n",
        "\n",
        "for n, item_Code in enumerate(train['item_Code'].unique().tolist()):\n",
        "\n",
        "  print('-' * 20)\n",
        "  print(f'count: {n}, item_Code: {item_Code}')\n",
        "  print('-' * 20)\n",
        "\n",
        "  item_set = train[train['item_Code'] == item_Code]\n",
        "  item_set = item_set.drop(['Date', 'item_Code', 'item_Name', 'ARIMA_close'], axis = 1)\n",
        "\n",
        "  print('Before feature importances', item_set.shape)\n",
        "\n",
        "  # if item_Code in zero_item_Code:\n",
        "  #   print(f'{item_Code} is zero')\n",
        "  #   stacked_vae_filter_data.append(torch.Tensor(item_set['Close2'].values))\n",
        "  #   continue\n",
        "\n",
        "  #item_set = item_set.astype(float)\n",
        "  item_set = item_set.fillna(method = 'ffill')\n",
        "  item_set = item_set.fillna(method = 'bfill')\n",
        "  print(item_set.isnull().sum()[item_set.isnull().sum() > 0])\n",
        "  columns_list = item_set.columns.tolist()\n",
        "\n",
        "  # scaling\n",
        "  scaler = StandardScaler()\n",
        "  #scaler = RobustScaler()\n",
        "  #scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "  scaler = scaler.fit(item_set.drop(['Close2'], axis = 1))\n",
        "  item_set_scaled = scaler.transform(item_set.drop(['Close2'], axis = 1))\n",
        "  print(item_set_scaled.shape, item_set['Close2'].shape)\n",
        "  item_set_scaled = np.hstack((item_set_scaled, np.array(item_set['Close2']).reshape(-1, 1)))\n",
        "  item_set_scaled_df = pd.DataFrame(item_set_scaled, columns = columns_list)\n",
        "\n",
        "  (X_train_fi, y_train_fi), (X_test_fi, y_test_fi) = train_test_split(item_set_scaled_df, 'Close2')\n",
        "  regressor = xgb.XGBRegressor()\n",
        "  regressor = regressor.fit(X_train_fi, y_train_fi, eval_set = [(X_train_fi, y_train_fi), (X_test_fi, y_test_fi)], verbose = False)\n",
        "\n",
        "  fi = X_train_fi.columns[np.array(regressor.feature_importances_) > 0.0].tolist()\n",
        "  #fi.append('Close2')\n",
        "  fi.append('ARIMA_close')\n",
        "  #print('len:', len(fi))\n",
        "\n",
        "  item_set = train[train['item_Code'] == item_Code]\n",
        "  item_set = item_set[list(set(fi))]\n",
        "\n",
        "  print('after feature importances', item_set.shape)\n",
        "\n",
        "  item_set = item_set.astype(float)\n",
        "  item_set = item_set.fillna(method = 'ffill')\n",
        "  item_set = item_set.fillna(method = 'bfill')\n",
        "  print(item_set.isnull().sum()[item_set.isnull().sum() > 0])\n",
        "\n",
        "  if item_set.shape[1] < 2:\n",
        "    print(f'{item_Code} is zero')\n",
        "    stacked_vae_filter_data.append(torch.Tensor(item_set['ARIMA_close'].values))\n",
        "    continue\n",
        "\n",
        "  # scaling\n",
        "  #scaler = StandardScaler()\n",
        "  #scaler = RobustScaler()\n",
        "  scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "  scaler = scaler.fit(item_set.drop(['ARIMA_close'], axis = 1))\n",
        "  item_set_scaled = scaler.transform(item_set.drop(['ARIMA_close'], axis = 1))\n",
        "\n",
        "  num_training_days = int(np.ceil(0.8 * item_set.shape[0]))\n",
        "\n",
        "  feats_item_set = item_set_scaled[:, :]\n",
        "  feats_train = item_set_scaled[:num_training_days, :]\n",
        "  feats_test = item_set_scaled[num_training_days:, :]\n",
        "\n",
        "  data_close = torch.tensor(item_set['ARIMA_close'].values)\n",
        "\n",
        "  feats_train_tensor = torch.tensor(feats_train, dtype = torch.float32)\n",
        "  feats_test_tensor = torch.tensor(feats_test, dtype = torch.float32)\n",
        "\n",
        "  train_loader = DataLoader(feats_train_tensor,  batch_size = batch_size, shuffle = False)\n",
        "  test_loader = DataLoader(feats_test_tensor, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "  if feats_train.shape[1] > 64:\n",
        "    n_hidden = 64\n",
        "  else:\n",
        "    n_hidden = 32\n",
        "\n",
        "  model = Stacked_VAE(n_in = feats_train.shape[1], n_hidden = n_hidden, n_latent = n_latent, n_layers = n_layers)\n",
        "  #model = Stacked_VAE(n_in = feats_train.shape[1], n_hidden = n_hidden, n_latent = n_latent)\n",
        "  optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "  loss_function = nn.MSELoss()\n",
        "  #print(model)\n",
        "\n",
        "  training_loss = []\n",
        "  validation_loss = []\n",
        "  for epoch in range(n_epoch):\n",
        "    epoch_loss = 0\n",
        "    epoch_val_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      outputs, weight_loss = model(data)\n",
        "      loss = loss_function(outputs, data)\n",
        "      loss = loss + torch.mean(weight_loss)\n",
        "      #recon_batch, mu, log_var = model(data)\n",
        "      #loss = vae_loss(recon_batch, data, mu, log_var)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "        outputs, weight_loss = model(data)\n",
        "        loss = loss_function(outputs, data)\n",
        "        loss = loss + torch.mean(weight_loss)\n",
        "        #recon_batch, mu, log_var = model(data)\n",
        "        #loss = vae_loss(recon_batch, data, mu, log_var)\n",
        "        epoch_val_loss += loss.item()\n",
        "\n",
        "    epoch_loss /= len(train_loader)\n",
        "    epoch_val_loss /= len(test_loader)\n",
        "\n",
        "    training_loss.append(epoch_loss)\n",
        "    validation_loss.append(epoch_val_loss)\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "      print('Epoch {}, Training loss {:.4f}, Validation loss {:.4f}'.format(epoch, epoch_loss, epoch_val_loss))\n",
        "\n",
        "  feats_data = torch.tensor(feats_item_set, dtype = torch.float32)\n",
        "  mu, lv = model.encoded(feats_data)\n",
        "  z = model.reparameterize(mu, lv)\n",
        "  decoded_data = model.decoded(z)\n",
        "  print(decoded_data.shape)\n",
        "  decoded_data = torch.cat((torch.tensor(scaler.inverse_transform(decoded_data.detach().numpy())), data_close.view(543, 1)), dim = 1)\n",
        "  print(decoded_data.shape)\n",
        "  stacked_vae_filter_data.append(decoded_data)\n",
        "#stacked_vae_filter_data_ = torch.cat(stacked_vae_filter_data, dim=0)\n",
        "#print(stacked_vae_filter_data_.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebp6sMgPa2uq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9558132-0cfe-4e62-9452-dc51f4e737f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "len(stacked_vae_filter_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceeRy7VVrGJe"
      },
      "outputs": [],
      "source": [
        "# train.item_Code.unique().tolist().index('A006580')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JpNaF3trGekw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b37d44-f7c0-42ad-b711-561dc7547d5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train_additional.csv',\n",
              " 'past_open',\n",
              " 'full_train.parquet',\n",
              " 'stacked_vae_nf_arima_stdv.npy']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "os.listdir('/content/drive/MyDrive/open (2)/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MkvVWSq7BQie"
      },
      "outputs": [],
      "source": [
        "#np.save('stacked_vae_nf_arima_std.npy', stacked_vae_filter_data)\n",
        "stacked_vae_filter_data = np.load('/content/drive/MyDrive/open (2)/stacked_vae_nf_arima_stdv.npy', allow_pickle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MWtYOCmWeXH3"
      },
      "outputs": [],
      "source": [
        "stacked_vae_filter_data = {idx:torch.tensor(data) for idx, data in enumerate(stacked_vae_filter_data)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2J9z4QvemQz"
      },
      "source": [
        "# train.parquet - not daily_range, mean\n",
        "# train.parquet (1) - add daily_range, mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBCinxEubMmm"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZcEP0WepbKm-"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size = 1, hidden_size = 100, n_layers = 5, dropout = 0.5, output_size = 15):\n",
        "      super(LSTM, self).__init__()\n",
        "      self.input_size = input_size\n",
        "      self.hidden_size = hidden_size\n",
        "      self.n_layers = n_layers\n",
        "      self.dropout = dropout\n",
        "      self.output_size = output_size\n",
        "      self.lstm = nn.LSTM(input_size, hidden_size, n_layers, dropout = dropout, batch_first = True)\n",
        "      self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "      h0 = Variable(torch.zeros(self.n_layers, x.size(0), self.hidden_size)).to(device)\n",
        "      c0 = Variable(torch.zeros(self.n_layers, x.size(0), self.hidden_size)).to(device)\n",
        "      output, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "      out = self.fc(output[:, -self.output_size, :])\n",
        "      return out\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size = 1, hidden_size = 100, n_layers = 5, dropout = 0.5, output_size = 15):\n",
        "      super(BiLSTM, self).__init__()\n",
        "      self.input_size = input_size\n",
        "      self.hidden_size = hidden_size\n",
        "      self.n_layers = n_layers\n",
        "      self.dropout = dropout\n",
        "      self.output_size = output_size\n",
        "      self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = n_layers, bidirectional = True, dropout = dropout, batch_first = True)\n",
        "      self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      h0 = Variable(torch.zeros(self.n_layers * 2, x.size(0), self.hidden_size)).to(device)\n",
        "      c0 = Variable(torch.zeros(self.n_layers * 2, x.size(0), self.hidden_size)).to(device)\n",
        "      output, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "      out = self.fc(output[:, -self.output_size, :])\n",
        "      out = self.relu(out)\n",
        "      return out\n",
        "\n",
        "class biGRU(nn.Module) :\n",
        "    def __init__(self, input_size = 1, hidden_size = 100, n_layers = 5, output_size = 15):\n",
        "      super(biGRU, self).__init__()\n",
        "      self.input_size = input_size\n",
        "      self.hidden_size = hidden_size\n",
        "      self.n_layers = n_layers\n",
        "      self.dropout = dropout\n",
        "      self.output_size = output_size\n",
        "\n",
        "      self.gru = nn.GRU(input_size = input_size, hidden_size = hidden_size, num_layers = n_layers, bidirectional=True, batch_first=True)\n",
        "      self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x) :\n",
        "      h_0 = Variable(torch.zeros(self.n_layers * 2, x.size(0), self.hidden_size)).to(device)\n",
        "      out, _ = self.gru(x, (h_0))\n",
        "      out = self.fc(out[:, -self.output_size, :])\n",
        "      out = self.relu(out)\n",
        "      return out\n",
        "\n",
        "class VanillaRNN(nn.Module):\n",
        "    def __init__(self, input_size = 1, hidden_size = 100, n_layers = 5, dropout = 0.5, output_size = 15):\n",
        "      super(VanillaRNN, self).__init__()\n",
        "      self.input_size = input_size\n",
        "      self.hidden_size = hidden_size\n",
        "      self.n_layers = n_layers\n",
        "      self.dropout = dropout\n",
        "      self.output_size = output_size\n",
        "      self.rnn = nn.RNN(input_size, hidden_size, n_layers, batch_first = True)\n",
        "      self.fc = nn.Sequential(nn.Linear(hidden_size, output_size), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "      h0 = Variable(torch.zeros(self.n_layers, x.size(0), self.hidden_size)).to(device)\n",
        "      output, _ = self.rnn(x, h0)\n",
        "      out = self.fc(output[:, -self.output_size, :])\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISIh5TdIv2mo",
        "outputId": "b3d6fb03-1fef-45f4-8dba-7c0943a77513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pMU4YqZaCJ-1"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv8x5zi8v7od",
        "outputId": "0e63744c-d833-4579-cd1c-eb6fcfc3d961"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "op4gnfz-RMRE"
      },
      "outputs": [],
      "source": [
        "CUDA_LAUNCH_BLOCKING=1\n",
        "#CUBLAS_WORKSPACE_CONFIG=:16:8\n",
        "#CUBLAS_WORKSPACE_CONFIG=:4096:2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5GUK09IQW3e",
        "outputId": "5e523ef4-fa8a-4b28-cae1-8aa2f33ad133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Epoch [130/200], Loss: 0.00022\n",
            "Epoch [140/200], Loss: 0.00032\n",
            "Epoch [150/200], Loss: 0.00025\n",
            "Epoch [160/200], Loss: 0.00018\n",
            "Epoch [170/200], Loss: 0.00029\n",
            "Epoch [180/200], Loss: 0.00105\n",
            "Epoch [190/200], Loss: 0.00009\n",
            "Epoch [200/200], Loss: 0.00011\n",
            "--------------------\n",
            "stock number: 1809\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02389\n",
            "Epoch [20/200], Loss: 0.00752\n",
            "Epoch [30/200], Loss: 0.00634\n",
            "Epoch [40/200], Loss: 0.00688\n",
            "Epoch [50/200], Loss: 0.00660\n",
            "Epoch [60/200], Loss: 0.00869\n",
            "Epoch [70/200], Loss: 0.00430\n",
            "Epoch [80/200], Loss: 0.00449\n",
            "Epoch [90/200], Loss: 0.00997\n",
            "Epoch [100/200], Loss: 0.01007\n",
            "Epoch [110/200], Loss: 0.00389\n",
            "Epoch [120/200], Loss: 0.00783\n",
            "Epoch [130/200], Loss: 0.00438\n",
            "Epoch [140/200], Loss: 0.00317\n",
            "Epoch [150/200], Loss: 0.01131\n",
            "Epoch [160/200], Loss: 0.00318\n",
            "Epoch [170/200], Loss: 0.00290\n",
            "Epoch [180/200], Loss: 0.00345\n",
            "Epoch [190/200], Loss: 0.00540\n",
            "Epoch [200/200], Loss: 0.00434\n",
            "--------------------\n",
            "stock number: 1810\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02976\n",
            "Epoch [20/200], Loss: 0.01736\n",
            "Epoch [30/200], Loss: 0.01219\n",
            "Epoch [40/200], Loss: 0.00817\n",
            "Epoch [50/200], Loss: 0.00597\n",
            "Epoch [60/200], Loss: 0.00766\n",
            "Epoch [70/200], Loss: 0.00308\n",
            "Epoch [80/200], Loss: 0.00287\n",
            "Epoch [90/200], Loss: 0.00276\n",
            "Epoch [100/200], Loss: 0.00217\n",
            "Epoch [110/200], Loss: 0.00286\n",
            "Epoch [120/200], Loss: 0.00179\n",
            "Epoch [130/200], Loss: 0.00230\n",
            "Epoch [140/200], Loss: 0.00172\n",
            "Epoch [150/200], Loss: 0.00267\n",
            "Epoch [160/200], Loss: 0.00167\n",
            "Epoch [170/200], Loss: 0.00100\n",
            "Epoch [180/200], Loss: 0.00121\n",
            "Epoch [190/200], Loss: 0.00121\n",
            "Epoch [200/200], Loss: 0.00158\n",
            "--------------------\n",
            "stock number: 1811\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00410\n",
            "Epoch [20/200], Loss: 0.00293\n",
            "Epoch [30/200], Loss: 0.00185\n",
            "Epoch [40/200], Loss: 0.00119\n",
            "Epoch [50/200], Loss: 0.00121\n",
            "Epoch [60/200], Loss: 0.00087\n",
            "Epoch [70/200], Loss: 0.00079\n",
            "Epoch [80/200], Loss: 0.00180\n",
            "Epoch [90/200], Loss: 0.00043\n",
            "Epoch [100/200], Loss: 0.00057\n",
            "Epoch [110/200], Loss: 0.00041\n",
            "Epoch [120/200], Loss: 0.00054\n",
            "Epoch [130/200], Loss: 0.00050\n",
            "Epoch [140/200], Loss: 0.00052\n",
            "Epoch [150/200], Loss: 0.00045\n",
            "Epoch [160/200], Loss: 0.00048\n",
            "Epoch [170/200], Loss: 0.00065\n",
            "Epoch [180/200], Loss: 0.00045\n",
            "Epoch [190/200], Loss: 0.00047\n",
            "Epoch [200/200], Loss: 0.00036\n",
            "--------------------\n",
            "stock number: 1812\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00901\n",
            "Epoch [20/200], Loss: 0.00298\n",
            "Epoch [30/200], Loss: 0.00386\n",
            "Epoch [40/200], Loss: 0.00173\n",
            "Epoch [50/200], Loss: 0.00091\n",
            "Epoch [60/200], Loss: 0.00100\n",
            "Epoch [70/200], Loss: 0.00098\n",
            "Epoch [80/200], Loss: 0.00080\n",
            "Epoch [90/200], Loss: 0.00078\n",
            "Epoch [100/200], Loss: 0.00060\n",
            "Epoch [110/200], Loss: 0.00089\n",
            "Epoch [120/200], Loss: 0.00076\n",
            "Epoch [130/200], Loss: 0.00062\n",
            "Epoch [140/200], Loss: 0.00048\n",
            "Epoch [150/200], Loss: 0.00056\n",
            "Epoch [160/200], Loss: 0.00085\n",
            "Epoch [170/200], Loss: 0.00058\n",
            "Epoch [180/200], Loss: 0.00045\n",
            "Epoch [190/200], Loss: 0.00040\n",
            "Epoch [200/200], Loss: 0.00043\n",
            "--------------------\n",
            "stock number: 1813\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00420\n",
            "Epoch [20/200], Loss: 0.00412\n",
            "Epoch [30/200], Loss: 0.00337\n",
            "Epoch [40/200], Loss: 0.00242\n",
            "Epoch [50/200], Loss: 0.00145\n",
            "Epoch [60/200], Loss: 0.00211\n",
            "Epoch [70/200], Loss: 0.00130\n",
            "Epoch [80/200], Loss: 0.00113\n",
            "Epoch [90/200], Loss: 0.00142\n",
            "Epoch [100/200], Loss: 0.00039\n",
            "Epoch [110/200], Loss: 0.00083\n",
            "Epoch [120/200], Loss: 0.00027\n",
            "Epoch [130/200], Loss: 0.00041\n",
            "Epoch [140/200], Loss: 0.00037\n",
            "Epoch [150/200], Loss: 0.00046\n",
            "Epoch [160/200], Loss: 0.00033\n",
            "Epoch [170/200], Loss: 0.00029\n",
            "Epoch [180/200], Loss: 0.00020\n",
            "Epoch [190/200], Loss: 0.00016\n",
            "Epoch [200/200], Loss: 0.00033\n",
            "--------------------\n",
            "stock number: 1814\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.20501\n",
            "Epoch [20/200], Loss: 0.04863\n",
            "Epoch [30/200], Loss: 0.03062\n",
            "Epoch [40/200], Loss: 0.01461\n",
            "Epoch [50/200], Loss: 0.01269\n",
            "Epoch [60/200], Loss: 0.00794\n",
            "Epoch [70/200], Loss: 0.00697\n",
            "Epoch [80/200], Loss: 0.00524\n",
            "Epoch [90/200], Loss: 0.00435\n",
            "Epoch [100/200], Loss: 0.00584\n",
            "Epoch [110/200], Loss: 0.00321\n",
            "Epoch [120/200], Loss: 0.00250\n",
            "Epoch [130/200], Loss: 0.00260\n",
            "Epoch [140/200], Loss: 0.00198\n",
            "Epoch [150/200], Loss: 0.00216\n",
            "Epoch [160/200], Loss: 0.00192\n",
            "Epoch [170/200], Loss: 0.00151\n",
            "Epoch [180/200], Loss: 0.00143\n",
            "Epoch [190/200], Loss: 0.00166\n",
            "Epoch [200/200], Loss: 0.00181\n",
            "--------------------\n",
            "stock number: 1815\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01698\n",
            "Epoch [20/200], Loss: 0.00777\n",
            "Epoch [30/200], Loss: 0.00279\n",
            "Epoch [40/200], Loss: 0.00870\n",
            "Epoch [50/200], Loss: 0.00218\n",
            "Epoch [60/200], Loss: 0.00270\n",
            "Epoch [70/200], Loss: 0.00153\n",
            "Epoch [80/200], Loss: 0.00171\n",
            "Epoch [90/200], Loss: 0.00180\n",
            "Epoch [100/200], Loss: 0.00186\n",
            "Epoch [110/200], Loss: 0.00131\n",
            "Epoch [120/200], Loss: 0.00171\n",
            "Epoch [130/200], Loss: 0.00162\n",
            "Epoch [140/200], Loss: 0.00276\n",
            "Epoch [150/200], Loss: 0.00107\n",
            "Epoch [160/200], Loss: 0.00061\n",
            "Epoch [170/200], Loss: 0.00070\n",
            "Epoch [180/200], Loss: 0.00059\n",
            "Epoch [190/200], Loss: 0.00071\n",
            "Epoch [200/200], Loss: 0.00118\n",
            "--------------------\n",
            "stock number: 1816\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00387\n",
            "Epoch [20/200], Loss: 0.00910\n",
            "Epoch [30/200], Loss: 0.00425\n",
            "Epoch [40/200], Loss: 0.00084\n",
            "Epoch [50/200], Loss: 0.00130\n",
            "Epoch [60/200], Loss: 0.00059\n",
            "Epoch [70/200], Loss: 0.00060\n",
            "Epoch [80/200], Loss: 0.00041\n",
            "Epoch [90/200], Loss: 0.00270\n",
            "Epoch [100/200], Loss: 0.00028\n",
            "Epoch [110/200], Loss: 0.00044\n",
            "Epoch [120/200], Loss: 0.00022\n",
            "Epoch [130/200], Loss: 0.00038\n",
            "Epoch [140/200], Loss: 0.00022\n",
            "Epoch [150/200], Loss: 0.00061\n",
            "Epoch [160/200], Loss: 0.00022\n",
            "Epoch [170/200], Loss: 0.00036\n",
            "Epoch [180/200], Loss: 0.00066\n",
            "Epoch [190/200], Loss: 0.00018\n",
            "Epoch [200/200], Loss: 0.00012\n",
            "--------------------\n",
            "stock number: 1817\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00287\n",
            "Epoch [20/200], Loss: 0.00265\n",
            "Epoch [30/200], Loss: 0.00516\n",
            "Epoch [40/200], Loss: 0.00291\n",
            "Epoch [50/200], Loss: 0.00217\n",
            "Epoch [60/200], Loss: 0.00181\n",
            "Epoch [70/200], Loss: 0.00250\n",
            "Epoch [80/200], Loss: 0.00161\n",
            "Epoch [90/200], Loss: 0.00223\n",
            "Epoch [100/200], Loss: 0.00164\n",
            "Epoch [110/200], Loss: 0.00248\n",
            "Epoch [120/200], Loss: 0.00126\n",
            "Epoch [130/200], Loss: 0.00179\n",
            "Epoch [140/200], Loss: 0.00118\n",
            "Epoch [150/200], Loss: 0.00188\n",
            "Epoch [160/200], Loss: 0.00142\n",
            "Epoch [170/200], Loss: 0.00128\n",
            "Epoch [180/200], Loss: 0.00118\n",
            "Epoch [190/200], Loss: 0.00101\n",
            "Epoch [200/200], Loss: 0.00101\n",
            "--------------------\n",
            "stock number: 1818\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01104\n",
            "Epoch [20/200], Loss: 0.00407\n",
            "Epoch [30/200], Loss: 0.00805\n",
            "Epoch [40/200], Loss: 0.00162\n",
            "Epoch [50/200], Loss: 0.00320\n",
            "Epoch [60/200], Loss: 0.00219\n",
            "Epoch [70/200], Loss: 0.00451\n",
            "Epoch [80/200], Loss: 0.00216\n",
            "Epoch [90/200], Loss: 0.00185\n",
            "Epoch [100/200], Loss: 0.00202\n",
            "Epoch [110/200], Loss: 0.00144\n",
            "Epoch [120/200], Loss: 0.00145\n",
            "Epoch [130/200], Loss: 0.00142\n",
            "Epoch [140/200], Loss: 0.00189\n",
            "Epoch [150/200], Loss: 0.00129\n",
            "Epoch [160/200], Loss: 0.00114\n",
            "Epoch [170/200], Loss: 0.00148\n",
            "Epoch [180/200], Loss: 0.00156\n",
            "Epoch [190/200], Loss: 0.00146\n",
            "Epoch [200/200], Loss: 0.00133\n",
            "--------------------\n",
            "stock number: 1819\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01219\n",
            "Epoch [20/200], Loss: 0.00372\n",
            "Epoch [30/200], Loss: 0.00740\n",
            "Epoch [40/200], Loss: 0.00291\n",
            "Epoch [50/200], Loss: 0.00603\n",
            "Epoch [60/200], Loss: 0.00123\n",
            "Epoch [70/200], Loss: 0.00637\n",
            "Epoch [80/200], Loss: 0.00073\n",
            "Epoch [90/200], Loss: 0.00114\n",
            "Epoch [100/200], Loss: 0.00127\n",
            "Epoch [110/200], Loss: 0.00205\n",
            "Epoch [120/200], Loss: 0.00351\n",
            "Epoch [130/200], Loss: 0.00116\n",
            "Epoch [140/200], Loss: 0.00109\n",
            "Epoch [150/200], Loss: 0.00237\n",
            "Epoch [160/200], Loss: 0.00215\n",
            "Epoch [170/200], Loss: 0.00060\n",
            "Epoch [180/200], Loss: 0.00182\n",
            "Epoch [190/200], Loss: 0.00178\n",
            "Epoch [200/200], Loss: 0.00174\n",
            "--------------------\n",
            "stock number: 1820\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.05516\n",
            "Epoch [20/200], Loss: 0.00513\n",
            "Epoch [30/200], Loss: 0.00364\n",
            "Epoch [40/200], Loss: 0.00246\n",
            "Epoch [50/200], Loss: 0.00509\n",
            "Epoch [60/200], Loss: 0.00218\n",
            "Epoch [70/200], Loss: 0.00224\n",
            "Epoch [80/200], Loss: 0.00266\n",
            "Epoch [90/200], Loss: 0.00300\n",
            "Epoch [100/200], Loss: 0.00336\n",
            "Epoch [110/200], Loss: 0.00192\n",
            "Epoch [120/200], Loss: 0.00239\n",
            "Epoch [130/200], Loss: 0.00201\n",
            "Epoch [140/200], Loss: 0.00148\n",
            "Epoch [150/200], Loss: 0.00210\n",
            "Epoch [160/200], Loss: 0.00172\n",
            "Epoch [170/200], Loss: 0.00348\n",
            "Epoch [180/200], Loss: 0.00171\n",
            "Epoch [190/200], Loss: 0.00246\n",
            "Epoch [200/200], Loss: 0.00156\n",
            "--------------------\n",
            "stock number: 1821\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00584\n",
            "Epoch [20/200], Loss: 0.00540\n",
            "Epoch [30/200], Loss: 0.00217\n",
            "Epoch [40/200], Loss: 0.01217\n",
            "Epoch [50/200], Loss: 0.00184\n",
            "Epoch [60/200], Loss: 0.00150\n",
            "Epoch [70/200], Loss: 0.00174\n",
            "Epoch [80/200], Loss: 0.00107\n",
            "Epoch [90/200], Loss: 0.00140\n",
            "Epoch [100/200], Loss: 0.00072\n",
            "Epoch [110/200], Loss: 0.00118\n",
            "Epoch [120/200], Loss: 0.00123\n",
            "Epoch [130/200], Loss: 0.00079\n",
            "Epoch [140/200], Loss: 0.00110\n",
            "Epoch [150/200], Loss: 0.00167\n",
            "Epoch [160/200], Loss: 0.00089\n",
            "Epoch [170/200], Loss: 0.00043\n",
            "Epoch [180/200], Loss: 0.00137\n",
            "Epoch [190/200], Loss: 0.00047\n",
            "Epoch [200/200], Loss: 0.00024\n",
            "--------------------\n",
            "stock number: 1822\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00004\n",
            "Epoch [20/200], Loss: 0.00004\n",
            "Epoch [30/200], Loss: 0.00004\n",
            "Epoch [40/200], Loss: 0.00004\n",
            "Epoch [50/200], Loss: 0.00004\n",
            "Epoch [60/200], Loss: 0.00004\n",
            "Epoch [70/200], Loss: 0.00004\n",
            "Epoch [80/200], Loss: 0.00004\n",
            "Epoch [90/200], Loss: 0.00004\n",
            "Epoch [100/200], Loss: 0.00004\n",
            "Epoch [110/200], Loss: 0.00004\n",
            "Epoch [120/200], Loss: 0.00004\n",
            "Epoch [130/200], Loss: 0.00004\n",
            "Epoch [140/200], Loss: 0.00004\n",
            "Epoch [150/200], Loss: 0.00004\n",
            "Epoch [160/200], Loss: 0.00004\n",
            "Epoch [170/200], Loss: 0.00004\n",
            "Epoch [180/200], Loss: 0.00005\n",
            "Epoch [190/200], Loss: 0.00004\n",
            "Epoch [200/200], Loss: 0.00004\n",
            "--------------------\n",
            "stock number: 1823\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00223\n",
            "Epoch [20/200], Loss: 0.00184\n",
            "Epoch [30/200], Loss: 0.00819\n",
            "Epoch [40/200], Loss: 0.00103\n",
            "Epoch [50/200], Loss: 0.00335\n",
            "Epoch [60/200], Loss: 0.00527\n",
            "Epoch [70/200], Loss: 0.00096\n",
            "Epoch [80/200], Loss: 0.00061\n",
            "Epoch [90/200], Loss: 0.00069\n",
            "Epoch [100/200], Loss: 0.00071\n",
            "Epoch [110/200], Loss: 0.00064\n",
            "Epoch [120/200], Loss: 0.00066\n",
            "Epoch [130/200], Loss: 0.00052\n",
            "Epoch [140/200], Loss: 0.00070\n",
            "Epoch [150/200], Loss: 0.00237\n",
            "Epoch [160/200], Loss: 0.00054\n",
            "Epoch [170/200], Loss: 0.00061\n",
            "Epoch [180/200], Loss: 0.00021\n",
            "Epoch [190/200], Loss: 0.00119\n",
            "Epoch [200/200], Loss: 0.00097\n",
            "--------------------\n",
            "stock number: 1824\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.06925\n",
            "Epoch [20/200], Loss: 0.01841\n",
            "Epoch [30/200], Loss: 0.01236\n",
            "Epoch [40/200], Loss: 0.00209\n",
            "Epoch [50/200], Loss: 0.00634\n",
            "Epoch [60/200], Loss: 0.00132\n",
            "Epoch [70/200], Loss: 0.00208\n",
            "Epoch [80/200], Loss: 0.00139\n",
            "Epoch [90/200], Loss: 0.00081\n",
            "Epoch [100/200], Loss: 0.00098\n",
            "Epoch [110/200], Loss: 0.00074\n",
            "Epoch [120/200], Loss: 0.00205\n",
            "Epoch [130/200], Loss: 0.00091\n",
            "Epoch [140/200], Loss: 0.00079\n",
            "Epoch [150/200], Loss: 0.00096\n",
            "Epoch [160/200], Loss: 0.00081\n",
            "Epoch [170/200], Loss: 0.00060\n",
            "Epoch [180/200], Loss: 0.00174\n",
            "Epoch [190/200], Loss: 0.00045\n",
            "Epoch [200/200], Loss: 0.00130\n",
            "--------------------\n",
            "stock number: 1825\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00958\n",
            "Epoch [20/200], Loss: 0.00194\n",
            "Epoch [30/200], Loss: 0.02856\n",
            "Epoch [40/200], Loss: 0.00141\n",
            "Epoch [50/200], Loss: 0.00117\n",
            "Epoch [60/200], Loss: 0.00488\n",
            "Epoch [70/200], Loss: 0.00096\n",
            "Epoch [80/200], Loss: 0.00282\n",
            "Epoch [90/200], Loss: 0.00460\n",
            "Epoch [100/200], Loss: 0.00197\n",
            "Epoch [110/200], Loss: 0.00134\n",
            "Epoch [120/200], Loss: 0.00481\n",
            "Epoch [130/200], Loss: 0.00184\n",
            "Epoch [140/200], Loss: 0.00282\n",
            "Epoch [150/200], Loss: 0.00300\n",
            "Epoch [160/200], Loss: 0.00192\n",
            "Epoch [170/200], Loss: 0.00573\n",
            "Epoch [180/200], Loss: 0.00235\n",
            "Epoch [190/200], Loss: 0.00259\n",
            "Epoch [200/200], Loss: 0.00355\n",
            "--------------------\n",
            "stock number: 1826\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02547\n",
            "Epoch [20/200], Loss: 0.00961\n",
            "Epoch [30/200], Loss: 0.00490\n",
            "Epoch [40/200], Loss: 0.00290\n",
            "Epoch [50/200], Loss: 0.00413\n",
            "Epoch [60/200], Loss: 0.00500\n",
            "Epoch [70/200], Loss: 0.00191\n",
            "Epoch [80/200], Loss: 0.00215\n",
            "Epoch [90/200], Loss: 0.00474\n",
            "Epoch [100/200], Loss: 0.00347\n",
            "Epoch [110/200], Loss: 0.00227\n",
            "Epoch [120/200], Loss: 0.00212\n",
            "Epoch [130/200], Loss: 0.00250\n",
            "Epoch [140/200], Loss: 0.00122\n",
            "Epoch [150/200], Loss: 0.00197\n",
            "Epoch [160/200], Loss: 0.00090\n",
            "Epoch [170/200], Loss: 0.00120\n",
            "Epoch [180/200], Loss: 0.00155\n",
            "Epoch [190/200], Loss: 0.00069\n",
            "Epoch [200/200], Loss: 0.00221\n",
            "--------------------\n",
            "stock number: 1827\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02028\n",
            "Epoch [20/200], Loss: 0.03198\n",
            "Epoch [30/200], Loss: 0.00482\n",
            "Epoch [40/200], Loss: 0.00275\n",
            "Epoch [50/200], Loss: 0.00221\n",
            "Epoch [60/200], Loss: 0.00200\n",
            "Epoch [70/200], Loss: 0.00133\n",
            "Epoch [80/200], Loss: 0.00346\n",
            "Epoch [90/200], Loss: 0.00136\n",
            "Epoch [100/200], Loss: 0.00098\n",
            "Epoch [110/200], Loss: 0.00194\n",
            "Epoch [120/200], Loss: 0.00097\n",
            "Epoch [130/200], Loss: 0.00087\n",
            "Epoch [140/200], Loss: 0.00087\n",
            "Epoch [150/200], Loss: 0.00334\n",
            "Epoch [160/200], Loss: 0.00079\n",
            "Epoch [170/200], Loss: 0.00072\n",
            "Epoch [180/200], Loss: 0.00068\n",
            "Epoch [190/200], Loss: 0.00183\n",
            "Epoch [200/200], Loss: 0.00098\n",
            "--------------------\n",
            "stock number: 1828\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01061\n",
            "Epoch [20/200], Loss: 0.00163\n",
            "Epoch [30/200], Loss: 0.00158\n",
            "Epoch [40/200], Loss: 0.00173\n",
            "Epoch [50/200], Loss: 0.00123\n",
            "Epoch [60/200], Loss: 0.00088\n",
            "Epoch [70/200], Loss: 0.00043\n",
            "Epoch [80/200], Loss: 0.00038\n",
            "Epoch [90/200], Loss: 0.00038\n",
            "Epoch [100/200], Loss: 0.00048\n",
            "Epoch [110/200], Loss: 0.00031\n",
            "Epoch [120/200], Loss: 0.00028\n",
            "Epoch [130/200], Loss: 0.00036\n",
            "Epoch [140/200], Loss: 0.00033\n",
            "Epoch [150/200], Loss: 0.00022\n",
            "Epoch [160/200], Loss: 0.00021\n",
            "Epoch [170/200], Loss: 0.00026\n",
            "Epoch [180/200], Loss: 0.00025\n",
            "Epoch [190/200], Loss: 0.00018\n",
            "Epoch [200/200], Loss: 0.00019\n",
            "--------------------\n",
            "stock number: 1829\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.04497\n",
            "Epoch [20/200], Loss: 0.00655\n",
            "Epoch [30/200], Loss: 0.00399\n",
            "Epoch [40/200], Loss: 0.00475\n",
            "Epoch [50/200], Loss: 0.01259\n",
            "Epoch [60/200], Loss: 0.00398\n",
            "Epoch [70/200], Loss: 0.00561\n",
            "Epoch [80/200], Loss: 0.00399\n",
            "Epoch [90/200], Loss: 0.00519\n",
            "Epoch [100/200], Loss: 0.00342\n",
            "Epoch [110/200], Loss: 0.00549\n",
            "Epoch [120/200], Loss: 0.00410\n",
            "Epoch [130/200], Loss: 0.00465\n",
            "Epoch [140/200], Loss: 0.00334\n",
            "Epoch [150/200], Loss: 0.00382\n",
            "Epoch [160/200], Loss: 0.00462\n",
            "Epoch [170/200], Loss: 0.00322\n",
            "Epoch [180/200], Loss: 0.00258\n",
            "Epoch [190/200], Loss: 0.00322\n",
            "Epoch [200/200], Loss: 0.00231\n",
            "--------------------\n",
            "stock number: 1830\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00772\n",
            "Epoch [20/200], Loss: 0.00533\n",
            "Epoch [30/200], Loss: 0.00126\n",
            "Epoch [40/200], Loss: 0.00111\n",
            "Epoch [50/200], Loss: 0.00069\n",
            "Epoch [60/200], Loss: 0.00360\n",
            "Epoch [70/200], Loss: 0.00092\n",
            "Epoch [80/200], Loss: 0.00037\n",
            "Epoch [90/200], Loss: 0.00060\n",
            "Epoch [100/200], Loss: 0.00069\n",
            "Epoch [110/200], Loss: 0.00033\n",
            "Epoch [120/200], Loss: 0.00029\n",
            "Epoch [130/200], Loss: 0.00023\n",
            "Epoch [140/200], Loss: 0.00382\n",
            "Epoch [150/200], Loss: 0.00031\n",
            "Epoch [160/200], Loss: 0.00022\n",
            "Epoch [170/200], Loss: 0.00078\n",
            "Epoch [180/200], Loss: 0.00029\n",
            "Epoch [190/200], Loss: 0.00041\n",
            "Epoch [200/200], Loss: 0.00047\n",
            "--------------------\n",
            "stock number: 1831\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.04126\n",
            "Epoch [20/200], Loss: 0.02042\n",
            "Epoch [30/200], Loss: 0.01258\n",
            "Epoch [40/200], Loss: 0.00167\n",
            "Epoch [50/200], Loss: 0.00085\n",
            "Epoch [60/200], Loss: 0.00082\n",
            "Epoch [70/200], Loss: 0.00036\n",
            "Epoch [80/200], Loss: 0.00416\n",
            "Epoch [90/200], Loss: 0.00025\n",
            "Epoch [100/200], Loss: 0.00059\n",
            "Epoch [110/200], Loss: 0.00178\n",
            "Epoch [120/200], Loss: 0.00039\n",
            "Epoch [130/200], Loss: 0.00099\n",
            "Epoch [140/200], Loss: 0.00048\n",
            "Epoch [150/200], Loss: 0.00048\n",
            "Epoch [160/200], Loss: 0.00046\n",
            "Epoch [170/200], Loss: 0.00050\n",
            "Epoch [180/200], Loss: 0.00052\n",
            "Epoch [190/200], Loss: 0.00080\n",
            "Epoch [200/200], Loss: 0.00060\n",
            "--------------------\n",
            "stock number: 1832\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00049\n",
            "Epoch [20/200], Loss: 0.00030\n",
            "Epoch [30/200], Loss: 0.00025\n",
            "Epoch [40/200], Loss: 0.00027\n",
            "Epoch [50/200], Loss: 0.00022\n",
            "Epoch [60/200], Loss: 0.00020\n",
            "Epoch [70/200], Loss: 0.00046\n",
            "Epoch [80/200], Loss: 0.00057\n",
            "Epoch [90/200], Loss: 0.00019\n",
            "Epoch [100/200], Loss: 0.00017\n",
            "Epoch [110/200], Loss: 0.00019\n",
            "Epoch [120/200], Loss: 0.00011\n",
            "Epoch [130/200], Loss: 0.00020\n",
            "Epoch [140/200], Loss: 0.00022\n",
            "Epoch [150/200], Loss: 0.00008\n",
            "Epoch [160/200], Loss: 0.00043\n",
            "Epoch [170/200], Loss: 0.00013\n",
            "Epoch [180/200], Loss: 0.00021\n",
            "Epoch [190/200], Loss: 0.00012\n",
            "Epoch [200/200], Loss: 0.00012\n",
            "--------------------\n",
            "stock number: 1833\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00474\n",
            "Epoch [20/200], Loss: 0.00820\n",
            "Epoch [30/200], Loss: 0.00297\n",
            "Epoch [40/200], Loss: 0.00353\n",
            "Epoch [50/200], Loss: 0.00551\n",
            "Epoch [60/200], Loss: 0.00309\n",
            "Epoch [70/200], Loss: 0.00326\n",
            "Epoch [80/200], Loss: 0.00574\n",
            "Epoch [90/200], Loss: 0.00656\n",
            "Epoch [100/200], Loss: 0.00281\n",
            "Epoch [110/200], Loss: 0.00253\n",
            "Epoch [120/200], Loss: 0.00142\n",
            "Epoch [130/200], Loss: 0.00251\n",
            "Epoch [140/200], Loss: 0.00209\n",
            "Epoch [150/200], Loss: 0.00312\n",
            "Epoch [160/200], Loss: 0.00308\n",
            "Epoch [170/200], Loss: 0.00244\n",
            "Epoch [180/200], Loss: 0.00152\n",
            "Epoch [190/200], Loss: 0.00245\n",
            "Epoch [200/200], Loss: 0.00115\n",
            "--------------------\n",
            "stock number: 1834\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00941\n",
            "Epoch [20/200], Loss: 0.00799\n",
            "Epoch [30/200], Loss: 0.00354\n",
            "Epoch [40/200], Loss: 0.00089\n",
            "Epoch [50/200], Loss: 0.00103\n",
            "Epoch [60/200], Loss: 0.00029\n",
            "Epoch [70/200], Loss: 0.00045\n",
            "Epoch [80/200], Loss: 0.00017\n",
            "Epoch [90/200], Loss: 0.00093\n",
            "Epoch [100/200], Loss: 0.00043\n",
            "Epoch [110/200], Loss: 0.00052\n",
            "Epoch [120/200], Loss: 0.00062\n",
            "Epoch [130/200], Loss: 0.00024\n",
            "Epoch [140/200], Loss: 0.00031\n",
            "Epoch [150/200], Loss: 0.00013\n",
            "Epoch [160/200], Loss: 0.00021\n",
            "Epoch [170/200], Loss: 0.00009\n",
            "Epoch [180/200], Loss: 0.00013\n",
            "Epoch [190/200], Loss: 0.00014\n",
            "Epoch [200/200], Loss: 0.00023\n",
            "--------------------\n",
            "stock number: 1835\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01938\n",
            "Epoch [20/200], Loss: 0.00915\n",
            "Epoch [30/200], Loss: 0.00204\n",
            "Epoch [40/200], Loss: 0.00277\n",
            "Epoch [50/200], Loss: 0.00301\n",
            "Epoch [60/200], Loss: 0.00242\n",
            "Epoch [70/200], Loss: 0.00125\n",
            "Epoch [80/200], Loss: 0.00149\n",
            "Epoch [90/200], Loss: 0.00119\n",
            "Epoch [100/200], Loss: 0.00055\n",
            "Epoch [110/200], Loss: 0.00352\n",
            "Epoch [120/200], Loss: 0.00055\n",
            "Epoch [130/200], Loss: 0.00043\n",
            "Epoch [140/200], Loss: 0.00051\n",
            "Epoch [150/200], Loss: 0.00407\n",
            "Epoch [160/200], Loss: 0.00044\n",
            "Epoch [170/200], Loss: 0.00042\n",
            "Epoch [180/200], Loss: 0.00395\n",
            "Epoch [190/200], Loss: 0.00025\n",
            "Epoch [200/200], Loss: 0.00031\n",
            "--------------------\n",
            "stock number: 1836\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02388\n",
            "Epoch [20/200], Loss: 0.02564\n",
            "Epoch [30/200], Loss: 0.00751\n",
            "Epoch [40/200], Loss: 0.01020\n",
            "Epoch [50/200], Loss: 0.00471\n",
            "Epoch [60/200], Loss: 0.00321\n",
            "Epoch [70/200], Loss: 0.00307\n",
            "Epoch [80/200], Loss: 0.00380\n",
            "Epoch [90/200], Loss: 0.00276\n",
            "Epoch [100/200], Loss: 0.00252\n",
            "Epoch [110/200], Loss: 0.00446\n",
            "Epoch [120/200], Loss: 0.00507\n",
            "Epoch [130/200], Loss: 0.00325\n",
            "Epoch [140/200], Loss: 0.00286\n",
            "Epoch [150/200], Loss: 0.00839\n",
            "Epoch [160/200], Loss: 0.00359\n",
            "Epoch [170/200], Loss: 0.00325\n",
            "Epoch [180/200], Loss: 0.00355\n",
            "Epoch [190/200], Loss: 0.00457\n",
            "Epoch [200/200], Loss: 0.00252\n",
            "--------------------\n",
            "stock number: 1837\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.04559\n",
            "Epoch [20/200], Loss: 0.00145\n",
            "Epoch [30/200], Loss: 0.00081\n",
            "Epoch [40/200], Loss: 0.01437\n",
            "Epoch [50/200], Loss: 0.00085\n",
            "Epoch [60/200], Loss: 0.00074\n",
            "Epoch [70/200], Loss: 0.00919\n",
            "Epoch [80/200], Loss: 0.00107\n",
            "Epoch [90/200], Loss: 0.00099\n",
            "Epoch [100/200], Loss: 0.00077\n",
            "Epoch [110/200], Loss: 0.00067\n",
            "Epoch [120/200], Loss: 0.00067\n",
            "Epoch [130/200], Loss: 0.00066\n",
            "Epoch [140/200], Loss: 0.00056\n",
            "Epoch [150/200], Loss: 0.00064\n",
            "Epoch [160/200], Loss: 0.00062\n",
            "Epoch [170/200], Loss: 0.00058\n",
            "Epoch [180/200], Loss: 0.00051\n",
            "Epoch [190/200], Loss: 0.00070\n",
            "Epoch [200/200], Loss: 0.00056\n",
            "--------------------\n",
            "stock number: 1838\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01806\n",
            "Epoch [20/200], Loss: 0.00836\n",
            "Epoch [30/200], Loss: 0.00261\n",
            "Epoch [40/200], Loss: 0.00851\n",
            "Epoch [50/200], Loss: 0.00072\n",
            "Epoch [60/200], Loss: 0.00166\n",
            "Epoch [70/200], Loss: 0.00040\n",
            "Epoch [80/200], Loss: 0.00089\n",
            "Epoch [90/200], Loss: 0.00046\n",
            "Epoch [100/200], Loss: 0.00054\n",
            "Epoch [110/200], Loss: 0.00100\n",
            "Epoch [120/200], Loss: 0.00025\n",
            "Epoch [130/200], Loss: 0.00042\n",
            "Epoch [140/200], Loss: 0.00114\n",
            "Epoch [150/200], Loss: 0.00039\n",
            "Epoch [160/200], Loss: 0.00027\n",
            "Epoch [170/200], Loss: 0.00018\n",
            "Epoch [180/200], Loss: 0.00019\n",
            "Epoch [190/200], Loss: 0.00107\n",
            "Epoch [200/200], Loss: 0.00053\n",
            "--------------------\n",
            "stock number: 1839\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02093\n",
            "Epoch [20/200], Loss: 0.01094\n",
            "Epoch [30/200], Loss: 0.00302\n",
            "Epoch [40/200], Loss: 0.00252\n",
            "Epoch [50/200], Loss: 0.00152\n",
            "Epoch [60/200], Loss: 0.00141\n",
            "Epoch [70/200], Loss: 0.00434\n",
            "Epoch [80/200], Loss: 0.00107\n",
            "Epoch [90/200], Loss: 0.00300\n",
            "Epoch [100/200], Loss: 0.00111\n",
            "Epoch [110/200], Loss: 0.00090\n",
            "Epoch [120/200], Loss: 0.00082\n",
            "Epoch [130/200], Loss: 0.00080\n",
            "Epoch [140/200], Loss: 0.00079\n",
            "Epoch [150/200], Loss: 0.00055\n",
            "Epoch [160/200], Loss: 0.00052\n",
            "Epoch [170/200], Loss: 0.00059\n",
            "Epoch [180/200], Loss: 0.00053\n",
            "Epoch [190/200], Loss: 0.00041\n",
            "Epoch [200/200], Loss: 0.00037\n",
            "--------------------\n",
            "stock number: 1840\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00588\n",
            "Epoch [20/200], Loss: 0.00237\n",
            "Epoch [30/200], Loss: 0.00173\n",
            "Epoch [40/200], Loss: 0.00107\n",
            "Epoch [50/200], Loss: 0.00195\n",
            "Epoch [60/200], Loss: 0.00092\n",
            "Epoch [70/200], Loss: 0.00149\n",
            "Epoch [80/200], Loss: 0.00055\n",
            "Epoch [90/200], Loss: 0.00082\n",
            "Epoch [100/200], Loss: 0.00104\n",
            "Epoch [110/200], Loss: 0.00049\n",
            "Epoch [120/200], Loss: 0.00051\n",
            "Epoch [130/200], Loss: 0.00040\n",
            "Epoch [140/200], Loss: 0.00126\n",
            "Epoch [150/200], Loss: 0.00116\n",
            "Epoch [160/200], Loss: 0.00037\n",
            "Epoch [170/200], Loss: 0.00032\n",
            "Epoch [180/200], Loss: 0.00029\n",
            "Epoch [190/200], Loss: 0.00047\n",
            "Epoch [200/200], Loss: 0.00026\n",
            "--------------------\n",
            "stock number: 1841\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00020\n",
            "Epoch [20/200], Loss: 0.00024\n",
            "Epoch [30/200], Loss: 0.00020\n",
            "Epoch [40/200], Loss: 0.00020\n",
            "Epoch [50/200], Loss: 0.00027\n",
            "Epoch [60/200], Loss: 0.00026\n",
            "Epoch [70/200], Loss: 0.00025\n",
            "Epoch [80/200], Loss: 0.00020\n",
            "Epoch [90/200], Loss: 0.00019\n",
            "Epoch [100/200], Loss: 0.00019\n",
            "Epoch [110/200], Loss: 0.00026\n",
            "Epoch [120/200], Loss: 0.00026\n",
            "Epoch [130/200], Loss: 0.00020\n",
            "Epoch [140/200], Loss: 0.00018\n",
            "Epoch [150/200], Loss: 0.00036\n",
            "Epoch [160/200], Loss: 0.00040\n",
            "Epoch [170/200], Loss: 0.00049\n",
            "Epoch [180/200], Loss: 0.00029\n",
            "Epoch [190/200], Loss: 0.00017\n",
            "Epoch [200/200], Loss: 0.00027\n",
            "--------------------\n",
            "stock number: 1842\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01989\n",
            "Epoch [20/200], Loss: 0.00817\n",
            "Epoch [30/200], Loss: 0.00213\n",
            "Epoch [40/200], Loss: 0.00163\n",
            "Epoch [50/200], Loss: 0.00146\n",
            "Epoch [60/200], Loss: 0.00111\n",
            "Epoch [70/200], Loss: 0.00114\n",
            "Epoch [80/200], Loss: 0.00091\n",
            "Epoch [90/200], Loss: 0.00106\n",
            "Epoch [100/200], Loss: 0.00396\n",
            "Epoch [110/200], Loss: 0.00084\n",
            "Epoch [120/200], Loss: 0.00082\n",
            "Epoch [130/200], Loss: 0.00155\n",
            "Epoch [140/200], Loss: 0.00070\n",
            "Epoch [150/200], Loss: 0.00126\n",
            "Epoch [160/200], Loss: 0.00124\n",
            "Epoch [170/200], Loss: 0.00199\n",
            "Epoch [180/200], Loss: 0.00065\n",
            "Epoch [190/200], Loss: 0.00065\n",
            "Epoch [200/200], Loss: 0.00074\n",
            "--------------------\n",
            "stock number: 1843\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00495\n",
            "Epoch [20/200], Loss: 0.00116\n",
            "Epoch [30/200], Loss: 0.00017\n",
            "Epoch [40/200], Loss: 0.00023\n",
            "Epoch [50/200], Loss: 0.00059\n",
            "Epoch [60/200], Loss: 0.00017\n",
            "Epoch [70/200], Loss: 0.00017\n",
            "Epoch [80/200], Loss: 0.00016\n",
            "Epoch [90/200], Loss: 0.00015\n",
            "Epoch [100/200], Loss: 0.00016\n",
            "Epoch [110/200], Loss: 0.00015\n",
            "Epoch [120/200], Loss: 0.00015\n",
            "Epoch [130/200], Loss: 0.00016\n",
            "Epoch [140/200], Loss: 0.00015\n",
            "Epoch [150/200], Loss: 0.00016\n",
            "Epoch [160/200], Loss: 0.00015\n",
            "Epoch [170/200], Loss: 0.00017\n",
            "Epoch [180/200], Loss: 0.00016\n",
            "Epoch [190/200], Loss: 0.00015\n",
            "Epoch [200/200], Loss: 0.00015\n",
            "--------------------\n",
            "stock number: 1844\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02705\n",
            "Epoch [20/200], Loss: 0.02705\n",
            "Epoch [30/200], Loss: 0.02705\n",
            "Epoch [40/200], Loss: 0.02640\n",
            "Epoch [50/200], Loss: 0.02287\n",
            "Epoch [60/200], Loss: 0.02252\n",
            "Epoch [70/200], Loss: 0.01428\n",
            "Epoch [80/200], Loss: 0.01809\n",
            "Epoch [90/200], Loss: 0.00695\n",
            "Epoch [100/200], Loss: 0.00253\n",
            "Epoch [110/200], Loss: 0.00307\n",
            "Epoch [120/200], Loss: 0.00037\n",
            "Epoch [130/200], Loss: 0.00043\n",
            "Epoch [140/200], Loss: 0.00020\n",
            "Epoch [150/200], Loss: 0.00029\n",
            "Epoch [160/200], Loss: 0.00038\n",
            "Epoch [170/200], Loss: 0.00040\n",
            "Epoch [180/200], Loss: 0.00017\n",
            "Epoch [190/200], Loss: 0.00033\n",
            "Epoch [200/200], Loss: 0.00021\n",
            "--------------------\n",
            "stock number: 1845\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00468\n",
            "Epoch [20/200], Loss: 0.00269\n",
            "Epoch [30/200], Loss: 0.00175\n",
            "Epoch [40/200], Loss: 0.00180\n",
            "Epoch [50/200], Loss: 0.00295\n",
            "Epoch [60/200], Loss: 0.00123\n",
            "Epoch [70/200], Loss: 0.00247\n",
            "Epoch [80/200], Loss: 0.00128\n",
            "Epoch [90/200], Loss: 0.00133\n",
            "Epoch [100/200], Loss: 0.00130\n",
            "Epoch [110/200], Loss: 0.00105\n",
            "Epoch [120/200], Loss: 0.00103\n",
            "Epoch [130/200], Loss: 0.00063\n",
            "Epoch [140/200], Loss: 0.00132\n",
            "Epoch [150/200], Loss: 0.00193\n",
            "Epoch [160/200], Loss: 0.00091\n",
            "Epoch [170/200], Loss: 0.00060\n",
            "Epoch [180/200], Loss: 0.00092\n",
            "Epoch [190/200], Loss: 0.00079\n",
            "Epoch [200/200], Loss: 0.00084\n",
            "--------------------\n",
            "stock number: 1846\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00834\n",
            "Epoch [20/200], Loss: 0.00528\n",
            "Epoch [30/200], Loss: 0.00912\n",
            "Epoch [40/200], Loss: 0.00429\n",
            "Epoch [50/200], Loss: 0.00601\n",
            "Epoch [60/200], Loss: 0.02114\n",
            "Epoch [70/200], Loss: 0.00244\n",
            "Epoch [80/200], Loss: 0.00301\n",
            "Epoch [90/200], Loss: 0.00210\n",
            "Epoch [100/200], Loss: 0.00203\n",
            "Epoch [110/200], Loss: 0.02192\n",
            "Epoch [120/200], Loss: 0.00248\n",
            "Epoch [130/200], Loss: 0.00220\n",
            "Epoch [140/200], Loss: 0.00628\n",
            "Epoch [150/200], Loss: 0.00296\n",
            "Epoch [160/200], Loss: 0.00157\n",
            "Epoch [170/200], Loss: 0.00366\n",
            "Epoch [180/200], Loss: 0.00274\n",
            "Epoch [190/200], Loss: 0.00130\n",
            "Epoch [200/200], Loss: 0.00241\n",
            "--------------------\n",
            "stock number: 1847\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.03242\n",
            "Epoch [20/200], Loss: 0.02259\n",
            "Epoch [30/200], Loss: 0.01407\n",
            "Epoch [40/200], Loss: 0.00511\n",
            "Epoch [50/200], Loss: 0.00186\n",
            "Epoch [60/200], Loss: 0.00337\n",
            "Epoch [70/200], Loss: 0.00085\n",
            "Epoch [80/200], Loss: 0.00122\n",
            "Epoch [90/200], Loss: 0.00132\n",
            "Epoch [100/200], Loss: 0.00134\n",
            "Epoch [110/200], Loss: 0.00062\n",
            "Epoch [120/200], Loss: 0.00130\n",
            "Epoch [130/200], Loss: 0.00101\n",
            "Epoch [140/200], Loss: 0.00088\n",
            "Epoch [150/200], Loss: 0.00092\n",
            "Epoch [160/200], Loss: 0.00080\n",
            "Epoch [170/200], Loss: 0.00165\n",
            "Epoch [180/200], Loss: 0.00037\n",
            "Epoch [190/200], Loss: 0.00078\n",
            "Epoch [200/200], Loss: 0.00136\n",
            "--------------------\n",
            "stock number: 1848\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.04845\n",
            "Epoch [20/200], Loss: 0.00551\n",
            "Epoch [30/200], Loss: 0.01149\n",
            "Epoch [40/200], Loss: 0.00288\n",
            "Epoch [50/200], Loss: 0.00236\n",
            "Epoch [60/200], Loss: 0.01065\n",
            "Epoch [70/200], Loss: 0.00360\n",
            "Epoch [80/200], Loss: 0.00276\n",
            "Epoch [90/200], Loss: 0.00228\n",
            "Epoch [100/200], Loss: 0.00256\n",
            "Epoch [110/200], Loss: 0.00350\n",
            "Epoch [120/200], Loss: 0.00292\n",
            "Epoch [130/200], Loss: 0.00213\n",
            "Epoch [140/200], Loss: 0.00204\n",
            "Epoch [150/200], Loss: 0.00377\n",
            "Epoch [160/200], Loss: 0.00406\n",
            "Epoch [170/200], Loss: 0.00200\n",
            "Epoch [180/200], Loss: 0.00119\n",
            "Epoch [190/200], Loss: 0.00195\n",
            "Epoch [200/200], Loss: 0.00178\n",
            "--------------------\n",
            "stock number: 1849\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00231\n",
            "Epoch [20/200], Loss: 0.00242\n",
            "Epoch [30/200], Loss: 0.00245\n",
            "Epoch [40/200], Loss: 0.00124\n",
            "Epoch [50/200], Loss: 0.00125\n",
            "Epoch [60/200], Loss: 0.00171\n",
            "Epoch [70/200], Loss: 0.00106\n",
            "Epoch [80/200], Loss: 0.00115\n",
            "Epoch [90/200], Loss: 0.00078\n",
            "Epoch [100/200], Loss: 0.00238\n",
            "Epoch [110/200], Loss: 0.00186\n",
            "Epoch [120/200], Loss: 0.00245\n",
            "Epoch [130/200], Loss: 0.00187\n",
            "Epoch [140/200], Loss: 0.00048\n",
            "Epoch [150/200], Loss: 0.00245\n",
            "Epoch [160/200], Loss: 0.00245\n",
            "Epoch [170/200], Loss: 0.00233\n",
            "Epoch [180/200], Loss: 0.00201\n",
            "Epoch [190/200], Loss: 0.00163\n",
            "Epoch [200/200], Loss: 0.00237\n",
            "--------------------\n",
            "stock number: 1850\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00443\n",
            "Epoch [20/200], Loss: 0.00176\n",
            "Epoch [30/200], Loss: 0.00572\n",
            "Epoch [40/200], Loss: 0.00139\n",
            "Epoch [50/200], Loss: 0.00141\n",
            "Epoch [60/200], Loss: 0.00160\n",
            "Epoch [70/200], Loss: 0.00096\n",
            "Epoch [80/200], Loss: 0.00112\n",
            "Epoch [90/200], Loss: 0.00134\n",
            "Epoch [100/200], Loss: 0.00098\n",
            "Epoch [110/200], Loss: 0.00115\n",
            "Epoch [120/200], Loss: 0.00065\n",
            "Epoch [130/200], Loss: 0.00071\n",
            "Epoch [140/200], Loss: 0.00040\n",
            "Epoch [150/200], Loss: 0.00103\n",
            "Epoch [160/200], Loss: 0.00053\n",
            "Epoch [170/200], Loss: 0.00054\n",
            "Epoch [180/200], Loss: 0.00113\n",
            "Epoch [190/200], Loss: 0.00143\n",
            "Epoch [200/200], Loss: 0.00110\n",
            "--------------------\n",
            "stock number: 1851\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00424\n",
            "Epoch [20/200], Loss: 0.00360\n",
            "Epoch [30/200], Loss: 0.00390\n",
            "Epoch [40/200], Loss: 0.00182\n",
            "Epoch [50/200], Loss: 0.00102\n",
            "Epoch [60/200], Loss: 0.00096\n",
            "Epoch [70/200], Loss: 0.00039\n",
            "Epoch [80/200], Loss: 0.00071\n",
            "Epoch [90/200], Loss: 0.00029\n",
            "Epoch [100/200], Loss: 0.00022\n",
            "Epoch [110/200], Loss: 0.00066\n",
            "Epoch [120/200], Loss: 0.00010\n",
            "Epoch [130/200], Loss: 0.00011\n",
            "Epoch [140/200], Loss: 0.00081\n",
            "Epoch [150/200], Loss: 0.00012\n",
            "Epoch [160/200], Loss: 0.00010\n",
            "Epoch [170/200], Loss: 0.00009\n",
            "Epoch [180/200], Loss: 0.00033\n",
            "Epoch [190/200], Loss: 0.00021\n",
            "Epoch [200/200], Loss: 0.00013\n",
            "--------------------\n",
            "stock number: 1852\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00174\n",
            "Epoch [20/200], Loss: 0.00058\n",
            "Epoch [30/200], Loss: 0.00054\n",
            "Epoch [40/200], Loss: 0.00052\n",
            "Epoch [50/200], Loss: 0.00067\n",
            "Epoch [60/200], Loss: 0.00067\n",
            "Epoch [70/200], Loss: 0.00074\n",
            "Epoch [80/200], Loss: 0.00066\n",
            "Epoch [90/200], Loss: 0.00071\n",
            "Epoch [100/200], Loss: 0.00073\n",
            "Epoch [110/200], Loss: 0.00072\n",
            "Epoch [120/200], Loss: 0.00074\n",
            "Epoch [130/200], Loss: 0.00044\n",
            "Epoch [140/200], Loss: 0.00040\n",
            "Epoch [150/200], Loss: 0.00059\n",
            "Epoch [160/200], Loss: 0.00054\n",
            "Epoch [170/200], Loss: 0.00062\n",
            "Epoch [180/200], Loss: 0.00044\n",
            "Epoch [190/200], Loss: 0.00035\n",
            "Epoch [200/200], Loss: 0.00031\n",
            "--------------------\n",
            "stock number: 1853\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00210\n",
            "Epoch [20/200], Loss: 0.00172\n",
            "Epoch [30/200], Loss: 0.00220\n",
            "Epoch [40/200], Loss: 0.00264\n",
            "Epoch [50/200], Loss: 0.00101\n",
            "Epoch [60/200], Loss: 0.00079\n",
            "Epoch [70/200], Loss: 0.00087\n",
            "Epoch [80/200], Loss: 0.00103\n",
            "Epoch [90/200], Loss: 0.00072\n",
            "Epoch [100/200], Loss: 0.00082\n",
            "Epoch [110/200], Loss: 0.00042\n",
            "Epoch [120/200], Loss: 0.00085\n",
            "Epoch [130/200], Loss: 0.00037\n",
            "Epoch [140/200], Loss: 0.00095\n",
            "Epoch [150/200], Loss: 0.00028\n",
            "Epoch [160/200], Loss: 0.00035\n",
            "Epoch [170/200], Loss: 0.00079\n",
            "Epoch [180/200], Loss: 0.00042\n",
            "Epoch [190/200], Loss: 0.00025\n",
            "Epoch [200/200], Loss: 0.00035\n",
            "--------------------\n",
            "stock number: 1854\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00464\n",
            "Epoch [20/200], Loss: 0.00623\n",
            "Epoch [30/200], Loss: 0.00114\n",
            "Epoch [40/200], Loss: 0.00087\n",
            "Epoch [50/200], Loss: 0.00092\n",
            "Epoch [60/200], Loss: 0.00041\n",
            "Epoch [70/200], Loss: 0.00096\n",
            "Epoch [80/200], Loss: 0.00026\n",
            "Epoch [90/200], Loss: 0.00121\n",
            "Epoch [100/200], Loss: 0.00069\n",
            "Epoch [110/200], Loss: 0.00026\n",
            "Epoch [120/200], Loss: 0.00038\n",
            "Epoch [130/200], Loss: 0.00054\n",
            "Epoch [140/200], Loss: 0.00022\n",
            "Epoch [150/200], Loss: 0.00041\n",
            "Epoch [160/200], Loss: 0.00019\n",
            "Epoch [170/200], Loss: 0.00016\n",
            "Epoch [180/200], Loss: 0.00016\n",
            "Epoch [190/200], Loss: 0.00032\n",
            "Epoch [200/200], Loss: 0.00049\n",
            "--------------------\n",
            "stock number: 1855\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.08772\n",
            "Epoch [20/200], Loss: 0.00129\n",
            "Epoch [30/200], Loss: 0.00006\n",
            "Epoch [40/200], Loss: 0.00006\n",
            "Epoch [50/200], Loss: 0.00006\n",
            "Epoch [60/200], Loss: 0.00006\n",
            "Epoch [70/200], Loss: 0.00006\n",
            "Epoch [80/200], Loss: 0.00006\n",
            "Epoch [90/200], Loss: 0.00006\n",
            "Epoch [100/200], Loss: 0.00006\n",
            "Epoch [110/200], Loss: 0.00006\n",
            "Epoch [120/200], Loss: 0.00006\n",
            "Epoch [130/200], Loss: 0.00006\n",
            "Epoch [140/200], Loss: 0.00006\n",
            "Epoch [150/200], Loss: 0.00007\n",
            "Epoch [160/200], Loss: 0.00006\n",
            "Epoch [170/200], Loss: 0.00006\n",
            "Epoch [180/200], Loss: 0.00006\n",
            "Epoch [190/200], Loss: 0.00007\n",
            "Epoch [200/200], Loss: 0.00006\n",
            "--------------------\n",
            "stock number: 1856\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00059\n",
            "Epoch [20/200], Loss: 0.00044\n",
            "Epoch [30/200], Loss: 0.00324\n",
            "Epoch [40/200], Loss: 0.00175\n",
            "Epoch [50/200], Loss: 0.00126\n",
            "Epoch [60/200], Loss: 0.00034\n",
            "Epoch [70/200], Loss: 0.00036\n",
            "Epoch [80/200], Loss: 0.00020\n",
            "Epoch [90/200], Loss: 0.00026\n",
            "Epoch [100/200], Loss: 0.00018\n",
            "Epoch [110/200], Loss: 0.00058\n",
            "Epoch [120/200], Loss: 0.00037\n",
            "Epoch [130/200], Loss: 0.00031\n",
            "Epoch [140/200], Loss: 0.00040\n",
            "Epoch [150/200], Loss: 0.00012\n",
            "Epoch [160/200], Loss: 0.00007\n",
            "Epoch [170/200], Loss: 0.00006\n",
            "Epoch [180/200], Loss: 0.00006\n",
            "Epoch [190/200], Loss: 0.00011\n",
            "Epoch [200/200], Loss: 0.00009\n",
            "--------------------\n",
            "stock number: 1857\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.05459\n",
            "Epoch [20/200], Loss: 0.04413\n",
            "Epoch [30/200], Loss: 0.00252\n",
            "Epoch [40/200], Loss: 0.00399\n",
            "Epoch [50/200], Loss: 0.00025\n",
            "Epoch [60/200], Loss: 0.00034\n",
            "Epoch [70/200], Loss: 0.00025\n",
            "Epoch [80/200], Loss: 0.00052\n",
            "Epoch [90/200], Loss: 0.00161\n",
            "Epoch [100/200], Loss: 0.00021\n",
            "Epoch [110/200], Loss: 0.00918\n",
            "Epoch [120/200], Loss: 0.00021\n",
            "Epoch [130/200], Loss: 0.00020\n",
            "Epoch [140/200], Loss: 0.00016\n",
            "Epoch [150/200], Loss: 0.00062\n",
            "Epoch [160/200], Loss: 0.00025\n",
            "Epoch [170/200], Loss: 0.00020\n",
            "Epoch [180/200], Loss: 0.00023\n",
            "Epoch [190/200], Loss: 0.00185\n",
            "Epoch [200/200], Loss: 0.00012\n",
            "--------------------\n",
            "stock number: 1858\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00904\n",
            "Epoch [20/200], Loss: 0.00235\n",
            "Epoch [30/200], Loss: 0.00299\n",
            "Epoch [40/200], Loss: 0.00536\n",
            "Epoch [50/200], Loss: 0.00141\n",
            "Epoch [60/200], Loss: 0.00118\n",
            "Epoch [70/200], Loss: 0.00247\n",
            "Epoch [80/200], Loss: 0.00080\n",
            "Epoch [90/200], Loss: 0.00152\n",
            "Epoch [100/200], Loss: 0.00072\n",
            "Epoch [110/200], Loss: 0.00090\n",
            "Epoch [120/200], Loss: 0.00057\n",
            "Epoch [130/200], Loss: 0.00094\n",
            "Epoch [140/200], Loss: 0.00072\n",
            "Epoch [150/200], Loss: 0.00254\n",
            "Epoch [160/200], Loss: 0.00046\n",
            "Epoch [170/200], Loss: 0.00095\n",
            "Epoch [180/200], Loss: 0.00058\n",
            "Epoch [190/200], Loss: 0.00082\n",
            "Epoch [200/200], Loss: 0.00087\n",
            "--------------------\n",
            "stock number: 1859\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00134\n",
            "Epoch [20/200], Loss: 0.00134\n",
            "Epoch [30/200], Loss: 0.00500\n",
            "Epoch [40/200], Loss: 0.00176\n",
            "Epoch [50/200], Loss: 0.00049\n",
            "Epoch [60/200], Loss: 0.00194\n",
            "Epoch [70/200], Loss: 0.00081\n",
            "Epoch [80/200], Loss: 0.00023\n",
            "Epoch [90/200], Loss: 0.00044\n",
            "Epoch [100/200], Loss: 0.00023\n",
            "Epoch [110/200], Loss: 0.00041\n",
            "Epoch [120/200], Loss: 0.00024\n",
            "Epoch [130/200], Loss: 0.00021\n",
            "Epoch [140/200], Loss: 0.00025\n",
            "Epoch [150/200], Loss: 0.00013\n",
            "Epoch [160/200], Loss: 0.00020\n",
            "Epoch [170/200], Loss: 0.00013\n",
            "Epoch [180/200], Loss: 0.00018\n",
            "Epoch [190/200], Loss: 0.00036\n",
            "Epoch [200/200], Loss: 0.00018\n",
            "--------------------\n",
            "stock number: 1860\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.03311\n",
            "Epoch [20/200], Loss: 0.02812\n",
            "Epoch [30/200], Loss: 0.02331\n",
            "Epoch [40/200], Loss: 0.02043\n",
            "Epoch [50/200], Loss: 0.01033\n",
            "Epoch [60/200], Loss: 0.00837\n",
            "Epoch [70/200], Loss: 0.00600\n",
            "Epoch [80/200], Loss: 0.00387\n",
            "Epoch [90/200], Loss: 0.00285\n",
            "Epoch [100/200], Loss: 0.00276\n",
            "Epoch [110/200], Loss: 0.00322\n",
            "Epoch [120/200], Loss: 0.00205\n",
            "Epoch [130/200], Loss: 0.00200\n",
            "Epoch [140/200], Loss: 0.00239\n",
            "Epoch [150/200], Loss: 0.00174\n",
            "Epoch [160/200], Loss: 0.00183\n",
            "Epoch [170/200], Loss: 0.00156\n",
            "Epoch [180/200], Loss: 0.00142\n",
            "Epoch [190/200], Loss: 0.00184\n",
            "Epoch [200/200], Loss: 0.00162\n",
            "--------------------\n",
            "stock number: 1861\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00446\n",
            "Epoch [20/200], Loss: 0.00365\n",
            "Epoch [30/200], Loss: 0.00122\n",
            "Epoch [40/200], Loss: 0.00456\n",
            "Epoch [50/200], Loss: 0.00184\n",
            "Epoch [60/200], Loss: 0.00350\n",
            "Epoch [70/200], Loss: 0.00035\n",
            "Epoch [80/200], Loss: 0.00013\n",
            "Epoch [90/200], Loss: 0.00139\n",
            "Epoch [100/200], Loss: 0.00015\n",
            "Epoch [110/200], Loss: 0.00049\n",
            "Epoch [120/200], Loss: 0.00113\n",
            "Epoch [130/200], Loss: 0.00032\n",
            "Epoch [140/200], Loss: 0.00022\n",
            "Epoch [150/200], Loss: 0.00028\n",
            "Epoch [160/200], Loss: 0.00036\n",
            "Epoch [170/200], Loss: 0.00016\n",
            "Epoch [180/200], Loss: 0.00041\n",
            "Epoch [190/200], Loss: 0.00017\n",
            "Epoch [200/200], Loss: 0.00024\n",
            "--------------------\n",
            "stock number: 1862\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.05152\n",
            "Epoch [20/200], Loss: 0.00098\n",
            "Epoch [30/200], Loss: 0.00181\n",
            "Epoch [40/200], Loss: 0.00249\n",
            "Epoch [50/200], Loss: 0.00166\n",
            "Epoch [60/200], Loss: 0.00312\n",
            "Epoch [70/200], Loss: 0.00196\n",
            "Epoch [80/200], Loss: 0.00062\n",
            "Epoch [90/200], Loss: 0.00085\n",
            "Epoch [100/200], Loss: 0.00120\n",
            "Epoch [110/200], Loss: 0.00109\n",
            "Epoch [120/200], Loss: 0.00113\n",
            "Epoch [130/200], Loss: 0.00351\n",
            "Epoch [140/200], Loss: 0.00068\n",
            "Epoch [150/200], Loss: 0.00053\n",
            "Epoch [160/200], Loss: 0.00112\n",
            "Epoch [170/200], Loss: 0.00296\n",
            "Epoch [180/200], Loss: 0.00109\n",
            "Epoch [190/200], Loss: 0.00165\n",
            "Epoch [200/200], Loss: 0.00083\n",
            "--------------------\n",
            "stock number: 1863\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00315\n",
            "Epoch [20/200], Loss: 0.00207\n",
            "Epoch [30/200], Loss: 0.00146\n",
            "Epoch [40/200], Loss: 0.00061\n",
            "Epoch [50/200], Loss: 0.00173\n",
            "Epoch [60/200], Loss: 0.00044\n",
            "Epoch [70/200], Loss: 0.00032\n",
            "Epoch [80/200], Loss: 0.00040\n",
            "Epoch [90/200], Loss: 0.00074\n",
            "Epoch [100/200], Loss: 0.00019\n",
            "Epoch [110/200], Loss: 0.00055\n",
            "Epoch [120/200], Loss: 0.00024\n",
            "Epoch [130/200], Loss: 0.00020\n",
            "Epoch [140/200], Loss: 0.00035\n",
            "Epoch [150/200], Loss: 0.00017\n",
            "Epoch [160/200], Loss: 0.00068\n",
            "Epoch [170/200], Loss: 0.00007\n",
            "Epoch [180/200], Loss: 0.00006\n",
            "Epoch [190/200], Loss: 0.00024\n",
            "Epoch [200/200], Loss: 0.00014\n",
            "--------------------\n",
            "stock number: 1864\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.11482\n",
            "Epoch [20/200], Loss: 0.06871\n",
            "Epoch [30/200], Loss: 0.03162\n",
            "Epoch [40/200], Loss: 0.01819\n",
            "Epoch [50/200], Loss: 0.01039\n",
            "Epoch [60/200], Loss: 0.00749\n",
            "Epoch [70/200], Loss: 0.00616\n",
            "Epoch [80/200], Loss: 0.00681\n",
            "Epoch [90/200], Loss: 0.00496\n",
            "Epoch [100/200], Loss: 0.00456\n",
            "Epoch [110/200], Loss: 0.00463\n",
            "Epoch [120/200], Loss: 0.00369\n",
            "Epoch [130/200], Loss: 0.00390\n",
            "Epoch [140/200], Loss: 0.00343\n",
            "Epoch [150/200], Loss: 0.00290\n",
            "Epoch [160/200], Loss: 0.00279\n",
            "Epoch [170/200], Loss: 0.00242\n",
            "Epoch [180/200], Loss: 0.00384\n",
            "Epoch [190/200], Loss: 0.00269\n",
            "Epoch [200/200], Loss: 0.00197\n",
            "--------------------\n",
            "stock number: 1865\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00617\n",
            "Epoch [20/200], Loss: 0.00294\n",
            "Epoch [30/200], Loss: 0.00499\n",
            "Epoch [40/200], Loss: 0.00084\n",
            "Epoch [50/200], Loss: 0.00129\n",
            "Epoch [60/200], Loss: 0.00068\n",
            "Epoch [70/200], Loss: 0.00018\n",
            "Epoch [80/200], Loss: 0.00034\n",
            "Epoch [90/200], Loss: 0.00077\n",
            "Epoch [100/200], Loss: 0.00040\n",
            "Epoch [110/200], Loss: 0.00033\n",
            "Epoch [120/200], Loss: 0.00029\n",
            "Epoch [130/200], Loss: 0.00016\n",
            "Epoch [140/200], Loss: 0.00056\n",
            "Epoch [150/200], Loss: 0.00025\n",
            "Epoch [160/200], Loss: 0.00015\n",
            "Epoch [170/200], Loss: 0.00043\n",
            "Epoch [180/200], Loss: 0.00017\n",
            "Epoch [190/200], Loss: 0.00034\n",
            "Epoch [200/200], Loss: 0.00018\n",
            "--------------------\n",
            "stock number: 1866\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01822\n",
            "Epoch [20/200], Loss: 0.00198\n",
            "Epoch [30/200], Loss: 0.00113\n",
            "Epoch [40/200], Loss: 0.00116\n",
            "Epoch [50/200], Loss: 0.00086\n",
            "Epoch [60/200], Loss: 0.00094\n",
            "Epoch [70/200], Loss: 0.00108\n",
            "Epoch [80/200], Loss: 0.00067\n",
            "Epoch [90/200], Loss: 0.00064\n",
            "Epoch [100/200], Loss: 0.00070\n",
            "Epoch [110/200], Loss: 0.00053\n",
            "Epoch [120/200], Loss: 0.00068\n",
            "Epoch [130/200], Loss: 0.00043\n",
            "Epoch [140/200], Loss: 0.00036\n",
            "Epoch [150/200], Loss: 0.00032\n",
            "Epoch [160/200], Loss: 0.00033\n",
            "Epoch [170/200], Loss: 0.00028\n",
            "Epoch [180/200], Loss: 0.00019\n",
            "Epoch [190/200], Loss: 0.00029\n",
            "Epoch [200/200], Loss: 0.00018\n",
            "--------------------\n",
            "stock number: 1867\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00914\n",
            "Epoch [20/200], Loss: 0.01362\n",
            "Epoch [30/200], Loss: 0.00360\n",
            "Epoch [40/200], Loss: 0.00325\n",
            "Epoch [50/200], Loss: 0.00302\n",
            "Epoch [60/200], Loss: 0.00578\n",
            "Epoch [70/200], Loss: 0.00374\n",
            "Epoch [80/200], Loss: 0.00729\n",
            "Epoch [90/200], Loss: 0.00187\n",
            "Epoch [100/200], Loss: 0.00382\n",
            "Epoch [110/200], Loss: 0.00294\n",
            "Epoch [120/200], Loss: 0.00374\n",
            "Epoch [130/200], Loss: 0.00513\n",
            "Epoch [140/200], Loss: 0.00169\n",
            "Epoch [150/200], Loss: 0.00368\n",
            "Epoch [160/200], Loss: 0.00335\n",
            "Epoch [170/200], Loss: 0.00334\n",
            "Epoch [180/200], Loss: 0.00239\n",
            "Epoch [190/200], Loss: 0.00312\n",
            "Epoch [200/200], Loss: 0.00188\n",
            "--------------------\n",
            "stock number: 1868\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01495\n",
            "Epoch [20/200], Loss: 0.00433\n",
            "Epoch [30/200], Loss: 0.00248\n",
            "Epoch [40/200], Loss: 0.00083\n",
            "Epoch [50/200], Loss: 0.00080\n",
            "Epoch [60/200], Loss: 0.00257\n",
            "Epoch [70/200], Loss: 0.00051\n",
            "Epoch [80/200], Loss: 0.00135\n",
            "Epoch [90/200], Loss: 0.00055\n",
            "Epoch [100/200], Loss: 0.00027\n",
            "Epoch [110/200], Loss: 0.00030\n",
            "Epoch [120/200], Loss: 0.00033\n",
            "Epoch [130/200], Loss: 0.00034\n",
            "Epoch [140/200], Loss: 0.00215\n",
            "Epoch [150/200], Loss: 0.00019\n",
            "Epoch [160/200], Loss: 0.00022\n",
            "Epoch [170/200], Loss: 0.00019\n",
            "Epoch [180/200], Loss: 0.00175\n",
            "Epoch [190/200], Loss: 0.00022\n",
            "Epoch [200/200], Loss: 0.00017\n",
            "--------------------\n",
            "stock number: 1869\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00159\n",
            "Epoch [20/200], Loss: 0.00257\n",
            "Epoch [30/200], Loss: 0.00085\n",
            "Epoch [40/200], Loss: 0.00040\n",
            "Epoch [50/200], Loss: 0.00022\n",
            "Epoch [60/200], Loss: 0.00029\n",
            "Epoch [70/200], Loss: 0.00091\n",
            "Epoch [80/200], Loss: 0.00019\n",
            "Epoch [90/200], Loss: 0.00023\n",
            "Epoch [100/200], Loss: 0.00017\n",
            "Epoch [110/200], Loss: 0.00023\n",
            "Epoch [120/200], Loss: 0.00038\n",
            "Epoch [130/200], Loss: 0.00041\n",
            "Epoch [140/200], Loss: 0.00022\n",
            "Epoch [150/200], Loss: 0.00016\n",
            "Epoch [160/200], Loss: 0.00016\n",
            "Epoch [170/200], Loss: 0.00029\n",
            "Epoch [180/200], Loss: 0.00012\n",
            "Epoch [190/200], Loss: 0.00058\n",
            "Epoch [200/200], Loss: 0.00021\n",
            "--------------------\n",
            "stock number: 1870\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00482\n",
            "Epoch [20/200], Loss: 0.00510\n",
            "Epoch [30/200], Loss: 0.00107\n",
            "Epoch [40/200], Loss: 0.00159\n",
            "Epoch [50/200], Loss: 0.00405\n",
            "Epoch [60/200], Loss: 0.00172\n",
            "Epoch [70/200], Loss: 0.00078\n",
            "Epoch [80/200], Loss: 0.00064\n",
            "Epoch [90/200], Loss: 0.00272\n",
            "Epoch [100/200], Loss: 0.00089\n",
            "Epoch [110/200], Loss: 0.00127\n",
            "Epoch [120/200], Loss: 0.00107\n",
            "Epoch [130/200], Loss: 0.00058\n",
            "Epoch [140/200], Loss: 0.00074\n",
            "Epoch [150/200], Loss: 0.00080\n",
            "Epoch [160/200], Loss: 0.00079\n",
            "Epoch [170/200], Loss: 0.00185\n",
            "Epoch [180/200], Loss: 0.00057\n",
            "Epoch [190/200], Loss: 0.00187\n",
            "Epoch [200/200], Loss: 0.00059\n",
            "--------------------\n",
            "stock number: 1871\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.07573\n",
            "Epoch [20/200], Loss: 0.05559\n",
            "Epoch [30/200], Loss: 0.03091\n",
            "Epoch [40/200], Loss: 0.00982\n",
            "Epoch [50/200], Loss: 0.00686\n",
            "Epoch [60/200], Loss: 0.00303\n",
            "Epoch [70/200], Loss: 0.00184\n",
            "Epoch [80/200], Loss: 0.00235\n",
            "Epoch [90/200], Loss: 0.00204\n",
            "Epoch [100/200], Loss: 0.00176\n",
            "Epoch [110/200], Loss: 0.00143\n",
            "Epoch [120/200], Loss: 0.00204\n",
            "Epoch [130/200], Loss: 0.00188\n",
            "Epoch [140/200], Loss: 0.00155\n",
            "Epoch [150/200], Loss: 0.00085\n",
            "Epoch [160/200], Loss: 0.00247\n",
            "Epoch [170/200], Loss: 0.00113\n",
            "Epoch [180/200], Loss: 0.00121\n",
            "Epoch [190/200], Loss: 0.00130\n",
            "Epoch [200/200], Loss: 0.00078\n",
            "--------------------\n",
            "stock number: 1872\n",
            "--------------------\n",
            "pca n_components_ 4\n",
            "sequences shape:  torch.Size([469, 60, 4])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00830\n",
            "Epoch [20/200], Loss: 0.00490\n",
            "Epoch [30/200], Loss: 0.00165\n",
            "Epoch [40/200], Loss: 0.00184\n",
            "Epoch [50/200], Loss: 0.00174\n",
            "Epoch [60/200], Loss: 0.00258\n",
            "Epoch [70/200], Loss: 0.00204\n",
            "Epoch [80/200], Loss: 0.00093\n",
            "Epoch [90/200], Loss: 0.00081\n",
            "Epoch [100/200], Loss: 0.00690\n",
            "Epoch [110/200], Loss: 0.00063\n",
            "Epoch [120/200], Loss: 0.00052\n",
            "Epoch [130/200], Loss: 0.00205\n",
            "Epoch [140/200], Loss: 0.00067\n",
            "Epoch [150/200], Loss: 0.00042\n",
            "Epoch [160/200], Loss: 0.00048\n",
            "Epoch [170/200], Loss: 0.00094\n",
            "Epoch [180/200], Loss: 0.00062\n",
            "Epoch [190/200], Loss: 0.00065\n",
            "Epoch [200/200], Loss: 0.00180\n",
            "--------------------\n",
            "stock number: 1873\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.03090\n",
            "Epoch [20/200], Loss: 0.00944\n",
            "Epoch [30/200], Loss: 0.00505\n",
            "Epoch [40/200], Loss: 0.00409\n",
            "Epoch [50/200], Loss: 0.00258\n",
            "Epoch [60/200], Loss: 0.00209\n",
            "Epoch [70/200], Loss: 0.00188\n",
            "Epoch [80/200], Loss: 0.00569\n",
            "Epoch [90/200], Loss: 0.00158\n",
            "Epoch [100/200], Loss: 0.00262\n",
            "Epoch [110/200], Loss: 0.00282\n",
            "Epoch [120/200], Loss: 0.00201\n",
            "Epoch [130/200], Loss: 0.00183\n",
            "Epoch [140/200], Loss: 0.00197\n",
            "Epoch [150/200], Loss: 0.00643\n",
            "Epoch [160/200], Loss: 0.00097\n",
            "Epoch [170/200], Loss: 0.00089\n",
            "Epoch [180/200], Loss: 0.00182\n",
            "Epoch [190/200], Loss: 0.00153\n",
            "Epoch [200/200], Loss: 0.00294\n",
            "--------------------\n",
            "stock number: 1874\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.04290\n",
            "Epoch [20/200], Loss: 0.01019\n",
            "Epoch [30/200], Loss: 0.00212\n",
            "Epoch [40/200], Loss: 0.00242\n",
            "Epoch [50/200], Loss: 0.00242\n",
            "Epoch [60/200], Loss: 0.00311\n",
            "Epoch [70/200], Loss: 0.00152\n",
            "Epoch [80/200], Loss: 0.00140\n",
            "Epoch [90/200], Loss: 0.00090\n",
            "Epoch [100/200], Loss: 0.00077\n",
            "Epoch [110/200], Loss: 0.00066\n",
            "Epoch [120/200], Loss: 0.00137\n",
            "Epoch [130/200], Loss: 0.00114\n",
            "Epoch [140/200], Loss: 0.00242\n",
            "Epoch [150/200], Loss: 0.00134\n",
            "Epoch [160/200], Loss: 0.00136\n",
            "Epoch [170/200], Loss: 0.00127\n",
            "Epoch [180/200], Loss: 0.00079\n",
            "Epoch [190/200], Loss: 0.00256\n",
            "Epoch [200/200], Loss: 0.00120\n",
            "--------------------\n",
            "stock number: 1875\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00007\n",
            "Epoch [20/200], Loss: 0.00033\n",
            "Epoch [30/200], Loss: 0.00104\n",
            "Epoch [40/200], Loss: 0.00071\n",
            "Epoch [50/200], Loss: 0.00041\n",
            "Epoch [60/200], Loss: 0.00024\n",
            "Epoch [70/200], Loss: 0.00036\n",
            "Epoch [80/200], Loss: 0.00046\n",
            "Epoch [90/200], Loss: 0.00037\n",
            "Epoch [100/200], Loss: 0.00025\n",
            "Epoch [110/200], Loss: 0.00035\n",
            "Epoch [120/200], Loss: 0.00019\n",
            "Epoch [130/200], Loss: 0.00021\n",
            "Epoch [140/200], Loss: 0.00016\n",
            "Epoch [150/200], Loss: 0.00019\n",
            "Epoch [160/200], Loss: 0.00014\n",
            "Epoch [170/200], Loss: 0.00004\n",
            "Epoch [180/200], Loss: 0.00030\n",
            "Epoch [190/200], Loss: 0.00006\n",
            "Epoch [200/200], Loss: 0.00012\n",
            "--------------------\n",
            "stock number: 1876\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01318\n",
            "Epoch [20/200], Loss: 0.00043\n",
            "Epoch [30/200], Loss: 0.00043\n",
            "Epoch [40/200], Loss: 0.00008\n",
            "Epoch [50/200], Loss: 0.00017\n",
            "Epoch [60/200], Loss: 0.00008\n",
            "Epoch [70/200], Loss: 0.00008\n",
            "Epoch [80/200], Loss: 0.00008\n",
            "Epoch [90/200], Loss: 0.00008\n",
            "Epoch [100/200], Loss: 0.00012\n",
            "Epoch [110/200], Loss: 0.00008\n",
            "Epoch [120/200], Loss: 0.00008\n",
            "Epoch [130/200], Loss: 0.00008\n",
            "Epoch [140/200], Loss: 0.00008\n",
            "Epoch [150/200], Loss: 0.00008\n",
            "Epoch [160/200], Loss: 0.00008\n",
            "Epoch [170/200], Loss: 0.00007\n",
            "Epoch [180/200], Loss: 0.00009\n",
            "Epoch [190/200], Loss: 0.00008\n",
            "Epoch [200/200], Loss: 0.00008\n",
            "--------------------\n",
            "stock number: 1877\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01016\n",
            "Epoch [20/200], Loss: 0.00850\n",
            "Epoch [30/200], Loss: 0.01161\n",
            "Epoch [40/200], Loss: 0.00289\n",
            "Epoch [50/200], Loss: 0.00222\n",
            "Epoch [60/200], Loss: 0.00207\n",
            "Epoch [70/200], Loss: 0.00224\n",
            "Epoch [80/200], Loss: 0.00336\n",
            "Epoch [90/200], Loss: 0.00224\n",
            "Epoch [100/200], Loss: 0.00108\n",
            "Epoch [110/200], Loss: 0.00168\n",
            "Epoch [120/200], Loss: 0.00154\n",
            "Epoch [130/200], Loss: 0.00084\n",
            "Epoch [140/200], Loss: 0.00086\n",
            "Epoch [150/200], Loss: 0.00187\n",
            "Epoch [160/200], Loss: 0.00096\n",
            "Epoch [170/200], Loss: 0.00079\n",
            "Epoch [180/200], Loss: 0.00127\n",
            "Epoch [190/200], Loss: 0.00139\n",
            "Epoch [200/200], Loss: 0.00059\n",
            "--------------------\n",
            "stock number: 1878\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01215\n",
            "Epoch [20/200], Loss: 0.00320\n",
            "Epoch [30/200], Loss: 0.00297\n",
            "Epoch [40/200], Loss: 0.00126\n",
            "Epoch [50/200], Loss: 0.00067\n",
            "Epoch [60/200], Loss: 0.00072\n",
            "Epoch [70/200], Loss: 0.00062\n",
            "Epoch [80/200], Loss: 0.00733\n",
            "Epoch [90/200], Loss: 0.00075\n",
            "Epoch [100/200], Loss: 0.00103\n",
            "Epoch [110/200], Loss: 0.00139\n",
            "Epoch [120/200], Loss: 0.00166\n",
            "Epoch [130/200], Loss: 0.00040\n",
            "Epoch [140/200], Loss: 0.00107\n",
            "Epoch [150/200], Loss: 0.00092\n",
            "Epoch [160/200], Loss: 0.00053\n",
            "Epoch [170/200], Loss: 0.00130\n",
            "Epoch [180/200], Loss: 0.00035\n",
            "Epoch [190/200], Loss: 0.00023\n",
            "Epoch [200/200], Loss: 0.00048\n",
            "--------------------\n",
            "stock number: 1879\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00006\n",
            "Epoch [20/200], Loss: 0.00006\n",
            "Epoch [30/200], Loss: 0.00006\n",
            "Epoch [40/200], Loss: 0.00006\n",
            "Epoch [50/200], Loss: 0.00006\n",
            "Epoch [60/200], Loss: 0.00006\n",
            "Epoch [70/200], Loss: 0.00006\n",
            "Epoch [80/200], Loss: 0.00006\n",
            "Epoch [90/200], Loss: 0.00006\n",
            "Epoch [100/200], Loss: 0.00006\n",
            "Epoch [110/200], Loss: 0.00006\n",
            "Epoch [120/200], Loss: 0.00006\n",
            "Epoch [130/200], Loss: 0.00006\n",
            "Epoch [140/200], Loss: 0.00006\n",
            "Epoch [150/200], Loss: 0.00006\n",
            "Epoch [160/200], Loss: 0.00006\n",
            "Epoch [170/200], Loss: 0.00006\n",
            "Epoch [180/200], Loss: 0.00006\n",
            "Epoch [190/200], Loss: 0.00006\n",
            "Epoch [200/200], Loss: 0.00006\n",
            "--------------------\n",
            "stock number: 1880\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00117\n",
            "Epoch [20/200], Loss: 0.00039\n",
            "Epoch [30/200], Loss: 0.00038\n",
            "Epoch [40/200], Loss: 0.00037\n",
            "Epoch [50/200], Loss: 0.00036\n",
            "Epoch [60/200], Loss: 0.00034\n",
            "Epoch [70/200], Loss: 0.00026\n",
            "Epoch [80/200], Loss: 0.00031\n",
            "Epoch [90/200], Loss: 0.00023\n",
            "Epoch [100/200], Loss: 0.00032\n",
            "Epoch [110/200], Loss: 0.00025\n",
            "Epoch [120/200], Loss: 0.00031\n",
            "Epoch [130/200], Loss: 0.00037\n",
            "Epoch [140/200], Loss: 0.00036\n",
            "Epoch [150/200], Loss: 0.00036\n",
            "Epoch [160/200], Loss: 0.00038\n",
            "Epoch [170/200], Loss: 0.00038\n",
            "Epoch [180/200], Loss: 0.00023\n",
            "Epoch [190/200], Loss: 0.00037\n",
            "Epoch [200/200], Loss: 0.00031\n",
            "--------------------\n",
            "stock number: 1881\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02958\n",
            "Epoch [20/200], Loss: 0.00419\n",
            "Epoch [30/200], Loss: 0.01724\n",
            "Epoch [40/200], Loss: 0.00500\n",
            "Epoch [50/200], Loss: 0.00384\n",
            "Epoch [60/200], Loss: 0.00470\n",
            "Epoch [70/200], Loss: 0.00436\n",
            "Epoch [80/200], Loss: 0.00287\n",
            "Epoch [90/200], Loss: 0.01106\n",
            "Epoch [100/200], Loss: 0.00218\n",
            "Epoch [110/200], Loss: 0.00275\n",
            "Epoch [120/200], Loss: 0.00359\n",
            "Epoch [130/200], Loss: 0.00261\n",
            "Epoch [140/200], Loss: 0.00257\n",
            "Epoch [150/200], Loss: 0.00247\n",
            "Epoch [160/200], Loss: 0.00259\n",
            "Epoch [170/200], Loss: 0.00173\n",
            "Epoch [180/200], Loss: 0.00221\n",
            "Epoch [190/200], Loss: 0.00212\n",
            "Epoch [200/200], Loss: 0.00168\n",
            "--------------------\n",
            "stock number: 1882\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00117\n",
            "Epoch [20/200], Loss: 0.00048\n",
            "Epoch [30/200], Loss: 0.00073\n",
            "Epoch [40/200], Loss: 0.00034\n",
            "Epoch [50/200], Loss: 0.00020\n",
            "Epoch [60/200], Loss: 0.00080\n",
            "Epoch [70/200], Loss: 0.00049\n",
            "Epoch [80/200], Loss: 0.00045\n",
            "Epoch [90/200], Loss: 0.00050\n",
            "Epoch [100/200], Loss: 0.00034\n",
            "Epoch [110/200], Loss: 0.00041\n",
            "Epoch [120/200], Loss: 0.00079\n",
            "Epoch [130/200], Loss: 0.00039\n",
            "Epoch [140/200], Loss: 0.00034\n",
            "Epoch [150/200], Loss: 0.00056\n",
            "Epoch [160/200], Loss: 0.00040\n",
            "Epoch [170/200], Loss: 0.00029\n",
            "Epoch [180/200], Loss: 0.00019\n",
            "Epoch [190/200], Loss: 0.00027\n",
            "Epoch [200/200], Loss: 0.00014\n",
            "--------------------\n",
            "stock number: 1883\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.08768\n",
            "Epoch [20/200], Loss: 0.03513\n",
            "Epoch [30/200], Loss: 0.00361\n",
            "Epoch [40/200], Loss: 0.00213\n",
            "Epoch [50/200], Loss: 0.00134\n",
            "Epoch [60/200], Loss: 0.01442\n",
            "Epoch [70/200], Loss: 0.00112\n",
            "Epoch [80/200], Loss: 0.00212\n",
            "Epoch [90/200], Loss: 0.00075\n",
            "Epoch [100/200], Loss: 0.00065\n",
            "Epoch [110/200], Loss: 0.00303\n",
            "Epoch [120/200], Loss: 0.00062\n",
            "Epoch [130/200], Loss: 0.00064\n",
            "Epoch [140/200], Loss: 0.00169\n",
            "Epoch [150/200], Loss: 0.00059\n",
            "Epoch [160/200], Loss: 0.00075\n",
            "Epoch [170/200], Loss: 0.00086\n",
            "Epoch [180/200], Loss: 0.00151\n",
            "Epoch [190/200], Loss: 0.00046\n",
            "Epoch [200/200], Loss: 0.00079\n",
            "--------------------\n",
            "stock number: 1884\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01067\n",
            "Epoch [20/200], Loss: 0.00212\n",
            "Epoch [30/200], Loss: 0.00593\n",
            "Epoch [40/200], Loss: 0.00253\n",
            "Epoch [50/200], Loss: 0.00080\n",
            "Epoch [60/200], Loss: 0.00427\n",
            "Epoch [70/200], Loss: 0.00206\n",
            "Epoch [80/200], Loss: 0.00062\n",
            "Epoch [90/200], Loss: 0.00108\n",
            "Epoch [100/200], Loss: 0.00040\n",
            "Epoch [110/200], Loss: 0.00031\n",
            "Epoch [120/200], Loss: 0.00028\n",
            "Epoch [130/200], Loss: 0.00049\n",
            "Epoch [140/200], Loss: 0.00052\n",
            "Epoch [150/200], Loss: 0.00037\n",
            "Epoch [160/200], Loss: 0.00019\n",
            "Epoch [170/200], Loss: 0.00019\n",
            "Epoch [180/200], Loss: 0.00044\n",
            "Epoch [190/200], Loss: 0.00018\n",
            "Epoch [200/200], Loss: 0.00037\n",
            "--------------------\n",
            "stock number: 1885\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02990\n",
            "Epoch [20/200], Loss: 0.00488\n",
            "Epoch [30/200], Loss: 0.00294\n",
            "Epoch [40/200], Loss: 0.00384\n",
            "Epoch [50/200], Loss: 0.00142\n",
            "Epoch [60/200], Loss: 0.00259\n",
            "Epoch [70/200], Loss: 0.00146\n",
            "Epoch [80/200], Loss: 0.00081\n",
            "Epoch [90/200], Loss: 0.01139\n",
            "Epoch [100/200], Loss: 0.00112\n",
            "Epoch [110/200], Loss: 0.00320\n",
            "Epoch [120/200], Loss: 0.00067\n",
            "Epoch [130/200], Loss: 0.00065\n",
            "Epoch [140/200], Loss: 0.00753\n",
            "Epoch [150/200], Loss: 0.00054\n",
            "Epoch [160/200], Loss: 0.00374\n",
            "Epoch [170/200], Loss: 0.00055\n",
            "Epoch [180/200], Loss: 0.00096\n",
            "Epoch [190/200], Loss: 0.00071\n",
            "Epoch [200/200], Loss: 0.00071\n",
            "--------------------\n",
            "stock number: 1886\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02164\n",
            "Epoch [20/200], Loss: 0.01149\n",
            "Epoch [30/200], Loss: 0.00715\n",
            "Epoch [40/200], Loss: 0.00451\n",
            "Epoch [50/200], Loss: 0.00247\n",
            "Epoch [60/200], Loss: 0.00223\n",
            "Epoch [70/200], Loss: 0.00234\n",
            "Epoch [80/200], Loss: 0.00145\n",
            "Epoch [90/200], Loss: 0.00159\n",
            "Epoch [100/200], Loss: 0.00240\n",
            "Epoch [110/200], Loss: 0.00141\n",
            "Epoch [120/200], Loss: 0.00113\n",
            "Epoch [130/200], Loss: 0.00170\n",
            "Epoch [140/200], Loss: 0.00448\n",
            "Epoch [150/200], Loss: 0.00313\n",
            "Epoch [160/200], Loss: 0.00079\n",
            "Epoch [170/200], Loss: 0.00132\n",
            "Epoch [180/200], Loss: 0.00097\n",
            "Epoch [190/200], Loss: 0.00090\n",
            "Epoch [200/200], Loss: 0.00186\n",
            "--------------------\n",
            "stock number: 1887\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01580\n",
            "Epoch [20/200], Loss: 0.00326\n",
            "Epoch [30/200], Loss: 0.00224\n",
            "Epoch [40/200], Loss: 0.00113\n",
            "Epoch [50/200], Loss: 0.00202\n",
            "Epoch [60/200], Loss: 0.00102\n",
            "Epoch [70/200], Loss: 0.00113\n",
            "Epoch [80/200], Loss: 0.00097\n",
            "Epoch [90/200], Loss: 0.00114\n",
            "Epoch [100/200], Loss: 0.00081\n",
            "Epoch [110/200], Loss: 0.00108\n",
            "Epoch [120/200], Loss: 0.00146\n",
            "Epoch [130/200], Loss: 0.00089\n",
            "Epoch [140/200], Loss: 0.00074\n",
            "Epoch [150/200], Loss: 0.00080\n",
            "Epoch [160/200], Loss: 0.00095\n",
            "Epoch [170/200], Loss: 0.00089\n",
            "Epoch [180/200], Loss: 0.00087\n",
            "Epoch [190/200], Loss: 0.00071\n",
            "Epoch [200/200], Loss: 0.00093\n",
            "--------------------\n",
            "stock number: 1888\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01013\n",
            "Epoch [20/200], Loss: 0.00678\n",
            "Epoch [30/200], Loss: 0.00328\n",
            "Epoch [40/200], Loss: 0.00055\n",
            "Epoch [50/200], Loss: 0.00096\n",
            "Epoch [60/200], Loss: 0.00039\n",
            "Epoch [70/200], Loss: 0.00099\n",
            "Epoch [80/200], Loss: 0.00022\n",
            "Epoch [90/200], Loss: 0.00023\n",
            "Epoch [100/200], Loss: 0.00041\n",
            "Epoch [110/200], Loss: 0.00184\n",
            "Epoch [120/200], Loss: 0.00011\n",
            "Epoch [130/200], Loss: 0.00072\n",
            "Epoch [140/200], Loss: 0.00047\n",
            "Epoch [150/200], Loss: 0.00010\n",
            "Epoch [160/200], Loss: 0.00010\n",
            "Epoch [170/200], Loss: 0.00014\n",
            "Epoch [180/200], Loss: 0.00011\n",
            "Epoch [190/200], Loss: 0.00019\n",
            "Epoch [200/200], Loss: 0.00008\n",
            "--------------------\n",
            "stock number: 1889\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00799\n",
            "Epoch [20/200], Loss: 0.00518\n",
            "Epoch [30/200], Loss: 0.00513\n",
            "Epoch [40/200], Loss: 0.00212\n",
            "Epoch [50/200], Loss: 0.00235\n",
            "Epoch [60/200], Loss: 0.00074\n",
            "Epoch [70/200], Loss: 0.00063\n",
            "Epoch [80/200], Loss: 0.00031\n",
            "Epoch [90/200], Loss: 0.00354\n",
            "Epoch [100/200], Loss: 0.00038\n",
            "Epoch [110/200], Loss: 0.00027\n",
            "Epoch [120/200], Loss: 0.00365\n",
            "Epoch [130/200], Loss: 0.00026\n",
            "Epoch [140/200], Loss: 0.00050\n",
            "Epoch [150/200], Loss: 0.00067\n",
            "Epoch [160/200], Loss: 0.00029\n",
            "Epoch [170/200], Loss: 0.00054\n",
            "Epoch [180/200], Loss: 0.00076\n",
            "Epoch [190/200], Loss: 0.00024\n",
            "Epoch [200/200], Loss: 0.00013\n",
            "--------------------\n",
            "stock number: 1890\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00660\n",
            "Epoch [20/200], Loss: 0.01066\n",
            "Epoch [30/200], Loss: 0.00407\n",
            "Epoch [40/200], Loss: 0.00443\n",
            "Epoch [50/200], Loss: 0.00266\n",
            "Epoch [60/200], Loss: 0.00246\n",
            "Epoch [70/200], Loss: 0.00305\n",
            "Epoch [80/200], Loss: 0.00182\n",
            "Epoch [90/200], Loss: 0.00286\n",
            "Epoch [100/200], Loss: 0.00153\n",
            "Epoch [110/200], Loss: 0.00145\n",
            "Epoch [120/200], Loss: 0.00189\n",
            "Epoch [130/200], Loss: 0.00124\n",
            "Epoch [140/200], Loss: 0.00156\n",
            "Epoch [150/200], Loss: 0.00168\n",
            "Epoch [160/200], Loss: 0.00135\n",
            "Epoch [170/200], Loss: 0.00103\n",
            "Epoch [180/200], Loss: 0.00100\n",
            "Epoch [190/200], Loss: 0.00084\n",
            "Epoch [200/200], Loss: 0.00140\n",
            "--------------------\n",
            "stock number: 1891\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00324\n",
            "Epoch [20/200], Loss: 0.00203\n",
            "Epoch [30/200], Loss: 0.00098\n",
            "Epoch [40/200], Loss: 0.00213\n",
            "Epoch [50/200], Loss: 0.00046\n",
            "Epoch [60/200], Loss: 0.00056\n",
            "Epoch [70/200], Loss: 0.00031\n",
            "Epoch [80/200], Loss: 0.00062\n",
            "Epoch [90/200], Loss: 0.00032\n",
            "Epoch [100/200], Loss: 0.00019\n",
            "Epoch [110/200], Loss: 0.00044\n",
            "Epoch [120/200], Loss: 0.00033\n",
            "Epoch [130/200], Loss: 0.00055\n",
            "Epoch [140/200], Loss: 0.00019\n",
            "Epoch [150/200], Loss: 0.00018\n",
            "Epoch [160/200], Loss: 0.00034\n",
            "Epoch [170/200], Loss: 0.00019\n",
            "Epoch [180/200], Loss: 0.00022\n",
            "Epoch [190/200], Loss: 0.00045\n",
            "Epoch [200/200], Loss: 0.00015\n",
            "--------------------\n",
            "stock number: 1892\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02621\n",
            "Epoch [20/200], Loss: 0.00525\n",
            "Epoch [30/200], Loss: 0.00429\n",
            "Epoch [40/200], Loss: 0.00128\n",
            "Epoch [50/200], Loss: 0.00174\n",
            "Epoch [60/200], Loss: 0.00135\n",
            "Epoch [70/200], Loss: 0.00088\n",
            "Epoch [80/200], Loss: 0.00108\n",
            "Epoch [90/200], Loss: 0.00078\n",
            "Epoch [100/200], Loss: 0.00070\n",
            "Epoch [110/200], Loss: 0.00064\n",
            "Epoch [120/200], Loss: 0.00059\n",
            "Epoch [130/200], Loss: 0.00089\n",
            "Epoch [140/200], Loss: 0.00097\n",
            "Epoch [150/200], Loss: 0.00072\n",
            "Epoch [160/200], Loss: 0.00072\n",
            "Epoch [170/200], Loss: 0.00070\n",
            "Epoch [180/200], Loss: 0.00038\n",
            "Epoch [190/200], Loss: 0.00055\n",
            "Epoch [200/200], Loss: 0.00031\n",
            "--------------------\n",
            "stock number: 1893\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01226\n",
            "Epoch [20/200], Loss: 0.00518\n",
            "Epoch [30/200], Loss: 0.01055\n",
            "Epoch [40/200], Loss: 0.00282\n",
            "Epoch [50/200], Loss: 0.00522\n",
            "Epoch [60/200], Loss: 0.00284\n",
            "Epoch [70/200], Loss: 0.01327\n",
            "Epoch [80/200], Loss: 0.00161\n",
            "Epoch [90/200], Loss: 0.00520\n",
            "Epoch [100/200], Loss: 0.00168\n",
            "Epoch [110/200], Loss: 0.00363\n",
            "Epoch [120/200], Loss: 0.00219\n",
            "Epoch [130/200], Loss: 0.00100\n",
            "Epoch [140/200], Loss: 0.00117\n",
            "Epoch [150/200], Loss: 0.00207\n",
            "Epoch [160/200], Loss: 0.00186\n",
            "Epoch [170/200], Loss: 0.00167\n",
            "Epoch [180/200], Loss: 0.00304\n",
            "Epoch [190/200], Loss: 0.00097\n",
            "Epoch [200/200], Loss: 0.00108\n",
            "--------------------\n",
            "stock number: 1894\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00331\n",
            "Epoch [20/200], Loss: 0.00146\n",
            "Epoch [30/200], Loss: 0.00062\n",
            "Epoch [40/200], Loss: 0.00095\n",
            "Epoch [50/200], Loss: 0.00062\n",
            "Epoch [60/200], Loss: 0.00084\n",
            "Epoch [70/200], Loss: 0.00068\n",
            "Epoch [80/200], Loss: 0.00068\n",
            "Epoch [90/200], Loss: 0.00050\n",
            "Epoch [100/200], Loss: 0.00029\n",
            "Epoch [110/200], Loss: 0.00036\n",
            "Epoch [120/200], Loss: 0.00019\n",
            "Epoch [130/200], Loss: 0.00020\n",
            "Epoch [140/200], Loss: 0.00038\n",
            "Epoch [150/200], Loss: 0.00016\n",
            "Epoch [160/200], Loss: 0.00015\n",
            "Epoch [170/200], Loss: 0.00015\n",
            "Epoch [180/200], Loss: 0.00023\n",
            "Epoch [190/200], Loss: 0.00031\n",
            "Epoch [200/200], Loss: 0.00015\n",
            "--------------------\n",
            "stock number: 1895\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.25528\n",
            "Epoch [20/200], Loss: 0.14099\n",
            "Epoch [30/200], Loss: 0.04377\n",
            "Epoch [40/200], Loss: 0.02689\n",
            "Epoch [50/200], Loss: 0.01814\n",
            "Epoch [60/200], Loss: 0.01258\n",
            "Epoch [70/200], Loss: 0.00613\n",
            "Epoch [80/200], Loss: 0.01412\n",
            "Epoch [90/200], Loss: 0.00359\n",
            "Epoch [100/200], Loss: 0.00278\n",
            "Epoch [110/200], Loss: 0.00655\n",
            "Epoch [120/200], Loss: 0.00249\n",
            "Epoch [130/200], Loss: 0.00235\n",
            "Epoch [140/200], Loss: 0.00236\n",
            "Epoch [150/200], Loss: 0.00287\n",
            "Epoch [160/200], Loss: 0.00224\n",
            "Epoch [170/200], Loss: 0.00221\n",
            "Epoch [180/200], Loss: 0.00315\n",
            "Epoch [190/200], Loss: 0.00332\n",
            "Epoch [200/200], Loss: 0.00171\n",
            "--------------------\n",
            "stock number: 1896\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.08923\n",
            "Epoch [20/200], Loss: 0.03066\n",
            "Epoch [30/200], Loss: 0.01230\n",
            "Epoch [40/200], Loss: 0.00964\n",
            "Epoch [50/200], Loss: 0.00489\n",
            "Epoch [60/200], Loss: 0.00424\n",
            "Epoch [70/200], Loss: 0.00253\n",
            "Epoch [80/200], Loss: 0.00234\n",
            "Epoch [90/200], Loss: 0.00211\n",
            "Epoch [100/200], Loss: 0.00433\n",
            "Epoch [110/200], Loss: 0.00156\n",
            "Epoch [120/200], Loss: 0.00352\n",
            "Epoch [130/200], Loss: 0.00200\n",
            "Epoch [140/200], Loss: 0.00203\n",
            "Epoch [150/200], Loss: 0.00160\n",
            "Epoch [160/200], Loss: 0.00188\n",
            "Epoch [170/200], Loss: 0.00134\n",
            "Epoch [180/200], Loss: 0.00155\n",
            "Epoch [190/200], Loss: 0.00260\n",
            "Epoch [200/200], Loss: 0.00173\n",
            "--------------------\n",
            "stock number: 1897\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.04732\n",
            "Epoch [20/200], Loss: 0.01476\n",
            "Epoch [30/200], Loss: 0.00458\n",
            "Epoch [40/200], Loss: 0.00324\n",
            "Epoch [50/200], Loss: 0.00242\n",
            "Epoch [60/200], Loss: 0.00194\n",
            "Epoch [70/200], Loss: 0.00290\n",
            "Epoch [80/200], Loss: 0.00266\n",
            "Epoch [90/200], Loss: 0.00257\n",
            "Epoch [100/200], Loss: 0.00363\n",
            "Epoch [110/200], Loss: 0.00203\n",
            "Epoch [120/200], Loss: 0.00238\n",
            "Epoch [130/200], Loss: 0.00349\n",
            "Epoch [140/200], Loss: 0.00290\n",
            "Epoch [150/200], Loss: 0.00315\n",
            "Epoch [160/200], Loss: 0.00289\n",
            "Epoch [170/200], Loss: 0.00236\n",
            "Epoch [180/200], Loss: 0.00158\n",
            "Epoch [190/200], Loss: 0.00140\n",
            "Epoch [200/200], Loss: 0.00203\n",
            "--------------------\n",
            "stock number: 1898\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00340\n",
            "Epoch [20/200], Loss: 0.01063\n",
            "Epoch [30/200], Loss: 0.00251\n",
            "Epoch [40/200], Loss: 0.00131\n",
            "Epoch [50/200], Loss: 0.00542\n",
            "Epoch [60/200], Loss: 0.00047\n",
            "Epoch [70/200], Loss: 0.00101\n",
            "Epoch [80/200], Loss: 0.00040\n",
            "Epoch [90/200], Loss: 0.00045\n",
            "Epoch [100/200], Loss: 0.00034\n",
            "Epoch [110/200], Loss: 0.00076\n",
            "Epoch [120/200], Loss: 0.00035\n",
            "Epoch [130/200], Loss: 0.00083\n",
            "Epoch [140/200], Loss: 0.00030\n",
            "Epoch [150/200], Loss: 0.00038\n",
            "Epoch [160/200], Loss: 0.00072\n",
            "Epoch [170/200], Loss: 0.00033\n",
            "Epoch [180/200], Loss: 0.00041\n",
            "Epoch [190/200], Loss: 0.00024\n",
            "Epoch [200/200], Loss: 0.00066\n",
            "--------------------\n",
            "stock number: 1899\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01086\n",
            "Epoch [20/200], Loss: 0.00331\n",
            "Epoch [30/200], Loss: 0.00133\n",
            "Epoch [40/200], Loss: 0.00115\n",
            "Epoch [50/200], Loss: 0.00080\n",
            "Epoch [60/200], Loss: 0.00078\n",
            "Epoch [70/200], Loss: 0.00091\n",
            "Epoch [80/200], Loss: 0.00085\n",
            "Epoch [90/200], Loss: 0.00059\n",
            "Epoch [100/200], Loss: 0.00128\n",
            "Epoch [110/200], Loss: 0.00038\n",
            "Epoch [120/200], Loss: 0.00044\n",
            "Epoch [130/200], Loss: 0.00071\n",
            "Epoch [140/200], Loss: 0.00029\n",
            "Epoch [150/200], Loss: 0.00029\n",
            "Epoch [160/200], Loss: 0.00024\n",
            "Epoch [170/200], Loss: 0.00033\n",
            "Epoch [180/200], Loss: 0.00018\n",
            "Epoch [190/200], Loss: 0.00023\n",
            "Epoch [200/200], Loss: 0.00020\n",
            "--------------------\n",
            "stock number: 1900\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01258\n",
            "Epoch [20/200], Loss: 0.00873\n",
            "Epoch [30/200], Loss: 0.00665\n",
            "Epoch [40/200], Loss: 0.00695\n",
            "Epoch [50/200], Loss: 0.00547\n",
            "Epoch [60/200], Loss: 0.00231\n",
            "Epoch [70/200], Loss: 0.00242\n",
            "Epoch [80/200], Loss: 0.01032\n",
            "Epoch [90/200], Loss: 0.00289\n",
            "Epoch [100/200], Loss: 0.00150\n",
            "Epoch [110/200], Loss: 0.00303\n",
            "Epoch [120/200], Loss: 0.00185\n",
            "Epoch [130/200], Loss: 0.00268\n",
            "Epoch [140/200], Loss: 0.00399\n",
            "Epoch [150/200], Loss: 0.00093\n",
            "Epoch [160/200], Loss: 0.00178\n",
            "Epoch [170/200], Loss: 0.00270\n",
            "Epoch [180/200], Loss: 0.00093\n",
            "Epoch [190/200], Loss: 0.00464\n",
            "Epoch [200/200], Loss: 0.00114\n",
            "--------------------\n",
            "stock number: 1901\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02709\n",
            "Epoch [20/200], Loss: 0.01647\n",
            "Epoch [30/200], Loss: 0.00385\n",
            "Epoch [40/200], Loss: 0.00239\n",
            "Epoch [50/200], Loss: 0.00390\n",
            "Epoch [60/200], Loss: 0.00181\n",
            "Epoch [70/200], Loss: 0.00190\n",
            "Epoch [80/200], Loss: 0.00379\n",
            "Epoch [90/200], Loss: 0.00125\n",
            "Epoch [100/200], Loss: 0.00457\n",
            "Epoch [110/200], Loss: 0.00126\n",
            "Epoch [120/200], Loss: 0.00074\n",
            "Epoch [130/200], Loss: 0.00086\n",
            "Epoch [140/200], Loss: 0.00107\n",
            "Epoch [150/200], Loss: 0.00076\n",
            "Epoch [160/200], Loss: 0.00115\n",
            "Epoch [170/200], Loss: 0.00069\n",
            "Epoch [180/200], Loss: 0.00095\n",
            "Epoch [190/200], Loss: 0.00059\n",
            "Epoch [200/200], Loss: 0.00057\n",
            "--------------------\n",
            "stock number: 1902\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00389\n",
            "Epoch [20/200], Loss: 0.00300\n",
            "Epoch [30/200], Loss: 0.00229\n",
            "Epoch [40/200], Loss: 0.00256\n",
            "Epoch [50/200], Loss: 0.00087\n",
            "Epoch [60/200], Loss: 0.00114\n",
            "Epoch [70/200], Loss: 0.00394\n",
            "Epoch [80/200], Loss: 0.00285\n",
            "Epoch [90/200], Loss: 0.00212\n",
            "Epoch [100/200], Loss: 0.00199\n",
            "Epoch [110/200], Loss: 0.00096\n",
            "Epoch [120/200], Loss: 0.00052\n",
            "Epoch [130/200], Loss: 0.00035\n",
            "Epoch [140/200], Loss: 0.00102\n",
            "Epoch [150/200], Loss: 0.00086\n",
            "Epoch [160/200], Loss: 0.00036\n",
            "Epoch [170/200], Loss: 0.00024\n",
            "Epoch [180/200], Loss: 0.00065\n",
            "Epoch [190/200], Loss: 0.00012\n",
            "Epoch [200/200], Loss: 0.00020\n",
            "--------------------\n",
            "stock number: 1903\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01248\n",
            "Epoch [20/200], Loss: 0.00941\n",
            "Epoch [30/200], Loss: 0.00236\n",
            "Epoch [40/200], Loss: 0.00123\n",
            "Epoch [50/200], Loss: 0.00170\n",
            "Epoch [60/200], Loss: 0.00068\n",
            "Epoch [70/200], Loss: 0.00064\n",
            "Epoch [80/200], Loss: 0.00170\n",
            "Epoch [90/200], Loss: 0.00122\n",
            "Epoch [100/200], Loss: 0.00070\n",
            "Epoch [110/200], Loss: 0.00064\n",
            "Epoch [120/200], Loss: 0.00145\n",
            "Epoch [130/200], Loss: 0.00131\n",
            "Epoch [140/200], Loss: 0.00082\n",
            "Epoch [150/200], Loss: 0.00073\n",
            "Epoch [160/200], Loss: 0.00098\n",
            "Epoch [170/200], Loss: 0.00060\n",
            "Epoch [180/200], Loss: 0.00076\n",
            "Epoch [190/200], Loss: 0.00123\n",
            "Epoch [200/200], Loss: 0.00129\n",
            "--------------------\n",
            "stock number: 1904\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.04578\n",
            "Epoch [20/200], Loss: 0.00927\n",
            "Epoch [30/200], Loss: 0.00149\n",
            "Epoch [40/200], Loss: 0.00067\n",
            "Epoch [50/200], Loss: 0.00085\n",
            "Epoch [60/200], Loss: 0.00050\n",
            "Epoch [70/200], Loss: 0.00089\n",
            "Epoch [80/200], Loss: 0.00060\n",
            "Epoch [90/200], Loss: 0.00072\n",
            "Epoch [100/200], Loss: 0.00055\n",
            "Epoch [110/200], Loss: 0.00042\n",
            "Epoch [120/200], Loss: 0.00123\n",
            "Epoch [130/200], Loss: 0.00076\n",
            "Epoch [140/200], Loss: 0.00034\n",
            "Epoch [150/200], Loss: 0.00048\n",
            "Epoch [160/200], Loss: 0.00124\n",
            "Epoch [170/200], Loss: 0.00035\n",
            "Epoch [180/200], Loss: 0.00027\n",
            "Epoch [190/200], Loss: 0.00038\n",
            "Epoch [200/200], Loss: 0.00046\n",
            "--------------------\n",
            "stock number: 1905\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.03068\n",
            "Epoch [20/200], Loss: 0.00930\n",
            "Epoch [30/200], Loss: 0.00434\n",
            "Epoch [40/200], Loss: 0.00470\n",
            "Epoch [50/200], Loss: 0.00190\n",
            "Epoch [60/200], Loss: 0.00182\n",
            "Epoch [70/200], Loss: 0.00096\n",
            "Epoch [80/200], Loss: 0.00061\n",
            "Epoch [90/200], Loss: 0.00060\n",
            "Epoch [100/200], Loss: 0.00058\n",
            "Epoch [110/200], Loss: 0.00068\n",
            "Epoch [120/200], Loss: 0.00048\n",
            "Epoch [130/200], Loss: 0.00051\n",
            "Epoch [140/200], Loss: 0.00075\n",
            "Epoch [150/200], Loss: 0.00048\n",
            "Epoch [160/200], Loss: 0.00048\n",
            "Epoch [170/200], Loss: 0.00053\n",
            "Epoch [180/200], Loss: 0.00041\n",
            "Epoch [190/200], Loss: 0.00044\n",
            "Epoch [200/200], Loss: 0.00040\n",
            "--------------------\n",
            "stock number: 1906\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00435\n",
            "Epoch [20/200], Loss: 0.00168\n",
            "Epoch [30/200], Loss: 0.00098\n",
            "Epoch [40/200], Loss: 0.00172\n",
            "Epoch [50/200], Loss: 0.00334\n",
            "Epoch [60/200], Loss: 0.00036\n",
            "Epoch [70/200], Loss: 0.00033\n",
            "Epoch [80/200], Loss: 0.00226\n",
            "Epoch [90/200], Loss: 0.00049\n",
            "Epoch [100/200], Loss: 0.00027\n",
            "Epoch [110/200], Loss: 0.00033\n",
            "Epoch [120/200], Loss: 0.00093\n",
            "Epoch [130/200], Loss: 0.00073\n",
            "Epoch [140/200], Loss: 0.00049\n",
            "Epoch [150/200], Loss: 0.00012\n",
            "Epoch [160/200], Loss: 0.00018\n",
            "Epoch [170/200], Loss: 0.00021\n",
            "Epoch [180/200], Loss: 0.00019\n",
            "Epoch [190/200], Loss: 0.00017\n",
            "Epoch [200/200], Loss: 0.00012\n",
            "--------------------\n",
            "stock number: 1907\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.03865\n",
            "Epoch [20/200], Loss: 0.01067\n",
            "Epoch [30/200], Loss: 0.00417\n",
            "Epoch [40/200], Loss: 0.00322\n",
            "Epoch [50/200], Loss: 0.01124\n",
            "Epoch [60/200], Loss: 0.00151\n",
            "Epoch [70/200], Loss: 0.00195\n",
            "Epoch [80/200], Loss: 0.00644\n",
            "Epoch [90/200], Loss: 0.00097\n",
            "Epoch [100/200], Loss: 0.00324\n",
            "Epoch [110/200], Loss: 0.00256\n",
            "Epoch [120/200], Loss: 0.00125\n",
            "Epoch [130/200], Loss: 0.00146\n",
            "Epoch [140/200], Loss: 0.00217\n",
            "Epoch [150/200], Loss: 0.00160\n",
            "Epoch [160/200], Loss: 0.00159\n",
            "Epoch [170/200], Loss: 0.00149\n",
            "Epoch [180/200], Loss: 0.00230\n",
            "Epoch [190/200], Loss: 0.00330\n",
            "Epoch [200/200], Loss: 0.00137\n",
            "--------------------\n",
            "stock number: 1908\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01052\n",
            "Epoch [20/200], Loss: 0.00253\n",
            "Epoch [30/200], Loss: 0.00254\n",
            "Epoch [40/200], Loss: 0.00280\n",
            "Epoch [50/200], Loss: 0.00144\n",
            "Epoch [60/200], Loss: 0.00174\n",
            "Epoch [70/200], Loss: 0.00159\n",
            "Epoch [80/200], Loss: 0.00121\n",
            "Epoch [90/200], Loss: 0.00092\n",
            "Epoch [100/200], Loss: 0.00076\n",
            "Epoch [110/200], Loss: 0.00064\n",
            "Epoch [120/200], Loss: 0.00075\n",
            "Epoch [130/200], Loss: 0.00071\n",
            "Epoch [140/200], Loss: 0.00063\n",
            "Epoch [150/200], Loss: 0.00063\n",
            "Epoch [160/200], Loss: 0.00047\n",
            "Epoch [170/200], Loss: 0.00056\n",
            "Epoch [180/200], Loss: 0.00047\n",
            "Epoch [190/200], Loss: 0.00082\n",
            "Epoch [200/200], Loss: 0.00039\n",
            "--------------------\n",
            "stock number: 1909\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00223\n",
            "Epoch [20/200], Loss: 0.00487\n",
            "Epoch [30/200], Loss: 0.00231\n",
            "Epoch [40/200], Loss: 0.00088\n",
            "Epoch [50/200], Loss: 0.00215\n",
            "Epoch [60/200], Loss: 0.00053\n",
            "Epoch [70/200], Loss: 0.00073\n",
            "Epoch [80/200], Loss: 0.00078\n",
            "Epoch [90/200], Loss: 0.00039\n",
            "Epoch [100/200], Loss: 0.00055\n",
            "Epoch [110/200], Loss: 0.00045\n",
            "Epoch [120/200], Loss: 0.00026\n",
            "Epoch [130/200], Loss: 0.00024\n",
            "Epoch [140/200], Loss: 0.00033\n",
            "Epoch [150/200], Loss: 0.00118\n",
            "Epoch [160/200], Loss: 0.00026\n",
            "Epoch [170/200], Loss: 0.00019\n",
            "Epoch [180/200], Loss: 0.00018\n",
            "Epoch [190/200], Loss: 0.00022\n",
            "Epoch [200/200], Loss: 0.00024\n",
            "--------------------\n",
            "stock number: 1910\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.16693\n",
            "Epoch [20/200], Loss: 0.03756\n",
            "Epoch [30/200], Loss: 0.03964\n",
            "Epoch [40/200], Loss: 0.01725\n",
            "Epoch [50/200], Loss: 0.06193\n",
            "Epoch [60/200], Loss: 0.02370\n",
            "Epoch [70/200], Loss: 0.00480\n",
            "Epoch [80/200], Loss: 0.00478\n",
            "Epoch [90/200], Loss: 0.00660\n",
            "Epoch [100/200], Loss: 0.00224\n",
            "Epoch [110/200], Loss: 0.00287\n",
            "Epoch [120/200], Loss: 0.00236\n",
            "Epoch [130/200], Loss: 0.00169\n",
            "Epoch [140/200], Loss: 0.00518\n",
            "Epoch [150/200], Loss: 0.00232\n",
            "Epoch [160/200], Loss: 0.00157\n",
            "Epoch [170/200], Loss: 0.00196\n",
            "Epoch [180/200], Loss: 0.00183\n",
            "Epoch [190/200], Loss: 0.00141\n",
            "Epoch [200/200], Loss: 0.00164\n",
            "--------------------\n",
            "stock number: 1911\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02630\n",
            "Epoch [20/200], Loss: 0.02773\n",
            "Epoch [30/200], Loss: 0.00192\n",
            "Epoch [40/200], Loss: 0.00143\n",
            "Epoch [50/200], Loss: 0.00318\n",
            "Epoch [60/200], Loss: 0.00202\n",
            "Epoch [70/200], Loss: 0.00077\n",
            "Epoch [80/200], Loss: 0.00072\n",
            "Epoch [90/200], Loss: 0.00105\n",
            "Epoch [100/200], Loss: 0.00052\n",
            "Epoch [110/200], Loss: 0.00062\n",
            "Epoch [120/200], Loss: 0.00052\n",
            "Epoch [130/200], Loss: 0.00047\n",
            "Epoch [140/200], Loss: 0.00047\n",
            "Epoch [150/200], Loss: 0.00066\n",
            "Epoch [160/200], Loss: 0.00057\n",
            "Epoch [170/200], Loss: 0.00040\n",
            "Epoch [180/200], Loss: 0.00043\n",
            "Epoch [190/200], Loss: 0.00036\n",
            "Epoch [200/200], Loss: 0.00042\n",
            "--------------------\n",
            "stock number: 1912\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02232\n",
            "Epoch [20/200], Loss: 0.00839\n",
            "Epoch [30/200], Loss: 0.00232\n",
            "Epoch [40/200], Loss: 0.00231\n",
            "Epoch [50/200], Loss: 0.00268\n",
            "Epoch [60/200], Loss: 0.00162\n",
            "Epoch [70/200], Loss: 0.00143\n",
            "Epoch [80/200], Loss: 0.00307\n",
            "Epoch [90/200], Loss: 0.00118\n",
            "Epoch [100/200], Loss: 0.00107\n",
            "Epoch [110/200], Loss: 0.00095\n",
            "Epoch [120/200], Loss: 0.00133\n",
            "Epoch [130/200], Loss: 0.00093\n",
            "Epoch [140/200], Loss: 0.00109\n",
            "Epoch [150/200], Loss: 0.00066\n",
            "Epoch [160/200], Loss: 0.00081\n",
            "Epoch [170/200], Loss: 0.00105\n",
            "Epoch [180/200], Loss: 0.00052\n",
            "Epoch [190/200], Loss: 0.00051\n",
            "Epoch [200/200], Loss: 0.00051\n",
            "--------------------\n",
            "stock number: 1913\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01889\n",
            "Epoch [20/200], Loss: 0.01586\n",
            "Epoch [30/200], Loss: 0.00656\n",
            "Epoch [40/200], Loss: 0.00188\n",
            "Epoch [50/200], Loss: 0.00057\n",
            "Epoch [60/200], Loss: 0.00204\n",
            "Epoch [70/200], Loss: 0.00078\n",
            "Epoch [80/200], Loss: 0.00054\n",
            "Epoch [90/200], Loss: 0.00054\n",
            "Epoch [100/200], Loss: 0.00086\n",
            "Epoch [110/200], Loss: 0.00019\n",
            "Epoch [120/200], Loss: 0.00043\n",
            "Epoch [130/200], Loss: 0.00070\n",
            "Epoch [140/200], Loss: 0.00017\n",
            "Epoch [150/200], Loss: 0.00166\n",
            "Epoch [160/200], Loss: 0.00034\n",
            "Epoch [170/200], Loss: 0.00104\n",
            "Epoch [180/200], Loss: 0.00051\n",
            "Epoch [190/200], Loss: 0.00023\n",
            "Epoch [200/200], Loss: 0.00074\n",
            "--------------------\n",
            "stock number: 1914\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00180\n",
            "Epoch [20/200], Loss: 0.00359\n",
            "Epoch [30/200], Loss: 0.00134\n",
            "Epoch [40/200], Loss: 0.00133\n",
            "Epoch [50/200], Loss: 0.00080\n",
            "Epoch [60/200], Loss: 0.00061\n",
            "Epoch [70/200], Loss: 0.00040\n",
            "Epoch [80/200], Loss: 0.00039\n",
            "Epoch [90/200], Loss: 0.00065\n",
            "Epoch [100/200], Loss: 0.00049\n",
            "Epoch [110/200], Loss: 0.00023\n",
            "Epoch [120/200], Loss: 0.00045\n",
            "Epoch [130/200], Loss: 0.00031\n",
            "Epoch [140/200], Loss: 0.00021\n",
            "Epoch [150/200], Loss: 0.00025\n",
            "Epoch [160/200], Loss: 0.00023\n",
            "Epoch [170/200], Loss: 0.00026\n",
            "Epoch [180/200], Loss: 0.00018\n",
            "Epoch [190/200], Loss: 0.00012\n",
            "Epoch [200/200], Loss: 0.00024\n",
            "--------------------\n",
            "stock number: 1915\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01290\n",
            "Epoch [20/200], Loss: 0.01921\n",
            "Epoch [30/200], Loss: 0.00663\n",
            "Epoch [40/200], Loss: 0.00604\n",
            "Epoch [50/200], Loss: 0.00550\n",
            "Epoch [60/200], Loss: 0.00505\n",
            "Epoch [70/200], Loss: 0.00516\n",
            "Epoch [80/200], Loss: 0.00410\n",
            "Epoch [90/200], Loss: 0.00477\n",
            "Epoch [100/200], Loss: 0.00463\n",
            "Epoch [110/200], Loss: 0.00448\n",
            "Epoch [120/200], Loss: 0.00482\n",
            "Epoch [130/200], Loss: 0.00435\n",
            "Epoch [140/200], Loss: 0.00398\n",
            "Epoch [150/200], Loss: 0.00376\n",
            "Epoch [160/200], Loss: 0.00366\n",
            "Epoch [170/200], Loss: 0.00399\n",
            "Epoch [180/200], Loss: 0.00434\n",
            "Epoch [190/200], Loss: 0.00263\n",
            "Epoch [200/200], Loss: 0.00276\n",
            "--------------------\n",
            "stock number: 1916\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.03427\n",
            "Epoch [20/200], Loss: 0.00893\n",
            "Epoch [30/200], Loss: 0.00701\n",
            "Epoch [40/200], Loss: 0.00664\n",
            "Epoch [50/200], Loss: 0.00612\n",
            "Epoch [60/200], Loss: 0.00579\n",
            "Epoch [70/200], Loss: 0.00402\n",
            "Epoch [80/200], Loss: 0.00301\n",
            "Epoch [90/200], Loss: 0.00262\n",
            "Epoch [100/200], Loss: 0.00533\n",
            "Epoch [110/200], Loss: 0.00278\n",
            "Epoch [120/200], Loss: 0.00236\n",
            "Epoch [130/200], Loss: 0.00207\n",
            "Epoch [140/200], Loss: 0.00161\n",
            "Epoch [150/200], Loss: 0.00216\n",
            "Epoch [160/200], Loss: 0.00358\n",
            "Epoch [170/200], Loss: 0.00152\n",
            "Epoch [180/200], Loss: 0.00123\n",
            "Epoch [190/200], Loss: 0.00174\n",
            "Epoch [200/200], Loss: 0.00159\n",
            "--------------------\n",
            "stock number: 1917\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00779\n",
            "Epoch [20/200], Loss: 0.01063\n",
            "Epoch [30/200], Loss: 0.00456\n",
            "Epoch [40/200], Loss: 0.00799\n",
            "Epoch [50/200], Loss: 0.00530\n",
            "Epoch [60/200], Loss: 0.00920\n",
            "Epoch [70/200], Loss: 0.00901\n",
            "Epoch [80/200], Loss: 0.00476\n",
            "Epoch [90/200], Loss: 0.00318\n",
            "Epoch [100/200], Loss: 0.00406\n",
            "Epoch [110/200], Loss: 0.00263\n",
            "Epoch [120/200], Loss: 0.00267\n",
            "Epoch [130/200], Loss: 0.00260\n",
            "Epoch [140/200], Loss: 0.00187\n",
            "Epoch [150/200], Loss: 0.00120\n",
            "Epoch [160/200], Loss: 0.00089\n",
            "Epoch [170/200], Loss: 0.00167\n",
            "Epoch [180/200], Loss: 0.00397\n",
            "Epoch [190/200], Loss: 0.00118\n",
            "Epoch [200/200], Loss: 0.00055\n",
            "--------------------\n",
            "stock number: 1918\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02201\n",
            "Epoch [20/200], Loss: 0.00947\n",
            "Epoch [30/200], Loss: 0.00983\n",
            "Epoch [40/200], Loss: 0.00570\n",
            "Epoch [50/200], Loss: 0.00445\n",
            "Epoch [60/200], Loss: 0.00363\n",
            "Epoch [70/200], Loss: 0.00254\n",
            "Epoch [80/200], Loss: 0.00529\n",
            "Epoch [90/200], Loss: 0.00198\n",
            "Epoch [100/200], Loss: 0.00151\n",
            "Epoch [110/200], Loss: 0.00341\n",
            "Epoch [120/200], Loss: 0.00460\n",
            "Epoch [130/200], Loss: 0.00142\n",
            "Epoch [140/200], Loss: 0.00209\n",
            "Epoch [150/200], Loss: 0.00188\n",
            "Epoch [160/200], Loss: 0.00569\n",
            "Epoch [170/200], Loss: 0.00168\n",
            "Epoch [180/200], Loss: 0.00109\n",
            "Epoch [190/200], Loss: 0.00173\n",
            "Epoch [200/200], Loss: 0.00199\n",
            "--------------------\n",
            "stock number: 1919\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.03916\n",
            "Epoch [20/200], Loss: 0.02750\n",
            "Epoch [30/200], Loss: 0.00110\n",
            "Epoch [40/200], Loss: 0.00089\n",
            "Epoch [50/200], Loss: 0.00101\n",
            "Epoch [60/200], Loss: 0.00130\n",
            "Epoch [70/200], Loss: 0.00132\n",
            "Epoch [80/200], Loss: 0.00171\n",
            "Epoch [90/200], Loss: 0.00055\n",
            "Epoch [100/200], Loss: 0.00120\n",
            "Epoch [110/200], Loss: 0.00076\n",
            "Epoch [120/200], Loss: 0.00104\n",
            "Epoch [130/200], Loss: 0.00075\n",
            "Epoch [140/200], Loss: 0.00114\n",
            "Epoch [150/200], Loss: 0.00064\n",
            "Epoch [160/200], Loss: 0.00377\n",
            "Epoch [170/200], Loss: 0.00079\n",
            "Epoch [180/200], Loss: 0.00076\n",
            "Epoch [190/200], Loss: 0.00062\n",
            "Epoch [200/200], Loss: 0.00124\n",
            "--------------------\n",
            "stock number: 1920\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00232\n",
            "Epoch [20/200], Loss: 0.00259\n",
            "Epoch [30/200], Loss: 0.00119\n",
            "Epoch [40/200], Loss: 0.00401\n",
            "Epoch [50/200], Loss: 0.00140\n",
            "Epoch [60/200], Loss: 0.00160\n",
            "Epoch [70/200], Loss: 0.00204\n",
            "Epoch [80/200], Loss: 0.00063\n",
            "Epoch [90/200], Loss: 0.00101\n",
            "Epoch [100/200], Loss: 0.00044\n",
            "Epoch [110/200], Loss: 0.00047\n",
            "Epoch [120/200], Loss: 0.00053\n",
            "Epoch [130/200], Loss: 0.00073\n",
            "Epoch [140/200], Loss: 0.00079\n",
            "Epoch [150/200], Loss: 0.00046\n",
            "Epoch [160/200], Loss: 0.00076\n",
            "Epoch [170/200], Loss: 0.00049\n",
            "Epoch [180/200], Loss: 0.00033\n",
            "Epoch [190/200], Loss: 0.00037\n",
            "Epoch [200/200], Loss: 0.00059\n",
            "--------------------\n",
            "stock number: 1921\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00926\n",
            "Epoch [20/200], Loss: 0.00448\n",
            "Epoch [30/200], Loss: 0.00006\n",
            "Epoch [40/200], Loss: 0.00002\n",
            "Epoch [50/200], Loss: 0.00040\n",
            "Epoch [60/200], Loss: 0.00002\n",
            "Epoch [70/200], Loss: 0.00002\n",
            "Epoch [80/200], Loss: 0.00001\n",
            "Epoch [90/200], Loss: 0.00001\n",
            "Epoch [100/200], Loss: 0.00001\n",
            "Epoch [110/200], Loss: 0.00005\n",
            "Epoch [120/200], Loss: 0.00001\n",
            "Epoch [130/200], Loss: 0.00003\n",
            "Epoch [140/200], Loss: 0.00001\n",
            "Epoch [150/200], Loss: 0.00001\n",
            "Epoch [160/200], Loss: 0.00002\n",
            "Epoch [170/200], Loss: 0.00004\n",
            "Epoch [180/200], Loss: 0.00001\n",
            "Epoch [190/200], Loss: 0.00001\n",
            "Epoch [200/200], Loss: 0.00002\n",
            "--------------------\n",
            "stock number: 1922\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01163\n",
            "Epoch [20/200], Loss: 0.00187\n",
            "Epoch [30/200], Loss: 0.00085\n",
            "Epoch [40/200], Loss: 0.00090\n",
            "Epoch [50/200], Loss: 0.00265\n",
            "Epoch [60/200], Loss: 0.00104\n",
            "Epoch [70/200], Loss: 0.00067\n",
            "Epoch [80/200], Loss: 0.00077\n",
            "Epoch [90/200], Loss: 0.00269\n",
            "Epoch [100/200], Loss: 0.00070\n",
            "Epoch [110/200], Loss: 0.00125\n",
            "Epoch [120/200], Loss: 0.00040\n",
            "Epoch [130/200], Loss: 0.00040\n",
            "Epoch [140/200], Loss: 0.00090\n",
            "Epoch [150/200], Loss: 0.00064\n",
            "Epoch [160/200], Loss: 0.00054\n",
            "Epoch [170/200], Loss: 0.00028\n",
            "Epoch [180/200], Loss: 0.00030\n",
            "Epoch [190/200], Loss: 0.00043\n",
            "Epoch [200/200], Loss: 0.00092\n",
            "--------------------\n",
            "stock number: 1923\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00811\n",
            "Epoch [20/200], Loss: 0.00337\n",
            "Epoch [30/200], Loss: 0.01271\n",
            "Epoch [40/200], Loss: 0.01082\n",
            "Epoch [50/200], Loss: 0.00778\n",
            "Epoch [60/200], Loss: 0.00253\n",
            "Epoch [70/200], Loss: 0.00328\n",
            "Epoch [80/200], Loss: 0.01258\n",
            "Epoch [90/200], Loss: 0.00161\n",
            "Epoch [100/200], Loss: 0.00960\n",
            "Epoch [110/200], Loss: 0.00340\n",
            "Epoch [120/200], Loss: 0.00365\n",
            "Epoch [130/200], Loss: 0.00264\n",
            "Epoch [140/200], Loss: 0.00126\n",
            "Epoch [150/200], Loss: 0.00368\n",
            "Epoch [160/200], Loss: 0.00400\n",
            "Epoch [170/200], Loss: 0.00478\n",
            "Epoch [180/200], Loss: 0.00144\n",
            "Epoch [190/200], Loss: 0.00269\n",
            "Epoch [200/200], Loss: 0.00134\n",
            "--------------------\n",
            "stock number: 1924\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.04139\n",
            "Epoch [20/200], Loss: 0.04139\n",
            "Epoch [30/200], Loss: 0.04139\n",
            "Epoch [40/200], Loss: 0.04139\n",
            "Epoch [50/200], Loss: 0.04139\n",
            "Epoch [60/200], Loss: 0.04139\n",
            "Epoch [70/200], Loss: 0.04139\n",
            "Epoch [80/200], Loss: 0.04139\n",
            "Epoch [90/200], Loss: 0.04139\n",
            "Epoch [100/200], Loss: 0.04139\n",
            "Epoch [110/200], Loss: 0.04139\n",
            "Epoch [120/200], Loss: 0.04139\n",
            "Epoch [130/200], Loss: 0.04139\n",
            "Epoch [140/200], Loss: 0.04139\n",
            "Epoch [150/200], Loss: 0.04139\n",
            "Epoch [160/200], Loss: 0.04139\n",
            "Epoch [170/200], Loss: 0.04139\n",
            "Epoch [180/200], Loss: 0.04139\n",
            "Epoch [190/200], Loss: 0.04139\n",
            "Epoch [200/200], Loss: 0.04139\n",
            "--------------------\n",
            "stock number: 1925\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00815\n",
            "Epoch [20/200], Loss: 0.01459\n",
            "Epoch [30/200], Loss: 0.00334\n",
            "Epoch [40/200], Loss: 0.00223\n",
            "Epoch [50/200], Loss: 0.00049\n",
            "Epoch [60/200], Loss: 0.00098\n",
            "Epoch [70/200], Loss: 0.00042\n",
            "Epoch [80/200], Loss: 0.00038\n",
            "Epoch [90/200], Loss: 0.00045\n",
            "Epoch [100/200], Loss: 0.00030\n",
            "Epoch [110/200], Loss: 0.00099\n",
            "Epoch [120/200], Loss: 0.00049\n",
            "Epoch [130/200], Loss: 0.00020\n",
            "Epoch [140/200], Loss: 0.00017\n",
            "Epoch [150/200], Loss: 0.00043\n",
            "Epoch [160/200], Loss: 0.00027\n",
            "Epoch [170/200], Loss: 0.00064\n",
            "Epoch [180/200], Loss: 0.00012\n",
            "Epoch [190/200], Loss: 0.00037\n",
            "Epoch [200/200], Loss: 0.00013\n",
            "--------------------\n",
            "stock number: 1926\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.08926\n",
            "Epoch [20/200], Loss: 0.03705\n",
            "Epoch [30/200], Loss: 0.02554\n",
            "Epoch [40/200], Loss: 0.02374\n",
            "Epoch [50/200], Loss: 0.02366\n",
            "Epoch [60/200], Loss: 0.02392\n",
            "Epoch [70/200], Loss: 0.02337\n",
            "Epoch [80/200], Loss: 0.02382\n",
            "Epoch [90/200], Loss: 0.01333\n",
            "Epoch [100/200], Loss: 0.00234\n",
            "Epoch [110/200], Loss: 0.00120\n",
            "Epoch [120/200], Loss: 0.00114\n",
            "Epoch [130/200], Loss: 0.00258\n",
            "Epoch [140/200], Loss: 0.00116\n",
            "Epoch [150/200], Loss: 0.00218\n",
            "Epoch [160/200], Loss: 0.00146\n",
            "Epoch [170/200], Loss: 0.00158\n",
            "Epoch [180/200], Loss: 0.00151\n",
            "Epoch [190/200], Loss: 0.00277\n",
            "Epoch [200/200], Loss: 0.00271\n",
            "--------------------\n",
            "stock number: 1927\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01134\n",
            "Epoch [20/200], Loss: 0.00612\n",
            "Epoch [30/200], Loss: 0.00425\n",
            "Epoch [40/200], Loss: 0.00136\n",
            "Epoch [50/200], Loss: 0.00051\n",
            "Epoch [60/200], Loss: 0.00133\n",
            "Epoch [70/200], Loss: 0.00038\n",
            "Epoch [80/200], Loss: 0.00426\n",
            "Epoch [90/200], Loss: 0.00039\n",
            "Epoch [100/200], Loss: 0.00199\n",
            "Epoch [110/200], Loss: 0.00046\n",
            "Epoch [120/200], Loss: 0.00042\n",
            "Epoch [130/200], Loss: 0.00106\n",
            "Epoch [140/200], Loss: 0.00035\n",
            "Epoch [150/200], Loss: 0.00033\n",
            "Epoch [160/200], Loss: 0.00218\n",
            "Epoch [170/200], Loss: 0.00054\n",
            "Epoch [180/200], Loss: 0.00016\n",
            "Epoch [190/200], Loss: 0.00034\n",
            "Epoch [200/200], Loss: 0.00258\n",
            "--------------------\n",
            "stock number: 1928\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00196\n",
            "Epoch [20/200], Loss: 0.00150\n",
            "Epoch [30/200], Loss: 0.00131\n",
            "Epoch [40/200], Loss: 0.00097\n",
            "Epoch [50/200], Loss: 0.00077\n",
            "Epoch [60/200], Loss: 0.00059\n",
            "Epoch [70/200], Loss: 0.00075\n",
            "Epoch [80/200], Loss: 0.00151\n",
            "Epoch [90/200], Loss: 0.00102\n",
            "Epoch [100/200], Loss: 0.00098\n",
            "Epoch [110/200], Loss: 0.00070\n",
            "Epoch [120/200], Loss: 0.00043\n",
            "Epoch [130/200], Loss: 0.00037\n",
            "Epoch [140/200], Loss: 0.00127\n",
            "Epoch [150/200], Loss: 0.00027\n",
            "Epoch [160/200], Loss: 0.00049\n",
            "Epoch [170/200], Loss: 0.00026\n",
            "Epoch [180/200], Loss: 0.00047\n",
            "Epoch [190/200], Loss: 0.00016\n",
            "Epoch [200/200], Loss: 0.00028\n",
            "--------------------\n",
            "stock number: 1929\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01474\n",
            "Epoch [20/200], Loss: 0.01116\n",
            "Epoch [30/200], Loss: 0.00661\n",
            "Epoch [40/200], Loss: 0.00849\n",
            "Epoch [50/200], Loss: 0.01600\n",
            "Epoch [60/200], Loss: 0.00644\n",
            "Epoch [70/200], Loss: 0.00559\n",
            "Epoch [80/200], Loss: 0.00894\n",
            "Epoch [90/200], Loss: 0.00540\n",
            "Epoch [100/200], Loss: 0.00478\n",
            "Epoch [110/200], Loss: 0.00618\n",
            "Epoch [120/200], Loss: 0.00308\n",
            "Epoch [130/200], Loss: 0.00476\n",
            "Epoch [140/200], Loss: 0.00319\n",
            "Epoch [150/200], Loss: 0.00467\n",
            "Epoch [160/200], Loss: 0.00249\n",
            "Epoch [170/200], Loss: 0.00454\n",
            "Epoch [180/200], Loss: 0.00536\n",
            "Epoch [190/200], Loss: 0.00286\n",
            "Epoch [200/200], Loss: 0.00248\n",
            "--------------------\n",
            "stock number: 1930\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.13883\n",
            "Epoch [20/200], Loss: 0.01121\n",
            "Epoch [30/200], Loss: 0.01139\n",
            "Epoch [40/200], Loss: 0.01364\n",
            "Epoch [50/200], Loss: 0.00913\n",
            "Epoch [60/200], Loss: 0.00691\n",
            "Epoch [70/200], Loss: 0.00527\n",
            "Epoch [80/200], Loss: 0.00897\n",
            "Epoch [90/200], Loss: 0.00421\n",
            "Epoch [100/200], Loss: 0.00393\n",
            "Epoch [110/200], Loss: 0.00383\n",
            "Epoch [120/200], Loss: 0.00461\n",
            "Epoch [130/200], Loss: 0.00533\n",
            "Epoch [140/200], Loss: 0.00529\n",
            "Epoch [150/200], Loss: 0.00302\n",
            "Epoch [160/200], Loss: 0.00318\n",
            "Epoch [170/200], Loss: 0.00883\n",
            "Epoch [180/200], Loss: 0.00295\n",
            "Epoch [190/200], Loss: 0.00271\n",
            "Epoch [200/200], Loss: 0.00312\n",
            "--------------------\n",
            "stock number: 1931\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.03543\n",
            "Epoch [20/200], Loss: 0.02057\n",
            "Epoch [30/200], Loss: 0.00467\n",
            "Epoch [40/200], Loss: 0.00574\n",
            "Epoch [50/200], Loss: 0.00348\n",
            "Epoch [60/200], Loss: 0.00521\n",
            "Epoch [70/200], Loss: 0.00387\n",
            "Epoch [80/200], Loss: 0.00320\n",
            "Epoch [90/200], Loss: 0.01787\n",
            "Epoch [100/200], Loss: 0.00295\n",
            "Epoch [110/200], Loss: 0.00254\n",
            "Epoch [120/200], Loss: 0.00311\n",
            "Epoch [130/200], Loss: 0.00269\n",
            "Epoch [140/200], Loss: 0.00264\n",
            "Epoch [150/200], Loss: 0.03162\n",
            "Epoch [160/200], Loss: 0.00201\n",
            "Epoch [170/200], Loss: 0.00249\n",
            "Epoch [180/200], Loss: 0.00232\n",
            "Epoch [190/200], Loss: 0.00209\n",
            "Epoch [200/200], Loss: 0.00138\n",
            "--------------------\n",
            "stock number: 1932\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01305\n",
            "Epoch [20/200], Loss: 0.00702\n",
            "Epoch [30/200], Loss: 0.00378\n",
            "Epoch [40/200], Loss: 0.00190\n",
            "Epoch [50/200], Loss: 0.00061\n",
            "Epoch [60/200], Loss: 0.00082\n",
            "Epoch [70/200], Loss: 0.00055\n",
            "Epoch [80/200], Loss: 0.00129\n",
            "Epoch [90/200], Loss: 0.00040\n",
            "Epoch [100/200], Loss: 0.00036\n",
            "Epoch [110/200], Loss: 0.00031\n",
            "Epoch [120/200], Loss: 0.00058\n",
            "Epoch [130/200], Loss: 0.00030\n",
            "Epoch [140/200], Loss: 0.00043\n",
            "Epoch [150/200], Loss: 0.00048\n",
            "Epoch [160/200], Loss: 0.00125\n",
            "Epoch [170/200], Loss: 0.00029\n",
            "Epoch [180/200], Loss: 0.00026\n",
            "Epoch [190/200], Loss: 0.00047\n",
            "Epoch [200/200], Loss: 0.00054\n",
            "--------------------\n",
            "stock number: 1933\n",
            "--------------------\n",
            "pca n_components_ 4\n",
            "sequences shape:  torch.Size([469, 60, 4])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.05317\n",
            "Epoch [20/200], Loss: 0.02417\n",
            "Epoch [30/200], Loss: 0.01338\n",
            "Epoch [40/200], Loss: 0.00833\n",
            "Epoch [50/200], Loss: 0.00581\n",
            "Epoch [60/200], Loss: 0.00434\n",
            "Epoch [70/200], Loss: 0.00352\n",
            "Epoch [80/200], Loss: 0.00387\n",
            "Epoch [90/200], Loss: 0.00371\n",
            "Epoch [100/200], Loss: 0.00264\n",
            "Epoch [110/200], Loss: 0.00236\n",
            "Epoch [120/200], Loss: 0.00544\n",
            "Epoch [130/200], Loss: 0.00182\n",
            "Epoch [140/200], Loss: 0.00286\n",
            "Epoch [150/200], Loss: 0.00196\n",
            "Epoch [160/200], Loss: 0.00525\n",
            "Epoch [170/200], Loss: 0.00130\n",
            "Epoch [180/200], Loss: 0.00218\n",
            "Epoch [190/200], Loss: 0.00153\n",
            "Epoch [200/200], Loss: 0.00226\n",
            "--------------------\n",
            "stock number: 1934\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00322\n",
            "Epoch [20/200], Loss: 0.00336\n",
            "Epoch [30/200], Loss: 0.00244\n",
            "Epoch [40/200], Loss: 0.00220\n",
            "Epoch [50/200], Loss: 0.00196\n",
            "Epoch [60/200], Loss: 0.00122\n",
            "Epoch [70/200], Loss: 0.00105\n",
            "Epoch [80/200], Loss: 0.00072\n",
            "Epoch [90/200], Loss: 0.00092\n",
            "Epoch [100/200], Loss: 0.00061\n",
            "Epoch [110/200], Loss: 0.00091\n",
            "Epoch [120/200], Loss: 0.00072\n",
            "Epoch [130/200], Loss: 0.00022\n",
            "Epoch [140/200], Loss: 0.00060\n",
            "Epoch [150/200], Loss: 0.00089\n",
            "Epoch [160/200], Loss: 0.00060\n",
            "Epoch [170/200], Loss: 0.00040\n",
            "Epoch [180/200], Loss: 0.00020\n",
            "Epoch [190/200], Loss: 0.00026\n",
            "Epoch [200/200], Loss: 0.00066\n",
            "--------------------\n",
            "stock number: 1935\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00642\n",
            "Epoch [20/200], Loss: 0.00532\n",
            "Epoch [30/200], Loss: 0.00494\n",
            "Epoch [40/200], Loss: 0.00453\n",
            "Epoch [50/200], Loss: 0.00480\n",
            "Epoch [60/200], Loss: 0.00338\n",
            "Epoch [70/200], Loss: 0.00262\n",
            "Epoch [80/200], Loss: 0.00193\n",
            "Epoch [90/200], Loss: 0.00177\n",
            "Epoch [100/200], Loss: 0.00131\n",
            "Epoch [110/200], Loss: 0.00280\n",
            "Epoch [120/200], Loss: 0.00121\n",
            "Epoch [130/200], Loss: 0.00082\n",
            "Epoch [140/200], Loss: 0.00136\n",
            "Epoch [150/200], Loss: 0.00107\n",
            "Epoch [160/200], Loss: 0.00218\n",
            "Epoch [170/200], Loss: 0.00169\n",
            "Epoch [180/200], Loss: 0.00110\n",
            "Epoch [190/200], Loss: 0.00060\n",
            "Epoch [200/200], Loss: 0.00118\n",
            "--------------------\n",
            "stock number: 1936\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01334\n",
            "Epoch [20/200], Loss: 0.01164\n",
            "Epoch [30/200], Loss: 0.00988\n",
            "Epoch [40/200], Loss: 0.00201\n",
            "Epoch [50/200], Loss: 0.00175\n",
            "Epoch [60/200], Loss: 0.00122\n",
            "Epoch [70/200], Loss: 0.00125\n",
            "Epoch [80/200], Loss: 0.00090\n",
            "Epoch [90/200], Loss: 0.00082\n",
            "Epoch [100/200], Loss: 0.00064\n",
            "Epoch [110/200], Loss: 0.00057\n",
            "Epoch [120/200], Loss: 0.00050\n",
            "Epoch [130/200], Loss: 0.00049\n",
            "Epoch [140/200], Loss: 0.00047\n",
            "Epoch [150/200], Loss: 0.00040\n",
            "Epoch [160/200], Loss: 0.00035\n",
            "Epoch [170/200], Loss: 0.00035\n",
            "Epoch [180/200], Loss: 0.00034\n",
            "Epoch [190/200], Loss: 0.00036\n",
            "Epoch [200/200], Loss: 0.00045\n",
            "--------------------\n",
            "stock number: 1937\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00409\n",
            "Epoch [20/200], Loss: 0.00326\n",
            "Epoch [30/200], Loss: 0.00162\n",
            "Epoch [40/200], Loss: 0.00136\n",
            "Epoch [50/200], Loss: 0.00127\n",
            "Epoch [60/200], Loss: 0.00056\n",
            "Epoch [70/200], Loss: 0.00236\n",
            "Epoch [80/200], Loss: 0.00048\n",
            "Epoch [90/200], Loss: 0.00066\n",
            "Epoch [100/200], Loss: 0.00040\n",
            "Epoch [110/200], Loss: 0.00060\n",
            "Epoch [120/200], Loss: 0.00042\n",
            "Epoch [130/200], Loss: 0.00032\n",
            "Epoch [140/200], Loss: 0.00028\n",
            "Epoch [150/200], Loss: 0.00028\n",
            "Epoch [160/200], Loss: 0.00026\n",
            "Epoch [170/200], Loss: 0.00024\n",
            "Epoch [180/200], Loss: 0.00055\n",
            "Epoch [190/200], Loss: 0.00021\n",
            "Epoch [200/200], Loss: 0.00019\n",
            "--------------------\n",
            "stock number: 1938\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.08950\n",
            "Epoch [20/200], Loss: 0.05577\n",
            "Epoch [30/200], Loss: 0.00285\n",
            "Epoch [40/200], Loss: 0.00360\n",
            "Epoch [50/200], Loss: 0.00258\n",
            "Epoch [60/200], Loss: 0.00139\n",
            "Epoch [70/200], Loss: 0.00141\n",
            "Epoch [80/200], Loss: 0.00118\n",
            "Epoch [90/200], Loss: 0.00093\n",
            "Epoch [100/200], Loss: 0.00653\n",
            "Epoch [110/200], Loss: 0.00080\n",
            "Epoch [120/200], Loss: 0.00061\n",
            "Epoch [130/200], Loss: 0.00089\n",
            "Epoch [140/200], Loss: 0.00122\n",
            "Epoch [150/200], Loss: 0.00085\n",
            "Epoch [160/200], Loss: 0.00113\n",
            "Epoch [170/200], Loss: 0.00153\n",
            "Epoch [180/200], Loss: 0.00040\n",
            "Epoch [190/200], Loss: 0.00065\n",
            "Epoch [200/200], Loss: 0.00062\n",
            "--------------------\n",
            "stock number: 1939\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.03101\n",
            "Epoch [20/200], Loss: 0.03101\n",
            "Epoch [30/200], Loss: 0.03101\n",
            "Epoch [40/200], Loss: 0.03101\n",
            "Epoch [50/200], Loss: 0.03101\n",
            "Epoch [60/200], Loss: 0.03101\n",
            "Epoch [70/200], Loss: 0.03101\n",
            "Epoch [80/200], Loss: 0.03101\n",
            "Epoch [90/200], Loss: 0.03101\n",
            "Epoch [100/200], Loss: 0.03101\n",
            "Epoch [110/200], Loss: 0.03101\n",
            "Epoch [120/200], Loss: 0.03101\n",
            "Epoch [130/200], Loss: 0.03101\n",
            "Epoch [140/200], Loss: 0.03101\n",
            "Epoch [150/200], Loss: 0.03101\n",
            "Epoch [160/200], Loss: 0.03101\n",
            "Epoch [170/200], Loss: 0.03101\n",
            "Epoch [180/200], Loss: 0.03101\n",
            "Epoch [190/200], Loss: 0.03101\n",
            "Epoch [200/200], Loss: 0.02831\n",
            "--------------------\n",
            "stock number: 1940\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00683\n",
            "Epoch [20/200], Loss: 0.00770\n",
            "Epoch [30/200], Loss: 0.00587\n",
            "Epoch [40/200], Loss: 0.00795\n",
            "Epoch [50/200], Loss: 0.00424\n",
            "Epoch [60/200], Loss: 0.00513\n",
            "Epoch [70/200], Loss: 0.00340\n",
            "Epoch [80/200], Loss: 0.00292\n",
            "Epoch [90/200], Loss: 0.00357\n",
            "Epoch [100/200], Loss: 0.00251\n",
            "Epoch [110/200], Loss: 0.00585\n",
            "Epoch [120/200], Loss: 0.00246\n",
            "Epoch [130/200], Loss: 0.00335\n",
            "Epoch [140/200], Loss: 0.00278\n",
            "Epoch [150/200], Loss: 0.00239\n",
            "Epoch [160/200], Loss: 0.00150\n",
            "Epoch [170/200], Loss: 0.00342\n",
            "Epoch [180/200], Loss: 0.00110\n",
            "Epoch [190/200], Loss: 0.00097\n",
            "Epoch [200/200], Loss: 0.00125\n",
            "--------------------\n",
            "stock number: 1941\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00211\n",
            "Epoch [20/200], Loss: 0.00211\n",
            "Epoch [30/200], Loss: 0.00211\n",
            "Epoch [40/200], Loss: 0.00210\n",
            "Epoch [50/200], Loss: 0.00205\n",
            "Epoch [60/200], Loss: 0.00211\n",
            "Epoch [70/200], Loss: 0.00208\n",
            "Epoch [80/200], Loss: 0.00186\n",
            "Epoch [90/200], Loss: 0.00166\n",
            "Epoch [100/200], Loss: 0.00196\n",
            "Epoch [110/200], Loss: 0.00194\n",
            "Epoch [120/200], Loss: 0.00192\n",
            "Epoch [130/200], Loss: 0.00195\n",
            "Epoch [140/200], Loss: 0.00193\n",
            "Epoch [150/200], Loss: 0.00203\n",
            "Epoch [160/200], Loss: 0.00199\n",
            "Epoch [170/200], Loss: 0.00175\n",
            "Epoch [180/200], Loss: 0.00210\n",
            "Epoch [190/200], Loss: 0.00180\n",
            "Epoch [200/200], Loss: 0.00189\n",
            "--------------------\n",
            "stock number: 1942\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.05555\n",
            "Epoch [20/200], Loss: 0.02811\n",
            "Epoch [30/200], Loss: 0.00755\n",
            "Epoch [40/200], Loss: 0.00539\n",
            "Epoch [50/200], Loss: 0.00256\n",
            "Epoch [60/200], Loss: 0.00264\n",
            "Epoch [70/200], Loss: 0.00097\n",
            "Epoch [80/200], Loss: 0.00341\n",
            "Epoch [90/200], Loss: 0.00178\n",
            "Epoch [100/200], Loss: 0.00086\n",
            "Epoch [110/200], Loss: 0.00301\n",
            "Epoch [120/200], Loss: 0.00110\n",
            "Epoch [130/200], Loss: 0.00336\n",
            "Epoch [140/200], Loss: 0.00086\n",
            "Epoch [150/200], Loss: 0.00142\n",
            "Epoch [160/200], Loss: 0.00375\n",
            "Epoch [170/200], Loss: 0.00066\n",
            "Epoch [180/200], Loss: 0.00066\n",
            "Epoch [190/200], Loss: 0.00164\n",
            "Epoch [200/200], Loss: 0.00297\n",
            "--------------------\n",
            "stock number: 1943\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00619\n",
            "Epoch [20/200], Loss: 0.00505\n",
            "Epoch [30/200], Loss: 0.00438\n",
            "Epoch [40/200], Loss: 0.00436\n",
            "Epoch [50/200], Loss: 0.00138\n",
            "Epoch [60/200], Loss: 0.00150\n",
            "Epoch [70/200], Loss: 0.00386\n",
            "Epoch [80/200], Loss: 0.00101\n",
            "Epoch [90/200], Loss: 0.00173\n",
            "Epoch [100/200], Loss: 0.00204\n",
            "Epoch [110/200], Loss: 0.00150\n",
            "Epoch [120/200], Loss: 0.00111\n",
            "Epoch [130/200], Loss: 0.00113\n",
            "Epoch [140/200], Loss: 0.00127\n",
            "Epoch [150/200], Loss: 0.00211\n",
            "Epoch [160/200], Loss: 0.00111\n",
            "Epoch [170/200], Loss: 0.00102\n",
            "Epoch [180/200], Loss: 0.00141\n",
            "Epoch [190/200], Loss: 0.00107\n",
            "Epoch [200/200], Loss: 0.00069\n",
            "--------------------\n",
            "stock number: 1944\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.07169\n",
            "Epoch [20/200], Loss: 0.01493\n",
            "Epoch [30/200], Loss: 0.00262\n",
            "Epoch [40/200], Loss: 0.00516\n",
            "Epoch [50/200], Loss: 0.00541\n",
            "Epoch [60/200], Loss: 0.00088\n",
            "Epoch [70/200], Loss: 0.00274\n",
            "Epoch [80/200], Loss: 0.00073\n",
            "Epoch [90/200], Loss: 0.00076\n",
            "Epoch [100/200], Loss: 0.00120\n",
            "Epoch [110/200], Loss: 0.00071\n",
            "Epoch [120/200], Loss: 0.00149\n",
            "Epoch [130/200], Loss: 0.00048\n",
            "Epoch [140/200], Loss: 0.00046\n",
            "Epoch [150/200], Loss: 0.00118\n",
            "Epoch [160/200], Loss: 0.00047\n",
            "Epoch [170/200], Loss: 0.00053\n",
            "Epoch [180/200], Loss: 0.00047\n",
            "Epoch [190/200], Loss: 0.00085\n",
            "Epoch [200/200], Loss: 0.00045\n",
            "--------------------\n",
            "stock number: 1945\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01018\n",
            "Epoch [20/200], Loss: 0.00335\n",
            "Epoch [30/200], Loss: 0.00315\n",
            "Epoch [40/200], Loss: 0.00487\n",
            "Epoch [50/200], Loss: 0.00131\n",
            "Epoch [60/200], Loss: 0.00065\n",
            "Epoch [70/200], Loss: 0.00068\n",
            "Epoch [80/200], Loss: 0.00059\n",
            "Epoch [90/200], Loss: 0.00124\n",
            "Epoch [100/200], Loss: 0.00043\n",
            "Epoch [110/200], Loss: 0.00109\n",
            "Epoch [120/200], Loss: 0.00029\n",
            "Epoch [130/200], Loss: 0.00046\n",
            "Epoch [140/200], Loss: 0.00024\n",
            "Epoch [150/200], Loss: 0.00057\n",
            "Epoch [160/200], Loss: 0.00024\n",
            "Epoch [170/200], Loss: 0.00024\n",
            "Epoch [180/200], Loss: 0.00062\n",
            "Epoch [190/200], Loss: 0.00025\n",
            "Epoch [200/200], Loss: 0.00022\n",
            "--------------------\n",
            "stock number: 1946\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00441\n",
            "Epoch [20/200], Loss: 0.00492\n",
            "Epoch [30/200], Loss: 0.00161\n",
            "Epoch [40/200], Loss: 0.00178\n",
            "Epoch [50/200], Loss: 0.00068\n",
            "Epoch [60/200], Loss: 0.00061\n",
            "Epoch [70/200], Loss: 0.00132\n",
            "Epoch [80/200], Loss: 0.00100\n",
            "Epoch [90/200], Loss: 0.00045\n",
            "Epoch [100/200], Loss: 0.00052\n",
            "Epoch [110/200], Loss: 0.00041\n",
            "Epoch [120/200], Loss: 0.00059\n",
            "Epoch [130/200], Loss: 0.00015\n",
            "Epoch [140/200], Loss: 0.00050\n",
            "Epoch [150/200], Loss: 0.00024\n",
            "Epoch [160/200], Loss: 0.00017\n",
            "Epoch [170/200], Loss: 0.00011\n",
            "Epoch [180/200], Loss: 0.00013\n",
            "Epoch [190/200], Loss: 0.00044\n",
            "Epoch [200/200], Loss: 0.00182\n",
            "--------------------\n",
            "stock number: 1947\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02977\n",
            "Epoch [20/200], Loss: 0.00886\n",
            "Epoch [30/200], Loss: 0.00323\n",
            "Epoch [40/200], Loss: 0.00955\n",
            "Epoch [50/200], Loss: 0.00268\n",
            "Epoch [60/200], Loss: 0.00536\n",
            "Epoch [70/200], Loss: 0.00282\n",
            "Epoch [80/200], Loss: 0.00469\n",
            "Epoch [90/200], Loss: 0.00342\n",
            "Epoch [100/200], Loss: 0.00471\n",
            "Epoch [110/200], Loss: 0.00400\n",
            "Epoch [120/200], Loss: 0.00587\n",
            "Epoch [130/200], Loss: 0.00253\n",
            "Epoch [140/200], Loss: 0.00630\n",
            "Epoch [150/200], Loss: 0.00338\n",
            "Epoch [160/200], Loss: 0.00303\n",
            "Epoch [170/200], Loss: 0.00374\n",
            "Epoch [180/200], Loss: 0.00297\n",
            "Epoch [190/200], Loss: 0.00343\n",
            "Epoch [200/200], Loss: 0.00313\n",
            "--------------------\n",
            "stock number: 1948\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.06854\n",
            "Epoch [20/200], Loss: 0.02078\n",
            "Epoch [30/200], Loss: 0.00232\n",
            "Epoch [40/200], Loss: 0.00193\n",
            "Epoch [50/200], Loss: 0.00240\n",
            "Epoch [60/200], Loss: 0.00392\n",
            "Epoch [70/200], Loss: 0.00163\n",
            "Epoch [80/200], Loss: 0.00235\n",
            "Epoch [90/200], Loss: 0.00163\n",
            "Epoch [100/200], Loss: 0.00146\n",
            "Epoch [110/200], Loss: 0.00379\n",
            "Epoch [120/200], Loss: 0.00132\n",
            "Epoch [130/200], Loss: 0.00122\n",
            "Epoch [140/200], Loss: 0.00333\n",
            "Epoch [150/200], Loss: 0.00105\n",
            "Epoch [160/200], Loss: 0.00136\n",
            "Epoch [170/200], Loss: 0.00205\n",
            "Epoch [180/200], Loss: 0.00135\n",
            "Epoch [190/200], Loss: 0.00090\n",
            "Epoch [200/200], Loss: 0.00092\n",
            "--------------------\n",
            "stock number: 1949\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01210\n",
            "Epoch [20/200], Loss: 0.00935\n",
            "Epoch [30/200], Loss: 0.00238\n",
            "Epoch [40/200], Loss: 0.00216\n",
            "Epoch [50/200], Loss: 0.00128\n",
            "Epoch [60/200], Loss: 0.00126\n",
            "Epoch [70/200], Loss: 0.00068\n",
            "Epoch [80/200], Loss: 0.00139\n",
            "Epoch [90/200], Loss: 0.00069\n",
            "Epoch [100/200], Loss: 0.00073\n",
            "Epoch [110/200], Loss: 0.00035\n",
            "Epoch [120/200], Loss: 0.00200\n",
            "Epoch [130/200], Loss: 0.00104\n",
            "Epoch [140/200], Loss: 0.00034\n",
            "Epoch [150/200], Loss: 0.00046\n",
            "Epoch [160/200], Loss: 0.00111\n",
            "Epoch [170/200], Loss: 0.00071\n",
            "Epoch [180/200], Loss: 0.00033\n",
            "Epoch [190/200], Loss: 0.00023\n",
            "Epoch [200/200], Loss: 0.00031\n",
            "--------------------\n",
            "stock number: 1950\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00764\n",
            "Epoch [20/200], Loss: 0.00470\n",
            "Epoch [30/200], Loss: 0.00258\n",
            "Epoch [40/200], Loss: 0.00178\n",
            "Epoch [50/200], Loss: 0.00045\n",
            "Epoch [60/200], Loss: 0.00139\n",
            "Epoch [70/200], Loss: 0.00034\n",
            "Epoch [80/200], Loss: 0.00035\n",
            "Epoch [90/200], Loss: 0.00050\n",
            "Epoch [100/200], Loss: 0.00053\n",
            "Epoch [110/200], Loss: 0.00040\n",
            "Epoch [120/200], Loss: 0.00033\n",
            "Epoch [130/200], Loss: 0.00036\n",
            "Epoch [140/200], Loss: 0.00037\n",
            "Epoch [150/200], Loss: 0.00031\n",
            "Epoch [160/200], Loss: 0.00021\n",
            "Epoch [170/200], Loss: 0.00016\n",
            "Epoch [180/200], Loss: 0.00187\n",
            "Epoch [190/200], Loss: 0.00021\n",
            "Epoch [200/200], Loss: 0.00017\n",
            "--------------------\n",
            "stock number: 1951\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02877\n",
            "Epoch [20/200], Loss: 0.00391\n",
            "Epoch [30/200], Loss: 0.05309\n",
            "Epoch [40/200], Loss: 0.00241\n",
            "Epoch [50/200], Loss: 0.00896\n",
            "Epoch [60/200], Loss: 0.00339\n",
            "Epoch [70/200], Loss: 0.00449\n",
            "Epoch [80/200], Loss: 0.00296\n",
            "Epoch [90/200], Loss: 0.00293\n",
            "Epoch [100/200], Loss: 0.00400\n",
            "Epoch [110/200], Loss: 0.00227\n",
            "Epoch [120/200], Loss: 0.00318\n",
            "Epoch [130/200], Loss: 0.00118\n",
            "Epoch [140/200], Loss: 0.00367\n",
            "Epoch [150/200], Loss: 0.00180\n",
            "Epoch [160/200], Loss: 0.00479\n",
            "Epoch [170/200], Loss: 0.00412\n",
            "Epoch [180/200], Loss: 0.00202\n",
            "Epoch [190/200], Loss: 0.00123\n",
            "Epoch [200/200], Loss: 0.00292\n",
            "--------------------\n",
            "stock number: 1952\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00233\n",
            "Epoch [20/200], Loss: 0.00227\n",
            "Epoch [30/200], Loss: 0.00054\n",
            "Epoch [40/200], Loss: 0.00070\n",
            "Epoch [50/200], Loss: 0.00062\n",
            "Epoch [60/200], Loss: 0.00033\n",
            "Epoch [70/200], Loss: 0.00081\n",
            "Epoch [80/200], Loss: 0.00028\n",
            "Epoch [90/200], Loss: 0.00031\n",
            "Epoch [100/200], Loss: 0.00036\n",
            "Epoch [110/200], Loss: 0.00020\n",
            "Epoch [120/200], Loss: 0.00031\n",
            "Epoch [130/200], Loss: 0.00017\n",
            "Epoch [140/200], Loss: 0.00019\n",
            "Epoch [150/200], Loss: 0.00017\n",
            "Epoch [160/200], Loss: 0.00017\n",
            "Epoch [170/200], Loss: 0.00078\n",
            "Epoch [180/200], Loss: 0.00021\n",
            "Epoch [190/200], Loss: 0.00011\n",
            "Epoch [200/200], Loss: 0.00016\n",
            "--------------------\n",
            "stock number: 1953\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01083\n",
            "Epoch [20/200], Loss: 0.00757\n",
            "Epoch [30/200], Loss: 0.01195\n",
            "Epoch [40/200], Loss: 0.01016\n",
            "Epoch [50/200], Loss: 0.00697\n",
            "Epoch [60/200], Loss: 0.00415\n",
            "Epoch [70/200], Loss: 0.00722\n",
            "Epoch [80/200], Loss: 0.00705\n",
            "Epoch [90/200], Loss: 0.00480\n",
            "Epoch [100/200], Loss: 0.00535\n",
            "Epoch [110/200], Loss: 0.00439\n",
            "Epoch [120/200], Loss: 0.00236\n",
            "Epoch [130/200], Loss: 0.00316\n",
            "Epoch [140/200], Loss: 0.00360\n",
            "Epoch [150/200], Loss: 0.00411\n",
            "Epoch [160/200], Loss: 0.00376\n",
            "Epoch [170/200], Loss: 0.00410\n",
            "Epoch [180/200], Loss: 0.00263\n",
            "Epoch [190/200], Loss: 0.00292\n",
            "Epoch [200/200], Loss: 0.00276\n",
            "--------------------\n",
            "stock number: 1954\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00894\n",
            "Epoch [20/200], Loss: 0.00367\n",
            "Epoch [30/200], Loss: 0.00286\n",
            "Epoch [40/200], Loss: 0.00331\n",
            "Epoch [50/200], Loss: 0.00275\n",
            "Epoch [60/200], Loss: 0.00243\n",
            "Epoch [70/200], Loss: 0.00106\n",
            "Epoch [80/200], Loss: 0.00164\n",
            "Epoch [90/200], Loss: 0.00065\n",
            "Epoch [100/200], Loss: 0.00041\n",
            "Epoch [110/200], Loss: 0.00248\n",
            "Epoch [120/200], Loss: 0.00032\n",
            "Epoch [130/200], Loss: 0.00022\n",
            "Epoch [140/200], Loss: 0.00059\n",
            "Epoch [150/200], Loss: 0.00033\n",
            "Epoch [160/200], Loss: 0.00039\n",
            "Epoch [170/200], Loss: 0.00040\n",
            "Epoch [180/200], Loss: 0.00019\n",
            "Epoch [190/200], Loss: 0.00017\n",
            "Epoch [200/200], Loss: 0.00048\n",
            "--------------------\n",
            "stock number: 1955\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00815\n",
            "Epoch [20/200], Loss: 0.00344\n",
            "Epoch [30/200], Loss: 0.00224\n",
            "Epoch [40/200], Loss: 0.00120\n",
            "Epoch [50/200], Loss: 0.00159\n",
            "Epoch [60/200], Loss: 0.00310\n",
            "Epoch [70/200], Loss: 0.00120\n",
            "Epoch [80/200], Loss: 0.00301\n",
            "Epoch [90/200], Loss: 0.00082\n",
            "Epoch [100/200], Loss: 0.00070\n",
            "Epoch [110/200], Loss: 0.00108\n",
            "Epoch [120/200], Loss: 0.00039\n",
            "Epoch [130/200], Loss: 0.00062\n",
            "Epoch [140/200], Loss: 0.00056\n",
            "Epoch [150/200], Loss: 0.00070\n",
            "Epoch [160/200], Loss: 0.00040\n",
            "Epoch [170/200], Loss: 0.00021\n",
            "Epoch [180/200], Loss: 0.00023\n",
            "Epoch [190/200], Loss: 0.00023\n",
            "Epoch [200/200], Loss: 0.00127\n",
            "--------------------\n",
            "stock number: 1956\n",
            "--------------------\n",
            "pca n_components_ 4\n",
            "sequences shape:  torch.Size([469, 60, 4])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00263\n",
            "Epoch [20/200], Loss: 0.00636\n",
            "Epoch [30/200], Loss: 0.00152\n",
            "Epoch [40/200], Loss: 0.00104\n",
            "Epoch [50/200], Loss: 0.00347\n",
            "Epoch [60/200], Loss: 0.00311\n",
            "Epoch [70/200], Loss: 0.00127\n",
            "Epoch [80/200], Loss: 0.00259\n",
            "Epoch [90/200], Loss: 0.00217\n",
            "Epoch [100/200], Loss: 0.00126\n",
            "Epoch [110/200], Loss: 0.00084\n",
            "Epoch [120/200], Loss: 0.00079\n",
            "Epoch [130/200], Loss: 0.00133\n",
            "Epoch [140/200], Loss: 0.00084\n",
            "Epoch [150/200], Loss: 0.00131\n",
            "Epoch [160/200], Loss: 0.00104\n",
            "Epoch [170/200], Loss: 0.00052\n",
            "Epoch [180/200], Loss: 0.00126\n",
            "Epoch [190/200], Loss: 0.00072\n",
            "Epoch [200/200], Loss: 0.00107\n",
            "--------------------\n",
            "stock number: 1957\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.04138\n",
            "Epoch [20/200], Loss: 0.05258\n",
            "Epoch [30/200], Loss: 0.04037\n",
            "Epoch [40/200], Loss: 0.03852\n",
            "Epoch [50/200], Loss: 0.04256\n",
            "Epoch [60/200], Loss: 0.04449\n",
            "Epoch [70/200], Loss: 0.03744\n",
            "Epoch [80/200], Loss: 0.04117\n",
            "Epoch [90/200], Loss: 0.03877\n",
            "Epoch [100/200], Loss: 0.04561\n",
            "Epoch [110/200], Loss: 0.03775\n",
            "Epoch [120/200], Loss: 0.03927\n",
            "Epoch [130/200], Loss: 0.03730\n",
            "Epoch [140/200], Loss: 0.03790\n",
            "Epoch [150/200], Loss: 0.03900\n",
            "Epoch [160/200], Loss: 0.03823\n",
            "Epoch [170/200], Loss: 0.04176\n",
            "Epoch [180/200], Loss: 0.03737\n",
            "Epoch [190/200], Loss: 0.03766\n",
            "Epoch [200/200], Loss: 0.03841\n",
            "--------------------\n",
            "stock number: 1958\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00495\n",
            "Epoch [20/200], Loss: 0.00165\n",
            "Epoch [30/200], Loss: 0.00788\n",
            "Epoch [40/200], Loss: 0.00155\n",
            "Epoch [50/200], Loss: 0.00294\n",
            "Epoch [60/200], Loss: 0.00334\n",
            "Epoch [70/200], Loss: 0.00098\n",
            "Epoch [80/200], Loss: 0.00443\n",
            "Epoch [90/200], Loss: 0.00155\n",
            "Epoch [100/200], Loss: 0.00231\n",
            "Epoch [110/200], Loss: 0.00115\n",
            "Epoch [120/200], Loss: 0.00167\n",
            "Epoch [130/200], Loss: 0.00258\n",
            "Epoch [140/200], Loss: 0.00209\n",
            "Epoch [150/200], Loss: 0.00109\n",
            "Epoch [160/200], Loss: 0.00066\n",
            "Epoch [170/200], Loss: 0.00064\n",
            "Epoch [180/200], Loss: 0.00148\n",
            "Epoch [190/200], Loss: 0.00055\n",
            "Epoch [200/200], Loss: 0.00082\n",
            "--------------------\n",
            "stock number: 1959\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02757\n",
            "Epoch [20/200], Loss: 0.02992\n",
            "Epoch [30/200], Loss: 0.00457\n",
            "Epoch [40/200], Loss: 0.00289\n",
            "Epoch [50/200], Loss: 0.02321\n",
            "Epoch [60/200], Loss: 0.00103\n",
            "Epoch [70/200], Loss: 0.00133\n",
            "Epoch [80/200], Loss: 0.00057\n",
            "Epoch [90/200], Loss: 0.00089\n",
            "Epoch [100/200], Loss: 0.00059\n",
            "Epoch [110/200], Loss: 0.00056\n",
            "Epoch [120/200], Loss: 0.00054\n",
            "Epoch [130/200], Loss: 0.00028\n",
            "Epoch [140/200], Loss: 0.00160\n",
            "Epoch [150/200], Loss: 0.00020\n",
            "Epoch [160/200], Loss: 0.00019\n",
            "Epoch [170/200], Loss: 0.00051\n",
            "Epoch [180/200], Loss: 0.00103\n",
            "Epoch [190/200], Loss: 0.00131\n",
            "Epoch [200/200], Loss: 0.00016\n",
            "--------------------\n",
            "stock number: 1960\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01090\n",
            "Epoch [20/200], Loss: 0.00421\n",
            "Epoch [30/200], Loss: 0.00577\n",
            "Epoch [40/200], Loss: 0.00165\n",
            "Epoch [50/200], Loss: 0.00212\n",
            "Epoch [60/200], Loss: 0.00137\n",
            "Epoch [70/200], Loss: 0.00164\n",
            "Epoch [80/200], Loss: 0.00130\n",
            "Epoch [90/200], Loss: 0.00114\n",
            "Epoch [100/200], Loss: 0.00127\n",
            "Epoch [110/200], Loss: 0.00180\n",
            "Epoch [120/200], Loss: 0.00248\n",
            "Epoch [130/200], Loss: 0.00067\n",
            "Epoch [140/200], Loss: 0.00065\n",
            "Epoch [150/200], Loss: 0.00056\n",
            "Epoch [160/200], Loss: 0.00097\n",
            "Epoch [170/200], Loss: 0.00047\n",
            "Epoch [180/200], Loss: 0.00041\n",
            "Epoch [190/200], Loss: 0.00042\n",
            "Epoch [200/200], Loss: 0.00037\n",
            "--------------------\n",
            "stock number: 1961\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00009\n",
            "Epoch [20/200], Loss: 0.00009\n",
            "Epoch [30/200], Loss: 0.00009\n",
            "Epoch [40/200], Loss: 0.00009\n",
            "Epoch [50/200], Loss: 0.00009\n",
            "Epoch [60/200], Loss: 0.00009\n",
            "Epoch [70/200], Loss: 0.00009\n",
            "Epoch [80/200], Loss: 0.00009\n",
            "Epoch [90/200], Loss: 0.00009\n",
            "Epoch [100/200], Loss: 0.00009\n",
            "Epoch [110/200], Loss: 0.00009\n",
            "Epoch [120/200], Loss: 0.00009\n",
            "Epoch [130/200], Loss: 0.00009\n",
            "Epoch [140/200], Loss: 0.00009\n",
            "Epoch [150/200], Loss: 0.00009\n",
            "Epoch [160/200], Loss: 0.00009\n",
            "Epoch [170/200], Loss: 0.00009\n",
            "Epoch [180/200], Loss: 0.00009\n",
            "Epoch [190/200], Loss: 0.00009\n",
            "Epoch [200/200], Loss: 0.00009\n",
            "--------------------\n",
            "stock number: 1962\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.09514\n",
            "Epoch [20/200], Loss: 0.09247\n",
            "Epoch [30/200], Loss: 0.07860\n",
            "Epoch [40/200], Loss: 0.01183\n",
            "Epoch [50/200], Loss: 0.00590\n",
            "Epoch [60/200], Loss: 0.02323\n",
            "Epoch [70/200], Loss: 0.00193\n",
            "Epoch [80/200], Loss: 0.00205\n",
            "Epoch [90/200], Loss: 0.00103\n",
            "Epoch [100/200], Loss: 0.00189\n",
            "Epoch [110/200], Loss: 0.00112\n",
            "Epoch [120/200], Loss: 0.00097\n",
            "Epoch [130/200], Loss: 0.00183\n",
            "Epoch [140/200], Loss: 0.00112\n",
            "Epoch [150/200], Loss: 0.00068\n",
            "Epoch [160/200], Loss: 0.00230\n",
            "Epoch [170/200], Loss: 0.00060\n",
            "Epoch [180/200], Loss: 0.00052\n",
            "Epoch [190/200], Loss: 0.00096\n",
            "Epoch [200/200], Loss: 0.00129\n",
            "--------------------\n",
            "stock number: 1963\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01307\n",
            "Epoch [20/200], Loss: 0.00410\n",
            "Epoch [30/200], Loss: 0.00572\n",
            "Epoch [40/200], Loss: 0.00199\n",
            "Epoch [50/200], Loss: 0.00170\n",
            "Epoch [60/200], Loss: 0.00398\n",
            "Epoch [70/200], Loss: 0.00085\n",
            "Epoch [80/200], Loss: 0.00194\n",
            "Epoch [90/200], Loss: 0.00074\n",
            "Epoch [100/200], Loss: 0.00058\n",
            "Epoch [110/200], Loss: 0.00130\n",
            "Epoch [120/200], Loss: 0.00042\n",
            "Epoch [130/200], Loss: 0.00092\n",
            "Epoch [140/200], Loss: 0.00028\n",
            "Epoch [150/200], Loss: 0.00060\n",
            "Epoch [160/200], Loss: 0.00049\n",
            "Epoch [170/200], Loss: 0.00024\n",
            "Epoch [180/200], Loss: 0.00035\n",
            "Epoch [190/200], Loss: 0.00042\n",
            "Epoch [200/200], Loss: 0.00066\n",
            "--------------------\n",
            "stock number: 1964\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01255\n",
            "Epoch [20/200], Loss: 0.00294\n",
            "Epoch [30/200], Loss: 0.00446\n",
            "Epoch [40/200], Loss: 0.00134\n",
            "Epoch [50/200], Loss: 0.00159\n",
            "Epoch [60/200], Loss: 0.00135\n",
            "Epoch [70/200], Loss: 0.00114\n",
            "Epoch [80/200], Loss: 0.00110\n",
            "Epoch [90/200], Loss: 0.00085\n",
            "Epoch [100/200], Loss: 0.00064\n",
            "Epoch [110/200], Loss: 0.00074\n",
            "Epoch [120/200], Loss: 0.00082\n",
            "Epoch [130/200], Loss: 0.00069\n",
            "Epoch [140/200], Loss: 0.00054\n",
            "Epoch [150/200], Loss: 0.00104\n",
            "Epoch [160/200], Loss: 0.00043\n",
            "Epoch [170/200], Loss: 0.00108\n",
            "Epoch [180/200], Loss: 0.00054\n",
            "Epoch [190/200], Loss: 0.00064\n",
            "Epoch [200/200], Loss: 0.00035\n",
            "--------------------\n",
            "stock number: 1965\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00352\n",
            "Epoch [20/200], Loss: 0.00596\n",
            "Epoch [30/200], Loss: 0.00262\n",
            "Epoch [40/200], Loss: 0.00193\n",
            "Epoch [50/200], Loss: 0.00130\n",
            "Epoch [60/200], Loss: 0.00397\n",
            "Epoch [70/200], Loss: 0.00110\n",
            "Epoch [80/200], Loss: 0.00324\n",
            "Epoch [90/200], Loss: 0.00083\n",
            "Epoch [100/200], Loss: 0.00187\n",
            "Epoch [110/200], Loss: 0.00088\n",
            "Epoch [120/200], Loss: 0.00065\n",
            "Epoch [130/200], Loss: 0.00064\n",
            "Epoch [140/200], Loss: 0.00063\n",
            "Epoch [150/200], Loss: 0.00064\n",
            "Epoch [160/200], Loss: 0.00085\n",
            "Epoch [170/200], Loss: 0.00047\n",
            "Epoch [180/200], Loss: 0.00045\n",
            "Epoch [190/200], Loss: 0.00135\n",
            "Epoch [200/200], Loss: 0.00041\n",
            "--------------------\n",
            "stock number: 1966\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00156\n",
            "Epoch [20/200], Loss: 0.00103\n",
            "Epoch [30/200], Loss: 0.00113\n",
            "Epoch [40/200], Loss: 0.00131\n",
            "Epoch [50/200], Loss: 0.00044\n",
            "Epoch [60/200], Loss: 0.00055\n",
            "Epoch [70/200], Loss: 0.00033\n",
            "Epoch [80/200], Loss: 0.00050\n",
            "Epoch [90/200], Loss: 0.00031\n",
            "Epoch [100/200], Loss: 0.00138\n",
            "Epoch [110/200], Loss: 0.00035\n",
            "Epoch [120/200], Loss: 0.00042\n",
            "Epoch [130/200], Loss: 0.00033\n",
            "Epoch [140/200], Loss: 0.00032\n",
            "Epoch [150/200], Loss: 0.00035\n",
            "Epoch [160/200], Loss: 0.00035\n",
            "Epoch [170/200], Loss: 0.00039\n",
            "Epoch [180/200], Loss: 0.00013\n",
            "Epoch [190/200], Loss: 0.00016\n",
            "Epoch [200/200], Loss: 0.00025\n",
            "--------------------\n",
            "stock number: 1967\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01626\n",
            "Epoch [20/200], Loss: 0.01395\n",
            "Epoch [30/200], Loss: 0.00613\n",
            "Epoch [40/200], Loss: 0.00769\n",
            "Epoch [50/200], Loss: 0.01268\n",
            "Epoch [60/200], Loss: 0.00342\n",
            "Epoch [70/200], Loss: 0.00275\n",
            "Epoch [80/200], Loss: 0.00402\n",
            "Epoch [90/200], Loss: 0.00450\n",
            "Epoch [100/200], Loss: 0.00416\n",
            "Epoch [110/200], Loss: 0.00241\n",
            "Epoch [120/200], Loss: 0.00433\n",
            "Epoch [130/200], Loss: 0.00230\n",
            "Epoch [140/200], Loss: 0.00339\n",
            "Epoch [150/200], Loss: 0.00211\n",
            "Epoch [160/200], Loss: 0.00219\n",
            "Epoch [170/200], Loss: 0.00196\n",
            "Epoch [180/200], Loss: 0.00384\n",
            "Epoch [190/200], Loss: 0.00325\n",
            "Epoch [200/200], Loss: 0.00154\n",
            "--------------------\n",
            "stock number: 1968\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01077\n",
            "Epoch [20/200], Loss: 0.01419\n",
            "Epoch [30/200], Loss: 0.02656\n",
            "Epoch [40/200], Loss: 0.00897\n",
            "Epoch [50/200], Loss: 0.01377\n",
            "Epoch [60/200], Loss: 0.00567\n",
            "Epoch [70/200], Loss: 0.00876\n",
            "Epoch [80/200], Loss: 0.00954\n",
            "Epoch [90/200], Loss: 0.00765\n",
            "Epoch [100/200], Loss: 0.00468\n",
            "Epoch [110/200], Loss: 0.01007\n",
            "Epoch [120/200], Loss: 0.00450\n",
            "Epoch [130/200], Loss: 0.00706\n",
            "Epoch [140/200], Loss: 0.01046\n",
            "Epoch [150/200], Loss: 0.00315\n",
            "Epoch [160/200], Loss: 0.00393\n",
            "Epoch [170/200], Loss: 0.00258\n",
            "Epoch [180/200], Loss: 0.00283\n",
            "Epoch [190/200], Loss: 0.00533\n",
            "Epoch [200/200], Loss: 0.00215\n",
            "--------------------\n",
            "stock number: 1969\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00428\n",
            "Epoch [20/200], Loss: 0.00298\n",
            "Epoch [30/200], Loss: 0.00124\n",
            "Epoch [40/200], Loss: 0.00156\n",
            "Epoch [50/200], Loss: 0.00137\n",
            "Epoch [60/200], Loss: 0.00052\n",
            "Epoch [70/200], Loss: 0.00427\n",
            "Epoch [80/200], Loss: 0.00067\n",
            "Epoch [90/200], Loss: 0.00248\n",
            "Epoch [100/200], Loss: 0.00289\n",
            "Epoch [110/200], Loss: 0.00038\n",
            "Epoch [120/200], Loss: 0.00025\n",
            "Epoch [130/200], Loss: 0.00029\n",
            "Epoch [140/200], Loss: 0.00096\n",
            "Epoch [150/200], Loss: 0.00098\n",
            "Epoch [160/200], Loss: 0.00035\n",
            "Epoch [170/200], Loss: 0.00111\n",
            "Epoch [180/200], Loss: 0.00063\n",
            "Epoch [190/200], Loss: 0.00045\n",
            "Epoch [200/200], Loss: 0.00035\n",
            "--------------------\n",
            "stock number: 1970\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00758\n",
            "Epoch [20/200], Loss: 0.00872\n",
            "Epoch [30/200], Loss: 0.00618\n",
            "Epoch [40/200], Loss: 0.00373\n",
            "Epoch [50/200], Loss: 0.00232\n",
            "Epoch [60/200], Loss: 0.00209\n",
            "Epoch [70/200], Loss: 0.00105\n",
            "Epoch [80/200], Loss: 0.00077\n",
            "Epoch [90/200], Loss: 0.00069\n",
            "Epoch [100/200], Loss: 0.00078\n",
            "Epoch [110/200], Loss: 0.00067\n",
            "Epoch [120/200], Loss: 0.00047\n",
            "Epoch [130/200], Loss: 0.00057\n",
            "Epoch [140/200], Loss: 0.00068\n",
            "Epoch [150/200], Loss: 0.00035\n",
            "Epoch [160/200], Loss: 0.00046\n",
            "Epoch [170/200], Loss: 0.00034\n",
            "Epoch [180/200], Loss: 0.00033\n",
            "Epoch [190/200], Loss: 0.00042\n",
            "Epoch [200/200], Loss: 0.00044\n",
            "--------------------\n",
            "stock number: 1971\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01407\n",
            "Epoch [20/200], Loss: 0.02825\n",
            "Epoch [30/200], Loss: 0.00740\n",
            "Epoch [40/200], Loss: 0.00689\n",
            "Epoch [50/200], Loss: 0.00672\n",
            "Epoch [60/200], Loss: 0.01036\n",
            "Epoch [70/200], Loss: 0.00489\n",
            "Epoch [80/200], Loss: 0.00625\n",
            "Epoch [90/200], Loss: 0.00195\n",
            "Epoch [100/200], Loss: 0.00987\n",
            "Epoch [110/200], Loss: 0.00311\n",
            "Epoch [120/200], Loss: 0.00146\n",
            "Epoch [130/200], Loss: 0.00227\n",
            "Epoch [140/200], Loss: 0.00170\n",
            "Epoch [150/200], Loss: 0.00384\n",
            "Epoch [160/200], Loss: 0.00556\n",
            "Epoch [170/200], Loss: 0.00263\n",
            "Epoch [180/200], Loss: 0.00127\n",
            "Epoch [190/200], Loss: 0.00172\n",
            "Epoch [200/200], Loss: 0.00172\n",
            "--------------------\n",
            "stock number: 1972\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00037\n",
            "Epoch [20/200], Loss: 0.00037\n",
            "Epoch [30/200], Loss: 0.00036\n",
            "Epoch [40/200], Loss: 0.00035\n",
            "Epoch [50/200], Loss: 0.00040\n",
            "Epoch [60/200], Loss: 0.00037\n",
            "Epoch [70/200], Loss: 0.00037\n",
            "Epoch [80/200], Loss: 0.00037\n",
            "Epoch [90/200], Loss: 0.00037\n",
            "Epoch [100/200], Loss: 0.00037\n",
            "Epoch [110/200], Loss: 0.00034\n",
            "Epoch [120/200], Loss: 0.00038\n",
            "Epoch [130/200], Loss: 0.00034\n",
            "Epoch [140/200], Loss: 0.00044\n",
            "Epoch [150/200], Loss: 0.00032\n",
            "Epoch [160/200], Loss: 0.00035\n",
            "Epoch [170/200], Loss: 0.00035\n",
            "Epoch [180/200], Loss: 0.00020\n",
            "Epoch [190/200], Loss: 0.00029\n",
            "Epoch [200/200], Loss: 0.00073\n",
            "--------------------\n",
            "stock number: 1973\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.04200\n",
            "Epoch [20/200], Loss: 0.04923\n",
            "Epoch [30/200], Loss: 0.01211\n",
            "Epoch [40/200], Loss: 0.00230\n",
            "Epoch [50/200], Loss: 0.00114\n",
            "Epoch [60/200], Loss: 0.00078\n",
            "Epoch [70/200], Loss: 0.00076\n",
            "Epoch [80/200], Loss: 0.00079\n",
            "Epoch [90/200], Loss: 0.00059\n",
            "Epoch [100/200], Loss: 0.00054\n",
            "Epoch [110/200], Loss: 0.00090\n",
            "Epoch [120/200], Loss: 0.00093\n",
            "Epoch [130/200], Loss: 0.00067\n",
            "Epoch [140/200], Loss: 0.00111\n",
            "Epoch [150/200], Loss: 0.00057\n",
            "Epoch [160/200], Loss: 0.00074\n",
            "Epoch [170/200], Loss: 0.00071\n",
            "Epoch [180/200], Loss: 0.00045\n",
            "Epoch [190/200], Loss: 0.00060\n",
            "Epoch [200/200], Loss: 0.00033\n",
            "--------------------\n",
            "stock number: 1974\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00548\n",
            "Epoch [20/200], Loss: 0.00417\n",
            "Epoch [30/200], Loss: 0.00509\n",
            "Epoch [40/200], Loss: 0.00191\n",
            "Epoch [50/200], Loss: 0.00175\n",
            "Epoch [60/200], Loss: 0.00094\n",
            "Epoch [70/200], Loss: 0.00397\n",
            "Epoch [80/200], Loss: 0.00251\n",
            "Epoch [90/200], Loss: 0.00405\n",
            "Epoch [100/200], Loss: 0.00042\n",
            "Epoch [110/200], Loss: 0.00064\n",
            "Epoch [120/200], Loss: 0.00211\n",
            "Epoch [130/200], Loss: 0.00086\n",
            "Epoch [140/200], Loss: 0.00241\n",
            "Epoch [150/200], Loss: 0.00095\n",
            "Epoch [160/200], Loss: 0.00179\n",
            "Epoch [170/200], Loss: 0.00148\n",
            "Epoch [180/200], Loss: 0.00129\n",
            "Epoch [190/200], Loss: 0.00111\n",
            "Epoch [200/200], Loss: 0.00160\n",
            "--------------------\n",
            "stock number: 1975\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01954\n",
            "Epoch [20/200], Loss: 0.00381\n",
            "Epoch [30/200], Loss: 0.00334\n",
            "Epoch [40/200], Loss: 0.00133\n",
            "Epoch [50/200], Loss: 0.00152\n",
            "Epoch [60/200], Loss: 0.00146\n",
            "Epoch [70/200], Loss: 0.00154\n",
            "Epoch [80/200], Loss: 0.00131\n",
            "Epoch [90/200], Loss: 0.00090\n",
            "Epoch [100/200], Loss: 0.00083\n",
            "Epoch [110/200], Loss: 0.00180\n",
            "Epoch [120/200], Loss: 0.00113\n",
            "Epoch [130/200], Loss: 0.00128\n",
            "Epoch [140/200], Loss: 0.00102\n",
            "Epoch [150/200], Loss: 0.00103\n",
            "Epoch [160/200], Loss: 0.00079\n",
            "Epoch [170/200], Loss: 0.00066\n",
            "Epoch [180/200], Loss: 0.00191\n",
            "Epoch [190/200], Loss: 0.00076\n",
            "Epoch [200/200], Loss: 0.00117\n",
            "--------------------\n",
            "stock number: 1976\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.03346\n",
            "Epoch [20/200], Loss: 0.00171\n",
            "Epoch [30/200], Loss: 0.00089\n",
            "Epoch [40/200], Loss: 0.00081\n",
            "Epoch [50/200], Loss: 0.00303\n",
            "Epoch [60/200], Loss: 0.00062\n",
            "Epoch [70/200], Loss: 0.00095\n",
            "Epoch [80/200], Loss: 0.00055\n",
            "Epoch [90/200], Loss: 0.00059\n",
            "Epoch [100/200], Loss: 0.00069\n",
            "Epoch [110/200], Loss: 0.00085\n",
            "Epoch [120/200], Loss: 0.00125\n",
            "Epoch [130/200], Loss: 0.00093\n",
            "Epoch [140/200], Loss: 0.00070\n",
            "Epoch [150/200], Loss: 0.00060\n",
            "Epoch [160/200], Loss: 0.00168\n",
            "Epoch [170/200], Loss: 0.00046\n",
            "Epoch [180/200], Loss: 0.00035\n",
            "Epoch [190/200], Loss: 0.00037\n",
            "Epoch [200/200], Loss: 0.00079\n",
            "--------------------\n",
            "stock number: 1977\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.04914\n",
            "Epoch [20/200], Loss: 0.03454\n",
            "Epoch [30/200], Loss: 0.00960\n",
            "Epoch [40/200], Loss: 0.00721\n",
            "Epoch [50/200], Loss: 0.00932\n",
            "Epoch [60/200], Loss: 0.00495\n",
            "Epoch [70/200], Loss: 0.00579\n",
            "Epoch [80/200], Loss: 0.00684\n",
            "Epoch [90/200], Loss: 0.00500\n",
            "Epoch [100/200], Loss: 0.00381\n",
            "Epoch [110/200], Loss: 0.00754\n",
            "Epoch [120/200], Loss: 0.00371\n",
            "Epoch [130/200], Loss: 0.00277\n",
            "Epoch [140/200], Loss: 0.00348\n",
            "Epoch [150/200], Loss: 0.00256\n",
            "Epoch [160/200], Loss: 0.00249\n",
            "Epoch [170/200], Loss: 0.00270\n",
            "Epoch [180/200], Loss: 0.00548\n",
            "Epoch [190/200], Loss: 0.00197\n",
            "Epoch [200/200], Loss: 0.00118\n",
            "--------------------\n",
            "stock number: 1978\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00649\n",
            "Epoch [20/200], Loss: 0.00146\n",
            "Epoch [30/200], Loss: 0.00089\n",
            "Epoch [40/200], Loss: 0.00094\n",
            "Epoch [50/200], Loss: 0.00081\n",
            "Epoch [60/200], Loss: 0.00050\n",
            "Epoch [70/200], Loss: 0.00206\n",
            "Epoch [80/200], Loss: 0.00046\n",
            "Epoch [90/200], Loss: 0.00031\n",
            "Epoch [100/200], Loss: 0.00029\n",
            "Epoch [110/200], Loss: 0.00104\n",
            "Epoch [120/200], Loss: 0.00040\n",
            "Epoch [130/200], Loss: 0.00026\n",
            "Epoch [140/200], Loss: 0.00055\n",
            "Epoch [150/200], Loss: 0.00021\n",
            "Epoch [160/200], Loss: 0.00029\n",
            "Epoch [170/200], Loss: 0.00019\n",
            "Epoch [180/200], Loss: 0.00023\n",
            "Epoch [190/200], Loss: 0.00027\n",
            "Epoch [200/200], Loss: 0.00029\n",
            "--------------------\n",
            "stock number: 1979\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00454\n",
            "Epoch [20/200], Loss: 0.00270\n",
            "Epoch [30/200], Loss: 0.00173\n",
            "Epoch [40/200], Loss: 0.00202\n",
            "Epoch [50/200], Loss: 0.00226\n",
            "Epoch [60/200], Loss: 0.00091\n",
            "Epoch [70/200], Loss: 0.00072\n",
            "Epoch [80/200], Loss: 0.00088\n",
            "Epoch [90/200], Loss: 0.00036\n",
            "Epoch [100/200], Loss: 0.00039\n",
            "Epoch [110/200], Loss: 0.00020\n",
            "Epoch [120/200], Loss: 0.00036\n",
            "Epoch [130/200], Loss: 0.00019\n",
            "Epoch [140/200], Loss: 0.00014\n",
            "Epoch [150/200], Loss: 0.00022\n",
            "Epoch [160/200], Loss: 0.00014\n",
            "Epoch [170/200], Loss: 0.00022\n",
            "Epoch [180/200], Loss: 0.00012\n",
            "Epoch [190/200], Loss: 0.00022\n",
            "Epoch [200/200], Loss: 0.00020\n",
            "--------------------\n",
            "stock number: 1980\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01394\n",
            "Epoch [20/200], Loss: 0.00681\n",
            "Epoch [30/200], Loss: 0.00590\n",
            "Epoch [40/200], Loss: 0.00515\n",
            "Epoch [50/200], Loss: 0.00398\n",
            "Epoch [60/200], Loss: 0.00269\n",
            "Epoch [70/200], Loss: 0.01274\n",
            "Epoch [80/200], Loss: 0.00209\n",
            "Epoch [90/200], Loss: 0.00363\n",
            "Epoch [100/200], Loss: 0.00528\n",
            "Epoch [110/200], Loss: 0.00171\n",
            "Epoch [120/200], Loss: 0.00170\n",
            "Epoch [130/200], Loss: 0.00213\n",
            "Epoch [140/200], Loss: 0.00147\n",
            "Epoch [150/200], Loss: 0.00178\n",
            "Epoch [160/200], Loss: 0.00161\n",
            "Epoch [170/200], Loss: 0.00218\n",
            "Epoch [180/200], Loss: 0.00127\n",
            "Epoch [190/200], Loss: 0.00150\n",
            "Epoch [200/200], Loss: 0.00250\n",
            "--------------------\n",
            "stock number: 1981\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.10972\n",
            "Epoch [20/200], Loss: 0.08972\n",
            "Epoch [30/200], Loss: 0.08877\n",
            "Epoch [40/200], Loss: 0.00634\n",
            "Epoch [50/200], Loss: 0.04374\n",
            "Epoch [60/200], Loss: 0.00306\n",
            "Epoch [70/200], Loss: 0.00839\n",
            "Epoch [80/200], Loss: 0.00691\n",
            "Epoch [90/200], Loss: 0.00192\n",
            "Epoch [100/200], Loss: 0.00127\n",
            "Epoch [110/200], Loss: 0.00205\n",
            "Epoch [120/200], Loss: 0.00120\n",
            "Epoch [130/200], Loss: 0.00162\n",
            "Epoch [140/200], Loss: 0.00183\n",
            "Epoch [150/200], Loss: 0.00106\n",
            "Epoch [160/200], Loss: 0.00210\n",
            "Epoch [170/200], Loss: 0.00141\n",
            "Epoch [180/200], Loss: 0.00280\n",
            "Epoch [190/200], Loss: 0.00080\n",
            "Epoch [200/200], Loss: 0.00094\n",
            "--------------------\n",
            "stock number: 1982\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00136\n",
            "Epoch [20/200], Loss: 0.00703\n",
            "Epoch [30/200], Loss: 0.00303\n",
            "Epoch [40/200], Loss: 0.01954\n",
            "Epoch [50/200], Loss: 0.00113\n",
            "Epoch [60/200], Loss: 0.00106\n",
            "Epoch [70/200], Loss: 0.00115\n",
            "Epoch [80/200], Loss: 0.00077\n",
            "Epoch [90/200], Loss: 0.00093\n",
            "Epoch [100/200], Loss: 0.00053\n",
            "Epoch [110/200], Loss: 0.00060\n",
            "Epoch [120/200], Loss: 0.00060\n",
            "Epoch [130/200], Loss: 0.00096\n",
            "Epoch [140/200], Loss: 0.00042\n",
            "Epoch [150/200], Loss: 0.00058\n",
            "Epoch [160/200], Loss: 0.00130\n",
            "Epoch [170/200], Loss: 0.00047\n",
            "Epoch [180/200], Loss: 0.00050\n",
            "Epoch [190/200], Loss: 0.00041\n",
            "Epoch [200/200], Loss: 0.00037\n",
            "--------------------\n",
            "stock number: 1983\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00205\n",
            "Epoch [20/200], Loss: 0.00442\n",
            "Epoch [30/200], Loss: 0.00163\n",
            "Epoch [40/200], Loss: 0.00118\n",
            "Epoch [50/200], Loss: 0.00123\n",
            "Epoch [60/200], Loss: 0.00186\n",
            "Epoch [70/200], Loss: 0.00093\n",
            "Epoch [80/200], Loss: 0.00132\n",
            "Epoch [90/200], Loss: 0.00180\n",
            "Epoch [100/200], Loss: 0.00097\n",
            "Epoch [110/200], Loss: 0.00112\n",
            "Epoch [120/200], Loss: 0.00139\n",
            "Epoch [130/200], Loss: 0.00091\n",
            "Epoch [140/200], Loss: 0.00076\n",
            "Epoch [150/200], Loss: 0.00184\n",
            "Epoch [160/200], Loss: 0.00076\n",
            "Epoch [170/200], Loss: 0.00214\n",
            "Epoch [180/200], Loss: 0.00087\n",
            "Epoch [190/200], Loss: 0.00147\n",
            "Epoch [200/200], Loss: 0.00151\n",
            "--------------------\n",
            "stock number: 1984\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01127\n",
            "Epoch [20/200], Loss: 0.01166\n",
            "Epoch [30/200], Loss: 0.00443\n",
            "Epoch [40/200], Loss: 0.00302\n",
            "Epoch [50/200], Loss: 0.01841\n",
            "Epoch [60/200], Loss: 0.00200\n",
            "Epoch [70/200], Loss: 0.00184\n",
            "Epoch [80/200], Loss: 0.00156\n",
            "Epoch [90/200], Loss: 0.00159\n",
            "Epoch [100/200], Loss: 0.00166\n",
            "Epoch [110/200], Loss: 0.00205\n",
            "Epoch [120/200], Loss: 0.00158\n",
            "Epoch [130/200], Loss: 0.00142\n",
            "Epoch [140/200], Loss: 0.00115\n",
            "Epoch [150/200], Loss: 0.00431\n",
            "Epoch [160/200], Loss: 0.00215\n",
            "Epoch [170/200], Loss: 0.00220\n",
            "Epoch [180/200], Loss: 0.00265\n",
            "Epoch [190/200], Loss: 0.00166\n",
            "Epoch [200/200], Loss: 0.00159\n",
            "--------------------\n",
            "stock number: 1985\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.04438\n",
            "Epoch [20/200], Loss: 0.01630\n",
            "Epoch [30/200], Loss: 0.00849\n",
            "Epoch [40/200], Loss: 0.00685\n",
            "Epoch [50/200], Loss: 0.00658\n",
            "Epoch [60/200], Loss: 0.00459\n",
            "Epoch [70/200], Loss: 0.00588\n",
            "Epoch [80/200], Loss: 0.00435\n",
            "Epoch [90/200], Loss: 0.00469\n",
            "Epoch [100/200], Loss: 0.00314\n",
            "Epoch [110/200], Loss: 0.00324\n",
            "Epoch [120/200], Loss: 0.00367\n",
            "Epoch [130/200], Loss: 0.00264\n",
            "Epoch [140/200], Loss: 0.00343\n",
            "Epoch [150/200], Loss: 0.00211\n",
            "Epoch [160/200], Loss: 0.00429\n",
            "Epoch [170/200], Loss: 0.00246\n",
            "Epoch [180/200], Loss: 0.00275\n",
            "Epoch [190/200], Loss: 0.00484\n",
            "Epoch [200/200], Loss: 0.00218\n",
            "--------------------\n",
            "stock number: 1986\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01264\n",
            "Epoch [20/200], Loss: 0.00954\n",
            "Epoch [30/200], Loss: 0.01159\n",
            "Epoch [40/200], Loss: 0.00992\n",
            "Epoch [50/200], Loss: 0.00977\n",
            "Epoch [60/200], Loss: 0.00990\n",
            "Epoch [70/200], Loss: 0.01025\n",
            "Epoch [80/200], Loss: 0.00701\n",
            "Epoch [90/200], Loss: 0.00754\n",
            "Epoch [100/200], Loss: 0.01135\n",
            "Epoch [110/200], Loss: 0.01007\n",
            "Epoch [120/200], Loss: 0.01214\n",
            "Epoch [130/200], Loss: 0.01029\n",
            "Epoch [140/200], Loss: 0.01008\n",
            "Epoch [150/200], Loss: 0.00539\n",
            "Epoch [160/200], Loss: 0.00353\n",
            "Epoch [170/200], Loss: 0.00302\n",
            "Epoch [180/200], Loss: 0.01110\n",
            "Epoch [190/200], Loss: 0.01108\n",
            "Epoch [200/200], Loss: 0.00986\n",
            "--------------------\n",
            "stock number: 1987\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00080\n",
            "Epoch [20/200], Loss: 0.00078\n",
            "Epoch [30/200], Loss: 0.00071\n",
            "Epoch [40/200], Loss: 0.00114\n",
            "Epoch [50/200], Loss: 0.00082\n",
            "Epoch [60/200], Loss: 0.00047\n",
            "Epoch [70/200], Loss: 0.00045\n",
            "Epoch [80/200], Loss: 0.00071\n",
            "Epoch [90/200], Loss: 0.00025\n",
            "Epoch [100/200], Loss: 0.00049\n",
            "Epoch [110/200], Loss: 0.00024\n",
            "Epoch [120/200], Loss: 0.00035\n",
            "Epoch [130/200], Loss: 0.00052\n",
            "Epoch [140/200], Loss: 0.00016\n",
            "Epoch [150/200], Loss: 0.00013\n",
            "Epoch [160/200], Loss: 0.00017\n",
            "Epoch [170/200], Loss: 0.00011\n",
            "Epoch [180/200], Loss: 0.00006\n",
            "Epoch [190/200], Loss: 0.00022\n",
            "Epoch [200/200], Loss: 0.00019\n",
            "--------------------\n",
            "stock number: 1988\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.01441\n",
            "Epoch [20/200], Loss: 0.00303\n",
            "Epoch [30/200], Loss: 0.00158\n",
            "Epoch [40/200], Loss: 0.00151\n",
            "Epoch [50/200], Loss: 0.00056\n",
            "Epoch [60/200], Loss: 0.00067\n",
            "Epoch [70/200], Loss: 0.00089\n",
            "Epoch [80/200], Loss: 0.00035\n",
            "Epoch [90/200], Loss: 0.00041\n",
            "Epoch [100/200], Loss: 0.00030\n",
            "Epoch [110/200], Loss: 0.00136\n",
            "Epoch [120/200], Loss: 0.00156\n",
            "Epoch [130/200], Loss: 0.00036\n",
            "Epoch [140/200], Loss: 0.00046\n",
            "Epoch [150/200], Loss: 0.00053\n",
            "Epoch [160/200], Loss: 0.00051\n",
            "Epoch [170/200], Loss: 0.00030\n",
            "Epoch [180/200], Loss: 0.00132\n",
            "Epoch [190/200], Loss: 0.00024\n",
            "Epoch [200/200], Loss: 0.00041\n",
            "--------------------\n",
            "stock number: 1989\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.03674\n",
            "Epoch [20/200], Loss: 0.00314\n",
            "Epoch [30/200], Loss: 0.00402\n",
            "Epoch [40/200], Loss: 0.00269\n",
            "Epoch [50/200], Loss: 0.00375\n",
            "Epoch [60/200], Loss: 0.00639\n",
            "Epoch [70/200], Loss: 0.00504\n",
            "Epoch [80/200], Loss: 0.00377\n",
            "Epoch [90/200], Loss: 0.00291\n",
            "Epoch [100/200], Loss: 0.00341\n",
            "Epoch [110/200], Loss: 0.00242\n",
            "Epoch [120/200], Loss: 0.00657\n",
            "Epoch [130/200], Loss: 0.00239\n",
            "Epoch [140/200], Loss: 0.00287\n",
            "Epoch [150/200], Loss: 0.00239\n",
            "Epoch [160/200], Loss: 0.00191\n",
            "Epoch [170/200], Loss: 0.00184\n",
            "Epoch [180/200], Loss: 0.00403\n",
            "Epoch [190/200], Loss: 0.00412\n",
            "Epoch [200/200], Loss: 0.00205\n",
            "--------------------\n",
            "stock number: 1990\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.02274\n",
            "Epoch [20/200], Loss: 0.02274\n",
            "Epoch [30/200], Loss: 0.02274\n",
            "Epoch [40/200], Loss: 0.02274\n",
            "Epoch [50/200], Loss: 0.02191\n",
            "Epoch [60/200], Loss: 0.02009\n",
            "Epoch [70/200], Loss: 0.02207\n",
            "Epoch [80/200], Loss: 0.02126\n",
            "Epoch [90/200], Loss: 0.02103\n",
            "Epoch [100/200], Loss: 0.01916\n",
            "Epoch [110/200], Loss: 0.01275\n",
            "Epoch [120/200], Loss: 0.01116\n",
            "Epoch [130/200], Loss: 0.00520\n",
            "Epoch [140/200], Loss: 0.00128\n",
            "Epoch [150/200], Loss: 0.00214\n",
            "Epoch [160/200], Loss: 0.00085\n",
            "Epoch [170/200], Loss: 0.00104\n",
            "Epoch [180/200], Loss: 0.00188\n",
            "Epoch [190/200], Loss: 0.00063\n",
            "Epoch [200/200], Loss: 0.00109\n",
            "--------------------\n",
            "stock number: 1991\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00235\n",
            "Epoch [20/200], Loss: 0.00181\n",
            "Epoch [30/200], Loss: 0.00148\n",
            "Epoch [40/200], Loss: 0.00218\n",
            "Epoch [50/200], Loss: 0.00271\n",
            "Epoch [60/200], Loss: 0.00234\n",
            "Epoch [70/200], Loss: 0.00247\n",
            "Epoch [80/200], Loss: 0.00102\n",
            "Epoch [90/200], Loss: 0.00217\n",
            "Epoch [100/200], Loss: 0.00040\n",
            "Epoch [110/200], Loss: 0.00188\n",
            "Epoch [120/200], Loss: 0.00095\n",
            "Epoch [130/200], Loss: 0.00033\n",
            "Epoch [140/200], Loss: 0.00159\n",
            "Epoch [150/200], Loss: 0.00035\n",
            "Epoch [160/200], Loss: 0.00025\n",
            "Epoch [170/200], Loss: 0.00091\n",
            "Epoch [180/200], Loss: 0.00021\n",
            "Epoch [190/200], Loss: 0.00089\n",
            "Epoch [200/200], Loss: 0.00046\n",
            "--------------------\n",
            "stock number: 1992\n",
            "--------------------\n",
            "pca n_components_ 4\n",
            "sequences shape:  torch.Size([469, 60, 4])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00391\n",
            "Epoch [20/200], Loss: 0.01378\n",
            "Epoch [30/200], Loss: 0.00551\n",
            "Epoch [40/200], Loss: 0.00945\n",
            "Epoch [50/200], Loss: 0.00678\n",
            "Epoch [60/200], Loss: 0.00123\n",
            "Epoch [70/200], Loss: 0.00202\n",
            "Epoch [80/200], Loss: 0.00157\n",
            "Epoch [90/200], Loss: 0.00255\n",
            "Epoch [100/200], Loss: 0.00132\n",
            "Epoch [110/200], Loss: 0.00127\n",
            "Epoch [120/200], Loss: 0.00377\n",
            "Epoch [130/200], Loss: 0.00239\n",
            "Epoch [140/200], Loss: 0.00168\n",
            "Epoch [150/200], Loss: 0.00155\n",
            "Epoch [160/200], Loss: 0.00151\n",
            "Epoch [170/200], Loss: 0.00263\n",
            "Epoch [180/200], Loss: 0.00116\n",
            "Epoch [190/200], Loss: 0.00225\n",
            "Epoch [200/200], Loss: 0.00159\n",
            "--------------------\n",
            "stock number: 1993\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.03216\n",
            "Epoch [20/200], Loss: 0.04597\n",
            "Epoch [30/200], Loss: 0.00700\n",
            "Epoch [40/200], Loss: 0.00571\n",
            "Epoch [50/200], Loss: 0.00368\n",
            "Epoch [60/200], Loss: 0.00344\n",
            "Epoch [70/200], Loss: 0.00307\n",
            "Epoch [80/200], Loss: 0.00205\n",
            "Epoch [90/200], Loss: 0.00189\n",
            "Epoch [100/200], Loss: 0.00195\n",
            "Epoch [110/200], Loss: 0.00171\n",
            "Epoch [120/200], Loss: 0.00216\n",
            "Epoch [130/200], Loss: 0.00204\n",
            "Epoch [140/200], Loss: 0.00156\n",
            "Epoch [150/200], Loss: 0.00188\n",
            "Epoch [160/200], Loss: 0.00152\n",
            "Epoch [170/200], Loss: 0.00193\n",
            "Epoch [180/200], Loss: 0.00240\n",
            "Epoch [190/200], Loss: 0.00108\n",
            "Epoch [200/200], Loss: 0.00140\n",
            "--------------------\n",
            "stock number: 1994\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00190\n",
            "Epoch [20/200], Loss: 0.00315\n",
            "Epoch [30/200], Loss: 0.00259\n",
            "Epoch [40/200], Loss: 0.00148\n",
            "Epoch [50/200], Loss: 0.00046\n",
            "Epoch [60/200], Loss: 0.00031\n",
            "Epoch [70/200], Loss: 0.00055\n",
            "Epoch [80/200], Loss: 0.00027\n",
            "Epoch [90/200], Loss: 0.00068\n",
            "Epoch [100/200], Loss: 0.00024\n",
            "Epoch [110/200], Loss: 0.00026\n",
            "Epoch [120/200], Loss: 0.00056\n",
            "Epoch [130/200], Loss: 0.00036\n",
            "Epoch [140/200], Loss: 0.00063\n",
            "Epoch [150/200], Loss: 0.00028\n",
            "Epoch [160/200], Loss: 0.00079\n",
            "Epoch [170/200], Loss: 0.00014\n",
            "Epoch [180/200], Loss: 0.00049\n",
            "Epoch [190/200], Loss: 0.00061\n",
            "Epoch [200/200], Loss: 0.00022\n",
            "--------------------\n",
            "stock number: 1995\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00211\n",
            "Epoch [20/200], Loss: 0.00066\n",
            "Epoch [30/200], Loss: 0.00041\n",
            "Epoch [40/200], Loss: 0.00054\n",
            "Epoch [50/200], Loss: 0.00209\n",
            "Epoch [60/200], Loss: 0.00028\n",
            "Epoch [70/200], Loss: 0.00024\n",
            "Epoch [80/200], Loss: 0.00064\n",
            "Epoch [90/200], Loss: 0.00027\n",
            "Epoch [100/200], Loss: 0.00035\n",
            "Epoch [110/200], Loss: 0.00017\n",
            "Epoch [120/200], Loss: 0.00135\n",
            "Epoch [130/200], Loss: 0.00029\n",
            "Epoch [140/200], Loss: 0.00044\n",
            "Epoch [150/200], Loss: 0.00051\n",
            "Epoch [160/200], Loss: 0.00042\n",
            "Epoch [170/200], Loss: 0.00039\n",
            "Epoch [180/200], Loss: 0.00021\n",
            "Epoch [190/200], Loss: 0.00031\n",
            "Epoch [200/200], Loss: 0.00056\n",
            "--------------------\n",
            "stock number: 1996\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00062\n",
            "Epoch [20/200], Loss: 0.00062\n",
            "Epoch [30/200], Loss: 0.00062\n",
            "Epoch [40/200], Loss: 0.00062\n",
            "Epoch [50/200], Loss: 0.00062\n",
            "Epoch [60/200], Loss: 0.00062\n",
            "Epoch [70/200], Loss: 0.00062\n",
            "Epoch [80/200], Loss: 0.00059\n",
            "Epoch [90/200], Loss: 0.00060\n",
            "Epoch [100/200], Loss: 0.00063\n",
            "Epoch [110/200], Loss: 0.00062\n",
            "Epoch [120/200], Loss: 0.00068\n",
            "Epoch [130/200], Loss: 0.00064\n",
            "Epoch [140/200], Loss: 0.00062\n",
            "Epoch [150/200], Loss: 0.00045\n",
            "Epoch [160/200], Loss: 0.00053\n",
            "Epoch [170/200], Loss: 0.00048\n",
            "Epoch [180/200], Loss: 0.00052\n",
            "Epoch [190/200], Loss: 0.00031\n",
            "Epoch [200/200], Loss: 0.00048\n",
            "--------------------\n",
            "stock number: 1997\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.03200\n",
            "Epoch [20/200], Loss: 0.00855\n",
            "Epoch [30/200], Loss: 0.00223\n",
            "Epoch [40/200], Loss: 0.00192\n",
            "Epoch [50/200], Loss: 0.00117\n",
            "Epoch [60/200], Loss: 0.00108\n",
            "Epoch [70/200], Loss: 0.00113\n",
            "Epoch [80/200], Loss: 0.00093\n",
            "Epoch [90/200], Loss: 0.00080\n",
            "Epoch [100/200], Loss: 0.00134\n",
            "Epoch [110/200], Loss: 0.00062\n",
            "Epoch [120/200], Loss: 0.00124\n",
            "Epoch [130/200], Loss: 0.00179\n",
            "Epoch [140/200], Loss: 0.00139\n",
            "Epoch [150/200], Loss: 0.00066\n",
            "Epoch [160/200], Loss: 0.00124\n",
            "Epoch [170/200], Loss: 0.00056\n",
            "Epoch [180/200], Loss: 0.00042\n",
            "Epoch [190/200], Loss: 0.00070\n",
            "Epoch [200/200], Loss: 0.00040\n",
            "--------------------\n",
            "stock number: 1998\n",
            "--------------------\n",
            "pca n_components_ 2\n",
            "sequences shape:  torch.Size([469, 60, 2])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00036\n",
            "Epoch [20/200], Loss: 0.00036\n",
            "Epoch [30/200], Loss: 0.00036\n",
            "Epoch [40/200], Loss: 0.00036\n",
            "Epoch [50/200], Loss: 0.00036\n",
            "Epoch [60/200], Loss: 0.00036\n",
            "Epoch [70/200], Loss: 0.00036\n",
            "Epoch [80/200], Loss: 0.00036\n",
            "Epoch [90/200], Loss: 0.00036\n",
            "Epoch [100/200], Loss: 0.00036\n",
            "Epoch [110/200], Loss: 0.00036\n",
            "Epoch [120/200], Loss: 0.00036\n",
            "Epoch [130/200], Loss: 0.00036\n",
            "Epoch [140/200], Loss: 0.00036\n",
            "Epoch [150/200], Loss: 0.00036\n",
            "Epoch [160/200], Loss: 0.00036\n",
            "Epoch [170/200], Loss: 0.00036\n",
            "Epoch [180/200], Loss: 0.00036\n",
            "Epoch [190/200], Loss: 0.00036\n",
            "Epoch [200/200], Loss: 0.00036\n",
            "--------------------\n",
            "stock number: 1999\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.05217\n",
            "Epoch [20/200], Loss: 0.01710\n",
            "Epoch [30/200], Loss: 0.02242\n",
            "Epoch [40/200], Loss: 0.00955\n",
            "Epoch [50/200], Loss: 0.00816\n",
            "Epoch [60/200], Loss: 0.00557\n",
            "Epoch [70/200], Loss: 0.00802\n",
            "Epoch [80/200], Loss: 0.00385\n",
            "Epoch [90/200], Loss: 0.00312\n",
            "Epoch [100/200], Loss: 0.00456\n",
            "Epoch [110/200], Loss: 0.00248\n",
            "Epoch [120/200], Loss: 0.00212\n",
            "Epoch [130/200], Loss: 0.00263\n",
            "Epoch [140/200], Loss: 0.00184\n",
            "Epoch [150/200], Loss: 0.00185\n",
            "Epoch [160/200], Loss: 0.00365\n",
            "Epoch [170/200], Loss: 0.00143\n",
            "Epoch [180/200], Loss: 0.00161\n",
            "Epoch [190/200], Loss: 0.00117\n",
            "Epoch [200/200], Loss: 0.00297\n",
            "--------------------\n",
            "stock number: 2000\n",
            "--------------------\n",
            "pca n_components_ 3\n",
            "sequences shape:  torch.Size([469, 60, 3])\n",
            "targets shape:  torch.Size([469, 15])\n",
            "Epoch [10/200], Loss: 0.00322\n",
            "Epoch [20/200], Loss: 0.00245\n",
            "Epoch [30/200], Loss: 0.00247\n",
            "Epoch [40/200], Loss: 0.01050\n",
            "Epoch [50/200], Loss: 0.00089\n",
            "Epoch [60/200], Loss: 0.00200\n",
            "Epoch [70/200], Loss: 0.00071\n",
            "Epoch [80/200], Loss: 0.00074\n",
            "Epoch [90/200], Loss: 0.00077\n",
            "Epoch [100/200], Loss: 0.00060\n",
            "Epoch [110/200], Loss: 0.00136\n",
            "Epoch [120/200], Loss: 0.00167\n",
            "Epoch [130/200], Loss: 0.00064\n",
            "Epoch [140/200], Loss: 0.00066\n",
            "Epoch [150/200], Loss: 0.00098\n",
            "Epoch [160/200], Loss: 0.00047\n",
            "Epoch [170/200], Loss: 0.00042\n",
            "Epoch [180/200], Loss: 0.00043\n",
            "Epoch [190/200], Loss: 0.00053\n",
            "Epoch [200/200], Loss: 0.00249\n"
          ]
        }
      ],
      "source": [
        "prediction_window = 15\n",
        "window_size = 60\n",
        "\n",
        "num_epochs = 200 # 100\n",
        "learning_rate = 0.001 # 0.0001\n",
        "batch_size = 64 # 32\n",
        "\n",
        "hidden_size = 64 # 16, 32, 50, 64 # biLSTM->16 * 2 = 32 32 * 2 64 장기 종속성을 캡처해야 하는 경우 더 큰 hidden_size가 필요할 수 있음 128, 256\n",
        "n_layers = 3\n",
        "dropout = 0.1 # 데이터 세트가 큰 경우 과적합이 발생할 가능성이 적고 dropout이 필요하지 않을 수도 있음, 본 데이터는 item_Code별로 split을 하였기에 데이터 세트가 작음, 따라서 dropout이 필요할 수 있음\n",
        "\n",
        "model_predictions = []\n",
        "returns_dailys = []\n",
        "max_drawdowns = []\n",
        "\n",
        "item_Code_list = train['item_Code'].unique().tolist()\n",
        "item_Name_list = train['item_Name'].unique().tolist()\n",
        "\n",
        "for j in range(0, len(stacked_vae_filter_data)):\n",
        "  print('-' * 20)\n",
        "  print(f'stock number: {j+1}')\n",
        "  print('-' * 20)\n",
        "  item_set = stacked_vae_filter_data[j]\n",
        "  #print(item_set)\n",
        "\n",
        "  sequences = []\n",
        "  targets = []\n",
        "  if len(item_set.shape) < 2:\n",
        "\n",
        "    print(f'{item_Code_list[j]} is zero')\n",
        "\n",
        "    #scaler = StandardScaler()\n",
        "    #scaler = RobustScaler()\n",
        "    scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "    item_set = scaler.fit_transform(item_set.detach().numpy().reshape(-1, 1))\n",
        "    item_set = torch.tensor(item_set, dtype = torch.float32)\n",
        "\n",
        "    item_set = item_set.squeeze()\n",
        "\n",
        "    for i in range(len(item_set) - window_size - prediction_window + 1):\n",
        "      sequences.append(item_set[i: i + window_size])\n",
        "      targets.append(item_set[i + window_size: i + window_size + prediction_window])\n",
        "\n",
        "    input_size = 1\n",
        "\n",
        "    sequences = torch.stack(sequences)\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    #scaler = StandardScaler()\n",
        "    #scaler = RobustScaler()\n",
        "    #scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "    #normalized_targets = scaler.fit_transform(targets.reshape(-1, 1))\n",
        "    #normalized_targets = normalized_targets.reshape(-1, prediction_window)\n",
        "    #normalized_targets = torch.tensor(normalized_targets, dtype = torch.float32)\n",
        "\n",
        "    print('sequences shape: ', sequences.shape)\n",
        "    print('targets shape: ', targets.shape)\n",
        "\n",
        "    # train_size = int(0.8 * len(sequences))\n",
        "    # train_sequences, test_sequences = sequences[: train_size], sequences[train_size: ]\n",
        "    # train_targets, test_targets =  targets[: train_size], targets[train_size: ]\n",
        "\n",
        "    train_dataset = TensorDataset(sequences, targets)\n",
        "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    output_size = prediction_window\n",
        "    #model = LSTM(input_size, hidden_size, n_layers, dropout, output_size)\n",
        "    #model = BiLSTM(input_size, hidden_size, n_layers, dropout, output_size)\n",
        "    model = biGRU(input_size, hidden_size, n_layers, output_size)\n",
        "    #model = VanillaRNN(input_size, hidden_size, n_layers, dropout, output_size)\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      for inputs, targets in train_loader:\n",
        "        inputs = inputs.unsqueeze(2)\n",
        "        targets = targets.unsqueeze(2)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets.squeeze())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.5f}')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "      last_sequences = sequences[-window_size: ]\n",
        "      last_sequences = last_sequences.to(device)\n",
        "      train_outputs = model(last_sequences.unsqueeze(2))\n",
        "\n",
        "    #train_outputs = train_outputs#.reshape(-1, output_size)\n",
        "    train_outputs = scaler.inverse_transform(train_outputs.cpu().numpy().reshape(-1, 1))\n",
        "    model_prediction = train_outputs.reshape(-1, output_size)[-1]\n",
        "    model_prediction = np.array(model_prediction)\n",
        "\n",
        "    # diff version\n",
        "    #close_diff = item_set[-1, -1] - model_prediction[0]\n",
        "    #model_prediction_diff_ver = model_prediction - close_diff\n",
        "\n",
        "    # all period close\n",
        "    #all_period_close = torch.cat((item_set[:, -1], torch.tensor(model_prediction)), dim = 0)\n",
        "    #all_period_close_table = pd.DataFrame({item_Name_list[j]: all_period_close})\n",
        "\n",
        "    # 최종 수익률 (Final Return)\n",
        "    returns_daily = (model_prediction[-1] - model_prediction[0]) / model_prediction[0]\n",
        "\n",
        "    ## MDD (Maximum Drawdown)\n",
        "    #cum_returns = np.cumprod(lstm_prediction + 1)\n",
        "    #max_drawdown = np.max(np.maximum.accumulate(cum_returns) - cum_returns)\n",
        "\n",
        "    model_predictions.append(model_prediction)\n",
        "    returns_dailys.append(returns_daily)\n",
        "    #max_drawdowns.append(max_drawdown)\n",
        "\n",
        "  else:\n",
        "    feats_item_set = item_set[:, :-1].detach().numpy()\n",
        "\n",
        "    # scaling\n",
        "    scaler = StandardScaler()\n",
        "    #scaler = RobustScaler()\n",
        "    #scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "    feats_item_set_scaled = scaler.fit_transform(feats_item_set)\n",
        "\n",
        "    # pca\n",
        "    pca = PCA(n_components=.8)\n",
        "    feats_item_set_pca = pca.fit_transform(feats_item_set_scaled)\n",
        "    print('pca n_components_', feats_item_set_pca.shape[1])\n",
        "\n",
        "    feats_item_set = torch.tensor(feats_item_set_pca, dtype = torch.float32)\n",
        "    item_set = torch.tensor(item_set, dtype = torch.float32)\n",
        "\n",
        "    for i in range(len(item_set) - window_size - prediction_window + 1):\n",
        "      sequences.append(feats_item_set[i: i + window_size, :])\n",
        "      targets.append(item_set[i + window_size: i + window_size + prediction_window, -1])\n",
        "\n",
        "    input_size = feats_item_set.shape[1]\n",
        "\n",
        "    sequences = torch.stack(sequences)\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    #scaler = StandardScaler()\n",
        "    #RobustScaler()\n",
        "    scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "    normalized_targets = scaler.fit_transform(targets.reshape(-1, 1))\n",
        "    normalized_targets = normalized_targets.reshape(-1, prediction_window)\n",
        "    normalized_targets = torch.tensor(normalized_targets, dtype = torch.float32)\n",
        "\n",
        "    print('sequences shape: ', sequences.shape)\n",
        "    print('targets shape: ', targets.shape)\n",
        "\n",
        "    # train_size = int(0.8 * len(sequences))\n",
        "    # train_sequences, test_sequences = sequences[: train_size], sequences[train_size: ]\n",
        "    # train_targets, test_targets =  targets[: train_size], targets[train_size: ]\n",
        "\n",
        "    train_dataset = TensorDataset(sequences, normalized_targets)\n",
        "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    output_size = prediction_window\n",
        "    #model = LSTM(input_size, hidden_size, n_layers, dropout, output_size)\n",
        "    #model = BiLSTM(input_size, hidden_size, n_layers, dropout, output_size)\n",
        "    model = biGRU(input_size, hidden_size, n_layers, output_size)\n",
        "    #model = VanillaRNN(input_size, hidden_size, n_layers, dropout, output_size)\n",
        "    model.to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "      for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.5f}')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "      last_sequences = sequences[-window_size: ]\n",
        "      last_sequences = last_sequences.to(device)\n",
        "      train_outputs = model(last_sequences)\n",
        "\n",
        "    #train_outputs = train_outputs#.reshape(-1, output_size)\n",
        "    train_outputs = scaler.inverse_transform(train_outputs.cpu().numpy().reshape(-1, 1))\n",
        "    model_prediction = train_outputs.reshape(-1, output_size)[-1]\n",
        "    model_prediction = np.array(model_prediction)\n",
        "\n",
        "    # diff version\n",
        "    #close_diff = item_set[-1, -1] - model_prediction[0]\n",
        "    #model_prediction_diff_ver = model_prediction - close_diff\n",
        "\n",
        "    # all period close\n",
        "    #all_period_close = torch.cat((item_set[:, -1], torch.tensor(model_prediction)), dim = 0)\n",
        "    #all_period_close_table = pd.DataFrame({item_Name_list[j]: all_period_close})\n",
        "\n",
        "    # 최종 수익률 (Final Return)\n",
        "    returns_daily = (model_prediction[-1] - model_prediction[0]) / model_prediction[0]\n",
        "\n",
        "    ## MDD (Maximum Drawdown)\n",
        "    #cum_returns = np.cumprod(model_prediction + 1)\n",
        "    #max_drawdown = np.max(np.maximum.accumulate(cum_returns) - cum_returns)\n",
        "\n",
        "    model_predictions.append(model_prediction)\n",
        "    returns_dailys.append(returns_daily)\n",
        "    #max_drawdowns.append(max_drawdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ndhQSupQTbGd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cadea33c-a7a1-4491-aba6-db49c9cd6926"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "np.array(model_predictions).shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(returns_dailys).shape"
      ],
      "metadata": {
        "id": "QGiQFvyoTlGi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c804e5ab-43f4-4996-8e26-b00b29bbe0a0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000,)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWnUtMxIa_jR"
      },
      "outputs": [],
      "source": [
        "# prediction_window = 15\n",
        "# window_size = 32\n",
        "\n",
        "# num_epochs = 100 # 1000\n",
        "# learning_rate = 0.001 # 0.0001\n",
        "# batch_size = 32 # 64\n",
        "\n",
        "# hidden_size = 64 # 16, 32, 50, 64 장기 종속성을 캡처해야 하는 경우 더 큰 hidden_size가 필요할 수 있음 128, 256\n",
        "# n_layers = 2\n",
        "# dropout = 0.1 # 데이터 세트가 큰 경우 과적합이 발생할 가능성이 적고 dropout이 필요하지 않을 수도 있음, 본 데이터는 item_Code별로 split을 하였기에 데이터 세트가 작음, 따라서 dropout이 필요할 수 있음\n",
        "\n",
        "# model_predictions = []\n",
        "# returns_dailys = []\n",
        "# max_drawdowns = []MINSU\n",
        "\n",
        "# item_Code_list = train['item_Code'].unique().tolist()\n",
        "# item_Name_list = train['item_Name'].unique().tolist()\n",
        "\n",
        "# for j in range(0, len(stacked_vae_filter_data)):\n",
        "#   print('-' * 20)\n",
        "#   print(f'stock number: {j+1}')\n",
        "#   print('-' * 20)\n",
        "#   item_set = stacked_vae_filter_data[j]\n",
        "#   #print(item_set)\n",
        "\n",
        "#   sequences = []\n",
        "#   targets = []\n",
        "#   if len(item_set.shape) < 2:\n",
        "\n",
        "#     print(f'{item_Code_list[j]} is zero')\n",
        "\n",
        "#     for i in range(len(item_set) - window_size - prediction_window + 1):\n",
        "#       sequences.append(item_set[i: i + window_size])\n",
        "#       targets.append(item_set[i + window_size: i + window_size + prediction_window])\n",
        "\n",
        "#     input_size = 1\n",
        "\n",
        "#     sequences = torch.stack(sequences)\n",
        "#     targets = torch.stack(targets)\n",
        "\n",
        "#     #scaler = StandardScaler()\n",
        "#     #scaler = RobustScaler()\n",
        "#     scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "#     normalized_targets = scaler.fit_transform(targets.reshape(-1, 1))\n",
        "#     normalized_targets = normalized_targets.reshape(-1, prediction_window)\n",
        "#     normalized_targets = torch.tensor(normalized_targets, dtype = torch.float32)\n",
        "\n",
        "#     print('sequences shape: ', sequences.shape)\n",
        "#     print('targets shape: ', targets.shape)\n",
        "\n",
        "#     # train_size = int(0.8 * len(sequences))\n",
        "#     # train_sequences, test_sequences = sequences[: train_size], sequences[train_size: ]\n",
        "#     # train_targets, test_targets =  targets[: train_size], targets[train_size: ]\n",
        "\n",
        "#     train_dataset = TensorDataset(sequences, normalized_targets)\n",
        "#     train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "#     output_size = prediction_window\n",
        "#     #model = LSTM(input_size, hidden_size, n_layers, dropout, output_size)\n",
        "#     #model = BiLSTM(input_size, hidden_size, n_layers, dropout, output_size)\n",
        "#     model = VanillaRNN(input_size, hidden_size, n_layers, dropout, output_size)\n",
        "#     model.to(device)\n",
        "#     criterion = nn.MSELoss()\n",
        "#     optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#       for inputs, targets in train_loader:\n",
        "#         inputs = inputs.unsqueeze(2)\n",
        "#         targets = targets.unsqueeze(2)\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, targets.squeeze())\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#       if (epoch + 1) % 10 == 0:\n",
        "#           print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.5f}')\n",
        "\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "\n",
        "#       last_sequences = sequences[-window_size: ]\n",
        "#       last_sequences = last_sequences.to(device)\n",
        "#       train_outputs = model(last_sequences.unsqueeze(2))\n",
        "\n",
        "#     #train_outputs = train_outputs#.reshape(-1, output_size)\n",
        "#     train_outputs = scaler.inverse_transform(train_outputs.cpu().numpy().reshape(-1, 1))\n",
        "#     model_prediction = train_outputs.reshape(-1, output_size)[-1]\n",
        "#     model_prediction = np.array(model_prediction)\n",
        "\n",
        "#     # all period close\n",
        "#     #all_period_close = torch.cat((item_set[:, -1], torch.tensor(model_prediction)), dim = 0)\n",
        "#     #all_period_close_table = pd.DataFrame({item_Name_list[j]: all_period_close})\n",
        "\n",
        "#     # 최종 수익률 (Final Return)\n",
        "#     returns_daily = (model_prediction[-1] - model_prediction[0]) / model_prediction[0]\n",
        "\n",
        "#     ## MDD (Maximum Drawdown)\n",
        "#     #cum_returns = np.cumprod(lstm_prediction + 1)\n",
        "#     #max_drawdown = np.max(np.maximum.accumulate(cum_returns) - cum_returns)\n",
        "\n",
        "#     model_predictions.append(model_prediction)\n",
        "#     returns_dailys.append(returns_daily)\n",
        "#     #max_drawdowns.append(max_drawdown)\n",
        "\n",
        "#   else:\n",
        "#     feats_item_set = item_set[:, :-1].detach().numpy()\n",
        "\n",
        "#     # scaling\n",
        "#     #scaler = StandardScaler()\n",
        "#     #scaler = RobustScaler()\n",
        "#     scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "#     feats_item_set_scaled = scaler.fit_transform(feats_item_set)\n",
        "\n",
        "#     # pca\n",
        "#     pca = PCA(n_components=.8)\n",
        "#     feats_item_set_pca = pca.fit_transform(feats_item_set_scaled)\n",
        "#     print('pca n_components_', feats_item_set_pca.shape[1])\n",
        "\n",
        "#     feats_item_set_pca = torch.tensor(feats_item_set_pca, dtype = torch.float32)\n",
        "#     item_set = torch.tensor(item_set, dtype = torch.float32)\n",
        "\n",
        "#     for i in range(len(item_set) - window_size - prediction_window + 1):\n",
        "#       sequences.append(feats_item_set_pca[i: i + window_size, :])\n",
        "#       targets.append(item_set[i + window_size: i + window_size + prediction_window, -1])\n",
        "\n",
        "#     input_size = feats_item_set_pca.shape[1]\n",
        "\n",
        "#     sequences = torch.stack(sequences)\n",
        "#     targets = torch.stack(targets)\n",
        "\n",
        "#     #scaler = StandardScaler()\n",
        "#     # RobustScaler()\n",
        "#     scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "#     normalized_targets = scaler.fit_transform(targets.reshape(-1, 1))\n",
        "#     normalized_targets = normalized_targets.reshape(-1, prediction_window)\n",
        "#     normalized_targets = torch.tensor(normalized_targets, dtype = torch.float32)\n",
        "\n",
        "#     print('sequences shape: ', sequences.shape)\n",
        "#     print('targets shape: ', targets.shape)\n",
        "\n",
        "#     # train_size = int(0.8 * len(sequences))\n",
        "#     # train_sequences, test_sequences = sequences[: train_size], sequences[train_size: ]\n",
        "#     # train_targets, test_targets =  targets[: train_size], targets[train_size: ]\n",
        "\n",
        "#     train_dataset = TensorDataset(sequences, normalized_targets)\n",
        "#     train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "#     output_size = prediction_window\n",
        "#     #model = LSTM(input_size, hidden_size, n_layers, dropout, output_size)\n",
        "#     #model = BiLSTM(input_size, hidden_size, n_layers, dropout, output_size)\n",
        "#     model = VanillaRNN(input_size, hidden_size, n_layers, dropout, output_size)\n",
        "#     model.to(device)\n",
        "#     criterion = nn.MSELoss()\n",
        "#     optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "#     for epoch in range(num_epochs):\n",
        "#       for inputs, targets in train_loader:\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#       if (epoch + 1) % 10 == 0:\n",
        "#           print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.5f}')\n",
        "\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "\n",
        "#       last_sequences = sequences[-window_size: ]\n",
        "#       last_sequences = last_sequences.to(device)\n",
        "#       train_outputs = model(last_sequences)\n",
        "\n",
        "#     #train_outputs = train_outputs#.reshape(-1, output_size)\n",
        "#     train_outputs = scaler.inverse_transform(train_outputs.cpu().numpy().reshape(-1, 1))\n",
        "#     model_prediction = train_outputs.reshape(-1, output_size)[-1]\n",
        "#     model_prediction = np.array(model_prediction)\n",
        "\n",
        "#     # all period close\n",
        "#     #all_period_close = torch.cat((item_set[:, -1], torch.tensor(model_prediction)), dim = 0)\n",
        "#     #all_period_close_table = pd.DataFrame({item_Name_list[j]: all_period_close})\n",
        "\n",
        "#     # 최종 수익률 (Final Return)\n",
        "#     returns_daily = (model_prediction[-1] - model_prediction[0]) / model_prediction[0]\n",
        "\n",
        "#     ## MDD (Maximum Drawdown)\n",
        "#     #cum_returns = np.cumprod(model_prediction + 1)\n",
        "#     #max_drawdown = np.max(np.maximum.accumulate(cum_returns) - cum_returns)\n",
        "\n",
        "#     model_predictions.append(model_prediction)\n",
        "#     returns_dailys.append(returns_daily)\n",
        "#     #max_drawdowns.append(max_drawdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rFJMQ2LYyvvF"
      },
      "outputs": [],
      "source": [
        "np.save('model_predictions_final_private.npy', model_predictions)\n",
        "np.save('returns_dailys_final_private.npy', returns_dailys)\n",
        "np.save('max_drawdowns_final_private.npy', max_drawdowns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "e9xvL9nqzIoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce5a742-d79c-4df5-daca-c689461e4d77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2000, 15), (2000,), (0,))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "np.array(model_predictions).shape, np.array(returns_dailys).shape, np.array(max_drawdowns).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "W3qjiGQen_R4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "b13f6769-d15f-4ada-ded4-05865235e196"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(range(0, 543), np.array(train.loc[train['item_Code'] == 'A000020', 'Close']), label = 'train')\n",
        "plt.plot(range(543, 558), model_predictions[0], label = 'predict')\n",
        "plt.title(f'A000020')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "|step_size = 15\n",
        "linear_predictions = []\n",
        "item_Code_list = train.item_Code.unique().tolist()\n",
        "\n",
        "for n, item_Code in enumerate(train['item_Code'].unique().tolist()):\n",
        "\n",
        "  print('-' * 20)\n",
        "  print(f'count: {n}, item_Code: {item_Code}')\n",
        "  print('-' * 20)\n",
        "\n",
        "  item_set = train[train['item_Code'] == item_Code]\n",
        "  item_set = item_set.drop(['Date', 'item_Code', 'item_Name', 'ARIMA_close'], axis = 1)\n",
        "\n",
        "  print('Before feature importances', item_set.shape)\n",
        "\n",
        "  # if item_Code in zero_item_Code:\n",
        "  #   print(f'{item_Code} is zero')\n",
        "  #   stacked_vae_filter_data.append(torch.Tensor(item_set['Close2'].values))\n",
        "  #   continue\n",
        "\n",
        "  #item_set = item_set.astype(float)\n",
        "  item_set = item_set.fillna(method = 'ffill')\n",
        "  item_set = item_set.fillna(method = 'bfill')\n",
        "  print(item_set.isnull().sum()[item_set.isnull().sum() > 0])\n",
        "  columns_list = item_set.columns.tolist()\n",
        "\n",
        "  # scaling\n",
        "  #scaler = StandardScaler()\n",
        "  #scaler = RobustScaler()\n",
        "  #scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "  #scaler = scaler.fit(item_set.drop(['Close2'], axis = 1))\n",
        "  #item_set_scaled = scaler.transform(item_set.drop(['Close2'], axis = 1))\n",
        "  #print(item_set_scaled.shape, item_set['Close2'].shape)\n",
        "  #item_set_scaled = np.hstack((item_set_scaled, np.array(item_set['Close2']).reshape(-1, 1)))\n",
        "  #item_set_scaled_df = pd.DataFrame(item_set_scaled, columns = columns_list)\n",
        "\n",
        "  (X_train_fi, y_train_fi), (X_test_fi, y_test_fi) = train_test_split(item_set, 'Close2')\n",
        "  regressor = xgb.XGBRegressor()\n",
        "  regressor = regressor.fit(X_train_fi, y_train_fi, eval_set = [(X_train_fi, y_train_fi), (X_test_fi, y_test_fi)], verbose = False)\n",
        "\n",
        "  fi = X_train_fi.columns[np.array(regressor.feature_importances_) > 0.0].tolist()\n",
        "  fi.append('Close2')\n",
        "  #print('len:', len(fi))\n",
        "\n",
        "  item_set = train[train['item_Code'] == item_Code]\n",
        "  item_set = item_set[list(set(fi))]\n",
        "\n",
        "  print('after feature importances', item_set.shape)\n",
        "\n",
        "  item_set = item_set.astype(float)\n",
        "  item_set = item_set.fillna(method = 'ffill')\n",
        "  item_set = item_set.fillna(method = 'bfill')\n",
        "  print(item_set.isnull().sum()[item_set.isnull().sum() > 0])\n",
        "\n",
        "  X = item_set.drop(['Close2'], axis = 1)\n",
        "  y = item_set['Close2']\n",
        "\n",
        "  if item_set.shape[1] < 2:\n",
        "\n",
        "    print(f'{item_Code} is zero')\n",
        "\n",
        "    y_test = y[-step_size:]\n",
        "    test_pred = y_test\n",
        "    linear_predictions.append(test_pred)\n",
        "\n",
        "  else:\n",
        "    X_train = X[:-2*step_size]\n",
        "    X_validation = X[-2*step_size:-step_size]\n",
        "    X_test = X[-step_size:]\n",
        "\n",
        "    #scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "    #y = scaler.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "    y_train = y[:-2*step_size]\n",
        "    y_validation = y[-2*step_size:-step_size]\n",
        "    y_test = y[-step_size:]\n",
        "\n",
        "    model = lm.LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    val_pred = model.predict(X_validation)\n",
        "\n",
        "    print(MAE(val_pred, y_validation))\n",
        "    print(MAPE(val_pred, y_validation))\n",
        "    print(RMSE(val_pred, y_validation))\n",
        "\n",
        "    model.fit(pd.concat([X_train, X_validation], axis = 0), pd.concat([y_train, y_validation], axis = 0))\n",
        "    test_pred = model.predict(X_test)\n",
        "    #test_pred = scaler.inverse_transform(test_pred)\n",
        "    linear_predictions.append(test_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzjNH_V7vzm0",
        "outputId": "7d7ce320-b500-4d05-a564-a9b96433e47e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "--------------------\n",
            "count: 1499, item_Code: A153490\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.012387842605357946\n",
            "0.0010546620043890061\n",
            "0.015224531158486909\n",
            "--------------------\n",
            "count: 1500, item_Code: A153710\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.02260397415187375\n",
            "0.0002824034967012696\n",
            "0.02479565824112596\n",
            "--------------------\n",
            "count: 1501, item_Code: A155650\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0027402558909064584\n",
            "4.2500542472530054e-05\n",
            "0.0031958866297720195\n",
            "--------------------\n",
            "count: 1502, item_Code: A155660\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.02949276450211376\n",
            "0.000564166299480086\n",
            "0.030314709132387846\n",
            "--------------------\n",
            "count: 1503, item_Code: A156100\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0017878260792713262\n",
            "2.4139503412033305e-05\n",
            "0.0020694343319531162\n",
            "--------------------\n",
            "count: 1504, item_Code: A158430\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0006868341833372445\n",
            "3.367855170405759e-06\n",
            "0.0007516387167648188\n",
            "--------------------\n",
            "count: 1505, item_Code: A159580\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.0012480148830945836\n",
            "1.561314939024497e-05\n",
            "0.0014677635832517004\n",
            "--------------------\n",
            "count: 1506, item_Code: A159910\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 95)\n",
            "Series([], dtype: int64)\n",
            "0.018357005104767645\n",
            "0.010610305801128381\n",
            "0.021699284034491215\n",
            "--------------------\n",
            "count: 1507, item_Code: A160550\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0003922913595791518\n",
            "5.93215170726121e-06\n",
            "0.00042497471724663936\n",
            "--------------------\n",
            "count: 1508, item_Code: A160600\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 1)\n",
            "Series([], dtype: int64)\n",
            "A160600 is zero\n",
            "--------------------\n",
            "count: 1509, item_Code: A160980\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.0018995471891685156\n",
            "1.4979120318191901e-05\n",
            "0.0021798896270551725\n",
            "--------------------\n",
            "count: 1510, item_Code: A161000\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.05971668042426851\n",
            "0.0003498849706990255\n",
            "0.07162095025610551\n",
            "--------------------\n",
            "count: 1511, item_Code: A161390\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.0016382347171505292\n",
            "4.693733222850085e-06\n",
            "0.0019704059476779692\n",
            "--------------------\n",
            "count: 1512, item_Code: A161580\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.02984705515979537\n",
            "0.0002985772557632862\n",
            "0.040258817558834314\n",
            "--------------------\n",
            "count: 1513, item_Code: A161890\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.012039493815003273\n",
            "2.9886268159734897e-05\n",
            "0.014337036024334995\n",
            "--------------------\n",
            "count: 1514, item_Code: A163560\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.009877864972258977\n",
            "0.00015861303852516862\n",
            "0.011109290864845724\n",
            "--------------------\n",
            "count: 1515, item_Code: A163730\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.002463277065544389\n",
            "1.9796981528466473e-05\n",
            "0.00309744639607792\n",
            "--------------------\n",
            "count: 1516, item_Code: A164060\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.00506644577866003\n",
            "7.361372109499097e-05\n",
            "0.006240343253329784\n",
            "--------------------\n",
            "count: 1517, item_Code: A166090\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.003003900730982423\n",
            "7.006843490740063e-06\n",
            "0.003322424303039131\n",
            "--------------------\n",
            "count: 1518, item_Code: A166480\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.008646032680492985\n",
            "0.0001074610072925503\n",
            "0.00951488580697363\n",
            "--------------------\n",
            "count: 1519, item_Code: A168330\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.0017097904380837766\n",
            "5.828055466455624e-05\n",
            "0.0025040377812146483\n",
            "--------------------\n",
            "count: 1520, item_Code: A169330\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.06098671238478346\n",
            "0.001770574379601119\n",
            "0.06368454989906511\n",
            "--------------------\n",
            "count: 1521, item_Code: A170030\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.00850730413888717\n",
            "9.988560570992958e-05\n",
            "0.011149511025976843\n",
            "--------------------\n",
            "count: 1522, item_Code: A170790\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0024604064823506634\n",
            "1.7003201979305534e-05\n",
            "0.0027538435894973214\n",
            "--------------------\n",
            "count: 1523, item_Code: A170900\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0010700251989571067\n",
            "1.89788592424919e-06\n",
            "0.0011980243895053388\n",
            "--------------------\n",
            "count: 1524, item_Code: A170920\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.005263518723586458\n",
            "4.368759928989199e-05\n",
            "0.006212623099622033\n",
            "--------------------\n",
            "count: 1525, item_Code: A171010\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.0020739644000059344\n",
            "3.430983179419311e-05\n",
            "0.0022639141110840006\n",
            "--------------------\n",
            "count: 1526, item_Code: A171090\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 116)\n",
            "Series([], dtype: int64)\n",
            "0.0032299511675470665\n",
            "9.191546491269009e-06\n",
            "0.0037375813552089957\n",
            "--------------------\n",
            "count: 1527, item_Code: A171120\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.02721403333859295\n",
            "0.0003495278188033587\n",
            "0.03308465035027403\n",
            "--------------------\n",
            "count: 1528, item_Code: A173130\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.10746013674142887\n",
            "0.0008176307879567309\n",
            "0.11258701314606265\n",
            "--------------------\n",
            "count: 1529, item_Code: A173940\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0029895027856885765\n",
            "5.5676966270380326e-05\n",
            "0.0037478855632399875\n",
            "--------------------\n",
            "count: 1530, item_Code: A174900\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.021684697823851216\n",
            "0.00011167443951000166\n",
            "0.023463029324757842\n",
            "--------------------\n",
            "count: 1531, item_Code: A175140\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.004242337869375963\n",
            "5.1001488826476586e-05\n",
            "0.00557616705704157\n",
            "--------------------\n",
            "count: 1532, item_Code: A175250\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.001564889712214305\n",
            "4.8774325760320084e-05\n",
            "0.001768782920225965\n",
            "--------------------\n",
            "count: 1533, item_Code: A175330\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.026476246052213052\n",
            "0.0003118613678583362\n",
            "0.033952053093133365\n",
            "--------------------\n",
            "count: 1534, item_Code: A177350\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0018594701773205695\n",
            "3.972377572570834e-05\n",
            "0.002037510448531008\n",
            "--------------------\n",
            "count: 1535, item_Code: A177830\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.01366972954050046\n",
            "0.00030428443701986186\n",
            "0.01462845812435788\n",
            "--------------------\n",
            "count: 1536, item_Code: A178320\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0025932692444863886\n",
            "1.5717920372131242e-05\n",
            "0.003433222460415591\n",
            "--------------------\n",
            "count: 1537, item_Code: A178780\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 1)\n",
            "Series([], dtype: int64)\n",
            "A178780 is zero\n",
            "--------------------\n",
            "count: 1538, item_Code: A178920\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0019969155623887976\n",
            "6.025115884617593e-06\n",
            "0.00219337316530737\n",
            "--------------------\n",
            "count: 1539, item_Code: A179290\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 118)\n",
            "Series([], dtype: int64)\n",
            "0.03859529015802157\n",
            "0.0002780752815203658\n",
            "0.04513337281812826\n",
            "--------------------\n",
            "count: 1540, item_Code: A179900\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 103)\n",
            "Series([], dtype: int64)\n",
            "0.0006706360957953924\n",
            "2.931205597076308e-06\n",
            "0.0008455230686423299\n",
            "--------------------\n",
            "count: 1541, item_Code: A180400\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 1)\n",
            "Series([], dtype: int64)\n",
            "A180400 is zero\n",
            "--------------------\n",
            "count: 1542, item_Code: A180640\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.014355423965025694\n",
            "3.329594867632178e-05\n",
            "0.015371626214631359\n",
            "--------------------\n",
            "count: 1543, item_Code: A181340\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 102)\n",
            "Series([], dtype: int64)\n",
            "9.521373188666378e-05\n",
            "3.519916251597496e-06\n",
            "0.0001149708478561106\n",
            "--------------------\n",
            "count: 1544, item_Code: A181710\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0006057314948217633\n",
            "2.2364697792958634e-06\n",
            "0.0006739947601920729\n",
            "--------------------\n",
            "count: 1545, item_Code: A182360\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.005351678280082221\n",
            "2.1959046894691392e-05\n",
            "0.005924956040679407\n",
            "--------------------\n",
            "count: 1546, item_Code: A182400\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 115)\n",
            "Series([], dtype: int64)\n",
            "0.0008633616920027029\n",
            "7.334699488574697e-06\n",
            "0.0008900870998588838\n",
            "--------------------\n",
            "count: 1547, item_Code: A183190\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.0007747808405838442\n",
            "7.551598161656499e-06\n",
            "0.0008213109681018628\n",
            "--------------------\n",
            "count: 1548, item_Code: A183300\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0009040307006216608\n",
            "2.0455670102287124e-06\n",
            "0.0009311697519417997\n",
            "--------------------\n",
            "count: 1549, item_Code: A183490\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.00196103411083944\n",
            "0.0001071741999393595\n",
            "0.002103176199446472\n",
            "--------------------\n",
            "count: 1550, item_Code: A184230\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.012268211007532651\n",
            "0.0012714310341876236\n",
            "0.015482561539460139\n",
            "--------------------\n",
            "count: 1551, item_Code: A185490\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.00331044450316161\n",
            "7.159120734411413e-05\n",
            "0.0033683026898603608\n",
            "--------------------\n",
            "count: 1552, item_Code: A185750\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0014150304913831254\n",
            "1.664416778764493e-06\n",
            "0.0016327153887086617\n",
            "--------------------\n",
            "count: 1553, item_Code: A186230\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0007098212485288968\n",
            "6.755533015066255e-06\n",
            "0.0008689451890464177\n",
            "--------------------\n",
            "count: 1554, item_Code: A187220\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.006134474115485015\n",
            "0.0001411667772068047\n",
            "0.006841157912219663\n",
            "--------------------\n",
            "count: 1555, item_Code: A187270\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.0037407008360484423\n",
            "7.8771581038903e-05\n",
            "0.003866287002070025\n",
            "--------------------\n",
            "count: 1556, item_Code: A187420\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.002060013644404535\n",
            "4.460136138220397e-05\n",
            "0.0026012206832531568\n",
            "--------------------\n",
            "count: 1557, item_Code: A187870\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 101)\n",
            "Series([], dtype: int64)\n",
            "0.001421629512575843\n",
            "8.022208754076581e-06\n",
            "0.0017581338731187275\n",
            "--------------------\n",
            "count: 1558, item_Code: A189300\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0077961359124553075\n",
            "1.0570161923440817e-05\n",
            "0.009258018107604939\n",
            "--------------------\n",
            "count: 1559, item_Code: A189330\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0023637242024657705\n",
            "1.4368148850064907e-05\n",
            "0.00312480284101443\n",
            "--------------------\n",
            "count: 1560, item_Code: A189690\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.009826126697832175\n",
            "0.00013350408027154413\n",
            "0.01034963443864491\n",
            "--------------------\n",
            "count: 1561, item_Code: A189860\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0009733068371739743\n",
            "1.506806841535867e-05\n",
            "0.0010412964814287779\n",
            "--------------------\n",
            "count: 1562, item_Code: A189980\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0026061381562916116\n",
            "0.00010089953627792755\n",
            "0.003294179334678729\n",
            "--------------------\n",
            "count: 1563, item_Code: A190510\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0009188110427203356\n",
            "5.994290638131998e-06\n",
            "0.0011561335027458406\n",
            "--------------------\n",
            "count: 1564, item_Code: A190650\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0007167681780022879\n",
            "9.09883282529631e-06\n",
            "0.0007595579075405895\n",
            "--------------------\n",
            "count: 1565, item_Code: A191420\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.00239554294203117\n",
            "1.7851417684441297e-05\n",
            "0.0025799679907071325\n",
            "--------------------\n",
            "count: 1566, item_Code: A192080\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.014629671400083073\n",
            "3.161383577897185e-05\n",
            "0.01809888623246708\n",
            "--------------------\n",
            "count: 1567, item_Code: A192250\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.002750888138401327\n",
            "0.000175296651723623\n",
            "0.003649900022290243\n",
            "--------------------\n",
            "count: 1568, item_Code: A192400\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.004062840154074365\n",
            "2.6249198977415123e-05\n",
            "0.004148904086108799\n",
            "--------------------\n",
            "count: 1569, item_Code: A192410\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.007289644457599328\n",
            "0.001071902523270701\n",
            "0.007812184608732046\n",
            "--------------------\n",
            "count: 1570, item_Code: A192440\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.0004287168875938126\n",
            "1.3443955760868118e-06\n",
            "0.0005154677049747305\n",
            "--------------------\n",
            "count: 1571, item_Code: A192650\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0066893864619487434\n",
            "6.999186893313114e-05\n",
            "0.007949227707624887\n",
            "--------------------\n",
            "count: 1572, item_Code: A192820\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0012810530160398534\n",
            "1.6712700404772107e-06\n",
            "0.001712666326354567\n",
            "--------------------\n",
            "count: 1573, item_Code: A193250\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.009168223037689435\n",
            "0.0008234253944010438\n",
            "0.009838686909914369\n",
            "--------------------\n",
            "count: 1574, item_Code: A194370\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.0034738124419770124\n",
            "2.6596901088257166e-05\n",
            "0.0036940386260895853\n",
            "--------------------\n",
            "count: 1575, item_Code: A194480\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 103)\n",
            "Series([], dtype: int64)\n",
            "0.00039345229937074084\n",
            "6.934647641088644e-07\n",
            "0.0004258379075050981\n",
            "--------------------\n",
            "count: 1576, item_Code: A194700\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.002094812887420024\n",
            "2.340976223606512e-05\n",
            "0.0023484851697855944\n",
            "--------------------\n",
            "count: 1577, item_Code: A195500\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.00250391033341657\n",
            "7.620622235329486e-05\n",
            "0.0031427843257065993\n",
            "--------------------\n",
            "count: 1578, item_Code: A195870\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0037121228577840764\n",
            "7.894194481094734e-06\n",
            "0.004266366257920871\n",
            "--------------------\n",
            "count: 1579, item_Code: A195990\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.015303375375492579\n",
            "0.002702001962287541\n",
            "0.0172385541518103\n",
            "--------------------\n",
            "count: 1580, item_Code: A196170\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0006602523018955253\n",
            "1.4483370771372213e-06\n",
            "0.0007312163138026933\n",
            "--------------------\n",
            "count: 1581, item_Code: A196300\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.01019397198203175\n",
            "8.508312652189199e-05\n",
            "0.010971359382623967\n",
            "--------------------\n",
            "count: 1582, item_Code: A196450\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.029846294246650966\n",
            "0.001382975278558671\n",
            "0.03518644138118903\n",
            "--------------------\n",
            "count: 1583, item_Code: A196490\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0011496242822128502\n",
            "2.431769073774538e-05\n",
            "0.0014563199090764185\n",
            "--------------------\n",
            "count: 1584, item_Code: A197140\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.003009782495064428\n",
            "5.963705748455791e-05\n",
            "0.0032128203964224794\n",
            "--------------------\n",
            "count: 1585, item_Code: A198440\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.009603222711302806\n",
            "0.0003865925400436781\n",
            "0.010545793940210402\n",
            "--------------------\n",
            "count: 1586, item_Code: A199800\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.005710945227959504\n",
            "1.1960010746361753e-05\n",
            "0.006388506039442627\n",
            "--------------------\n",
            "count: 1587, item_Code: A199820\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 102)\n",
            "Series([], dtype: int64)\n",
            "0.014589692625789515\n",
            "0.0001299767907409577\n",
            "0.015385748375216897\n",
            "--------------------\n",
            "count: 1588, item_Code: A200130\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 103)\n",
            "Series([], dtype: int64)\n",
            "0.00534621807091753\n",
            "2.3522642243484583e-05\n",
            "0.006208473252784008\n",
            "--------------------\n",
            "count: 1589, item_Code: A200230\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.007219984431564323\n",
            "0.0006417580248122495\n",
            "0.008739097527968054\n",
            "--------------------\n",
            "count: 1590, item_Code: A200350\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 116)\n",
            "Series([], dtype: int64)\n",
            "0.0030720327675226146\n",
            "1.604450522180623e-05\n",
            "0.0033963096356810748\n",
            "--------------------\n",
            "count: 1591, item_Code: A200470\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.010647916614834685\n",
            "0.0003745413435208852\n",
            "0.010879713874135067\n",
            "--------------------\n",
            "count: 1592, item_Code: A200670\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.013786654260669214\n",
            "5.11043433044138e-05\n",
            "0.015953374043743297\n",
            "--------------------\n",
            "count: 1593, item_Code: A200710\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0053401377008412965\n",
            "2.794521493017046e-05\n",
            "0.006578259866525542\n",
            "--------------------\n",
            "count: 1594, item_Code: A200780\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 102)\n",
            "Series([], dtype: int64)\n",
            "0.004965335194841222\n",
            "7.308127320384013e-05\n",
            "0.005091158240249708\n",
            "--------------------\n",
            "count: 1595, item_Code: A200880\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.03486433326130888\n",
            "0.00024013252277858897\n",
            "0.03938105371720687\n",
            "--------------------\n",
            "count: 1596, item_Code: A201490\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.009119962168551866\n",
            "0.00024135887388349064\n",
            "0.010413397037755045\n",
            "--------------------\n",
            "count: 1597, item_Code: A203400\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.012633774339337833\n",
            "0.00014440533325363206\n",
            "0.013974129475574684\n",
            "--------------------\n",
            "count: 1598, item_Code: A203450\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.00573250569659649\n",
            "0.0001776534348952063\n",
            "0.007128670021009404\n",
            "--------------------\n",
            "count: 1599, item_Code: A203650\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.003807848851010931\n",
            "0.00012714621597866442\n",
            "0.005234581305085646\n",
            "--------------------\n",
            "count: 1600, item_Code: A203690\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.3231910790926198\n",
            "0.0366289667242258\n",
            "0.33022955973137197\n",
            "--------------------\n",
            "count: 1601, item_Code: A204020\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.004466333133799101\n",
            "0.00016182145410601308\n",
            "0.005537514224156896\n",
            "--------------------\n",
            "count: 1602, item_Code: A204270\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.05506276814054824\n",
            "0.000667868318682787\n",
            "0.06099883652075855\n",
            "--------------------\n",
            "count: 1603, item_Code: A204320\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.009165624452968283\n",
            "1.965239374842754e-05\n",
            "0.010609060513243496\n",
            "--------------------\n",
            "count: 1604, item_Code: A204620\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.06839213894248436\n",
            "0.0017801668368071478\n",
            "0.08352495281529147\n",
            "--------------------\n",
            "count: 1605, item_Code: A204840\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0437072715206341\n",
            "0.005510583246416938\n",
            "0.05311466514111813\n",
            "--------------------\n",
            "count: 1606, item_Code: A205100\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.014999879307864224\n",
            "0.0003625273794599815\n",
            "0.01647980192623331\n",
            "--------------------\n",
            "count: 1607, item_Code: A205470\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.0011411488369655368\n",
            "3.307504300406385e-05\n",
            "0.0012995131562466165\n",
            "--------------------\n",
            "count: 1608, item_Code: A205500\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.0037974372146891257\n",
            "0.00013827228286610166\n",
            "0.004297567836177774\n",
            "--------------------\n",
            "count: 1609, item_Code: A206400\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0454004712109357\n",
            "0.0008709896974206393\n",
            "0.0488019968165027\n",
            "--------------------\n",
            "count: 1610, item_Code: A206560\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0008261598535076095\n",
            "7.546615583056201e-06\n",
            "0.0009498441125046624\n",
            "--------------------\n",
            "count: 1611, item_Code: A206640\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.012324132862219509\n",
            "0.00010239739449323474\n",
            "0.013852187904100869\n",
            "--------------------\n",
            "count: 1612, item_Code: A206650\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.00205086415274612\n",
            "2.7785531119675884e-05\n",
            "0.002830602248674466\n",
            "--------------------\n",
            "count: 1613, item_Code: A207760\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0015384284057593808\n",
            "5.499290266960316e-05\n",
            "0.001735852578090726\n",
            "--------------------\n",
            "count: 1614, item_Code: A207940\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0067320067745943865\n",
            "8.533923068951077e-07\n",
            "0.00708646003851129\n",
            "--------------------\n",
            "count: 1615, item_Code: A208140\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.015841447891640808\n",
            "0.0006989928153112427\n",
            "0.019698289675511155\n",
            "--------------------\n",
            "count: 1616, item_Code: A208340\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.016771343264008465\n",
            "0.0006153599989984943\n",
            "0.017150272370071586\n",
            "--------------------\n",
            "count: 1617, item_Code: A208370\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.18817706597571185\n",
            "0.0027390003341185454\n",
            "0.21566923765156154\n",
            "--------------------\n",
            "count: 1618, item_Code: A208640\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 98)\n",
            "Series([], dtype: int64)\n",
            "0.008883010963359084\n",
            "0.0015916155688234308\n",
            "0.009967380563630918\n",
            "--------------------\n",
            "count: 1619, item_Code: A210540\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.004328519286718802\n",
            "3.645554019036627e-05\n",
            "0.005375520010625252\n",
            "--------------------\n",
            "count: 1620, item_Code: A210980\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.03052039721151232\n",
            "0.00013686664568326518\n",
            "0.035946225927319164\n",
            "--------------------\n",
            "count: 1621, item_Code: A211050\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.002431440649403764\n",
            "2.2252525248989085e-05\n",
            "0.0031259689536095635\n",
            "--------------------\n",
            "count: 1622, item_Code: A211270\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.0047228687171203395\n",
            "2.5535200611788786e-05\n",
            "0.004876399231896911\n",
            "--------------------\n",
            "count: 1623, item_Code: A212560\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 116)\n",
            "Series([], dtype: int64)\n",
            "0.009243183476489019\n",
            "0.0001437539457135876\n",
            "0.011320547710197469\n",
            "--------------------\n",
            "count: 1624, item_Code: A213420\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.003933031845857234\n",
            "8.886104584337709e-06\n",
            "0.004678403726855074\n",
            "--------------------\n",
            "count: 1625, item_Code: A213500\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.00286983411606343\n",
            "2.5199797985142822e-05\n",
            "0.003952151133527301\n",
            "--------------------\n",
            "count: 1626, item_Code: A214150\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0131159782577015\n",
            "5.823112415439239e-05\n",
            "0.015052093653157258\n",
            "--------------------\n",
            "count: 1627, item_Code: A214180\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0014287751090402403\n",
            "1.1071100418257004e-05\n",
            "0.0018424735840686503\n",
            "--------------------\n",
            "count: 1628, item_Code: A214260\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.001001217522813628\n",
            "4.711773169661181e-06\n",
            "0.0011398547127511247\n",
            "--------------------\n",
            "count: 1629, item_Code: A214270\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0015405307163443164\n",
            "4.5068931424177806e-05\n",
            "0.0019985343343178457\n",
            "--------------------\n",
            "count: 1630, item_Code: A214320\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 103)\n",
            "Series([], dtype: int64)\n",
            "0.0016171945889558023\n",
            "4.051112196478594e-06\n",
            "0.001998095258729283\n",
            "--------------------\n",
            "count: 1631, item_Code: A214330\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.018859942510319645\n",
            "0.0022869517953239245\n",
            "0.02116374761674356\n",
            "--------------------\n",
            "count: 1632, item_Code: A214370\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0016165186980894456\n",
            "7.592031442027558e-07\n",
            "0.001924165500285278\n",
            "--------------------\n",
            "count: 1633, item_Code: A214390\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.00539460575873818\n",
            "8.04402061934932e-05\n",
            "0.006410133058153792\n",
            "--------------------\n",
            "count: 1634, item_Code: A214420\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.002720703302899589\n",
            "6.267764349771991e-05\n",
            "0.0032298648970042294\n",
            "--------------------\n",
            "count: 1635, item_Code: A214430\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.001798222630895907\n",
            "8.832860163833052e-06\n",
            "0.0020490187439370015\n",
            "--------------------\n",
            "count: 1636, item_Code: A214450\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.007433958389447071\n",
            "7.443982953340568e-06\n",
            "0.008480011408265119\n",
            "--------------------\n",
            "count: 1637, item_Code: A214610\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.003667941476487613\n",
            "4.9297972025260716e-05\n",
            "0.0037780789733115653\n",
            "--------------------\n",
            "count: 1638, item_Code: A214680\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.06113871021904439\n",
            "0.003175480950216848\n",
            "0.06431959773544574\n",
            "--------------------\n",
            "count: 1639, item_Code: A214870\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.004505300807128757\n",
            "0.0003257617975388303\n",
            "0.004576964499681381\n",
            "--------------------\n",
            "count: 1640, item_Code: A215000\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "6.721593672409654e-05\n",
            "5.883962696870081e-08\n",
            "7.736903542928521e-05\n",
            "--------------------\n",
            "count: 1641, item_Code: A215090\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 99)\n",
            "Series([], dtype: int64)\n",
            "0.005575163979665376\n",
            "0.0003704404303896525\n",
            "0.0073098193556152165\n",
            "--------------------\n",
            "count: 1642, item_Code: A215100\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.0006752031862561125\n",
            "1.1284178116102307e-05\n",
            "0.0008017531097142284\n",
            "--------------------\n",
            "count: 1643, item_Code: A215200\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.000959196918605206\n",
            "1.558630243464624e-06\n",
            "0.0013333395400046452\n",
            "--------------------\n",
            "count: 1644, item_Code: A215360\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.003417830636317376\n",
            "2.0116332642961307e-05\n",
            "0.004206100111136092\n",
            "--------------------\n",
            "count: 1645, item_Code: A215600\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 77)\n",
            "Series([], dtype: int64)\n",
            "0.012118110634886155\n",
            "0.00018912411688769523\n",
            "0.01422182905017355\n",
            "--------------------\n",
            "count: 1646, item_Code: A215790\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.009948144149666405\n",
            "0.0007903410405382209\n",
            "0.011987985341776424\n",
            "--------------------\n",
            "count: 1647, item_Code: A216050\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.002121934433065083\n",
            "1.4120285446846973e-05\n",
            "0.0022406269204040814\n",
            "--------------------\n",
            "count: 1648, item_Code: A216080\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.00024040873540798202\n",
            "1.1495682207519697e-06\n",
            "0.00029428424372642285\n",
            "--------------------\n",
            "count: 1649, item_Code: A217190\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.030740168479436156\n",
            "0.00033842790527099546\n",
            "0.035417990806137285\n",
            "--------------------\n",
            "count: 1650, item_Code: A217270\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.012822923123773458\n",
            "0.00014324946644654325\n",
            "0.01830921523365261\n",
            "--------------------\n",
            "count: 1651, item_Code: A217330\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0004179242734001794\n",
            "2.3273989247587415e-06\n",
            "0.0004602651914367324\n",
            "--------------------\n",
            "count: 1652, item_Code: A217500\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.042962047809335976\n",
            "0.001095371754532689\n",
            "0.04669316865362489\n",
            "--------------------\n",
            "count: 1653, item_Code: A217620\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 102)\n",
            "Series([], dtype: int64)\n",
            "0.005001955363566898\n",
            "0.0005342858732193598\n",
            "0.006475736267932349\n",
            "--------------------\n",
            "count: 1654, item_Code: A217730\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.0025004146027034343\n",
            "8.001809405675128e-05\n",
            "0.0026791804183342875\n",
            "--------------------\n",
            "count: 1655, item_Code: A217820\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.14269307565955386\n",
            "0.0014394977654952183\n",
            "0.15407880755055436\n",
            "--------------------\n",
            "count: 1656, item_Code: A218150\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.004222198031008399\n",
            "7.95946216123166e-05\n",
            "0.005031463516009856\n",
            "--------------------\n",
            "count: 1657, item_Code: A218410\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.002137445996777387\n",
            "9.032255913581426e-06\n",
            "0.0027637976711034217\n",
            "--------------------\n",
            "count: 1658, item_Code: A219130\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.011094474841229385\n",
            "4.705238310754612e-05\n",
            "0.014047735609010686\n",
            "--------------------\n",
            "count: 1659, item_Code: A219420\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0066105423304179565\n",
            "0.00010570458003321933\n",
            "0.007217002498934949\n",
            "--------------------\n",
            "count: 1660, item_Code: A219550\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0022630559529109935\n",
            "0.00020544346117067826\n",
            "0.002468367230394979\n",
            "--------------------\n",
            "count: 1661, item_Code: A220100\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.018938556938822633\n",
            "0.0002457011271382588\n",
            "0.02184062835530447\n",
            "--------------------\n",
            "count: 1662, item_Code: A220180\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.007497370275905269\n",
            "0.00021200426791731267\n",
            "0.007805053339603415\n",
            "--------------------\n",
            "count: 1663, item_Code: A220260\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.0029800488116355456\n",
            "2.7879350947185048e-05\n",
            "0.003562866966259086\n",
            "--------------------\n",
            "count: 1664, item_Code: A221840\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.0017006966650418083\n",
            "4.771738594179229e-05\n",
            "0.001929032500200317\n",
            "--------------------\n",
            "count: 1665, item_Code: A221980\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.006192874915601957\n",
            "4.566122414431543e-05\n",
            "0.007684785472501047\n",
            "--------------------\n",
            "count: 1666, item_Code: A222040\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.002479007102556352\n",
            "5.740248040347258e-05\n",
            "0.002806128512419604\n",
            "--------------------\n",
            "count: 1667, item_Code: A222080\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.008556546625914051\n",
            "6.629190846303522e-05\n",
            "0.009691545086802679\n",
            "--------------------\n",
            "count: 1668, item_Code: A222110\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.010421167800329083\n",
            "0.00017125566287683847\n",
            "0.011894815352511733\n",
            "--------------------\n",
            "count: 1669, item_Code: A222420\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.06746367737534152\n",
            "0.0030507667434128177\n",
            "0.07036251247395599\n",
            "--------------------\n",
            "count: 1670, item_Code: A222800\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0032872047360190965\n",
            "1.1513251425364201e-05\n",
            "0.003368441665715518\n",
            "--------------------\n",
            "count: 1671, item_Code: A222810\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.19936763716620287\n",
            "0.005226284136852568\n",
            "0.2552875846204208\n",
            "--------------------\n",
            "count: 1672, item_Code: A222980\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.003466674299549292\n",
            "6.780039353396103e-05\n",
            "0.004233507545849961\n",
            "--------------------\n",
            "count: 1673, item_Code: A223250\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0004145563857794817\n",
            "3.5731900096466093e-06\n",
            "0.0005846070664193285\n",
            "--------------------\n",
            "count: 1674, item_Code: A223310\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 1)\n",
            "Series([], dtype: int64)\n",
            "A223310 is zero\n",
            "--------------------\n",
            "count: 1675, item_Code: A224110\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.002690175834262239\n",
            "2.3501871626729727e-05\n",
            "0.0033044738064595294\n",
            "--------------------\n",
            "count: 1676, item_Code: A225190\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.004927393768108838\n",
            "4.552735518468997e-05\n",
            "0.005871744335601969\n",
            "--------------------\n",
            "count: 1677, item_Code: A225220\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.017054447440750665\n",
            "0.0002721701156979076\n",
            "0.02121484748672704\n",
            "--------------------\n",
            "count: 1678, item_Code: A225530\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.004079544481768001\n",
            "5.98458597188412e-05\n",
            "0.005202338782854641\n",
            "--------------------\n",
            "count: 1679, item_Code: A225570\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0011813487520460815\n",
            "5.866382416901863e-06\n",
            "0.0015372839126841593\n",
            "--------------------\n",
            "count: 1680, item_Code: A226320\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.0015263207330766212\n",
            "9.705229699667775e-06\n",
            "0.0017332130827193738\n",
            "--------------------\n",
            "count: 1681, item_Code: A226330\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.0013052782092927372\n",
            "1.667480150653152e-05\n",
            "0.0016130869666871985\n",
            "--------------------\n",
            "count: 1682, item_Code: A226340\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.029907001688949702\n",
            "0.0020126820651889077\n",
            "0.03437556967428566\n",
            "--------------------\n",
            "count: 1683, item_Code: A226400\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.005364699289566488\n",
            "0.0001286388649182137\n",
            "0.006503635787944863\n",
            "--------------------\n",
            "count: 1684, item_Code: A226440\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.0031550749869590313\n",
            "0.00020621457863655878\n",
            "0.0038871835201079474\n",
            "--------------------\n",
            "count: 1685, item_Code: A226950\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0028431992839614397\n",
            "1.3148112811728646e-05\n",
            "0.0032445834076107382\n",
            "--------------------\n",
            "count: 1686, item_Code: A227100\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0013908787530150827\n",
            "3.9234347089890796e-05\n",
            "0.0014521771778508157\n",
            "--------------------\n",
            "count: 1687, item_Code: A227840\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.00029250772786326706\n",
            "2.5936763846238433e-06\n",
            "0.0003096685214821911\n",
            "--------------------\n",
            "count: 1688, item_Code: A227950\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.016921357449109563\n",
            "0.0011141348934859826\n",
            "0.02283123162899089\n",
            "--------------------\n",
            "count: 1689, item_Code: A228340\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0011794415150689018\n",
            "3.74830834972472e-05\n",
            "0.0012764437692857316\n",
            "--------------------\n",
            "count: 1690, item_Code: A228670\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.009736091978508436\n",
            "2.7917734507578085e-05\n",
            "0.010662061429511685\n",
            "--------------------\n",
            "count: 1691, item_Code: A228760\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.006539477480813124\n",
            "5.1088838134492625e-05\n",
            "0.008347254227616584\n",
            "--------------------\n",
            "count: 1692, item_Code: A228850\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.0029115618382396253\n",
            "2.398506935012348e-05\n",
            "0.003425852502737393\n",
            "--------------------\n",
            "count: 1693, item_Code: A229000\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0174395063156832\n",
            "0.00041073117275588263\n",
            "0.01843339940403409\n",
            "--------------------\n",
            "count: 1694, item_Code: A229640\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.020666436908019628\n",
            "0.0002976531727828156\n",
            "0.02180573725202479\n",
            "--------------------\n",
            "count: 1695, item_Code: A230240\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.002129467287401591\n",
            "7.6093352881828e-06\n",
            "0.002320664047764605\n",
            "--------------------\n",
            "count: 1696, item_Code: A230360\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.0013750474081462016\n",
            "1.1893600415272067e-05\n",
            "0.0017610431930182892\n",
            "--------------------\n",
            "count: 1697, item_Code: A230980\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.02268000502997438\n",
            "0.002640098411901235\n",
            "0.02843729377345384\n",
            "--------------------\n",
            "count: 1698, item_Code: A232140\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.004729874109671073\n",
            "0.00011953307051659079\n",
            "0.005247172936860277\n",
            "--------------------\n",
            "count: 1699, item_Code: A232680\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.003901180131773193\n",
            "3.7465507314662353e-05\n",
            "0.004073723673742132\n",
            "--------------------\n",
            "count: 1700, item_Code: A234080\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.002091709890252484\n",
            "1.7593965574563013e-05\n",
            "0.0022011981515847765\n",
            "--------------------\n",
            "count: 1701, item_Code: A234100\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.0028374703649509075\n",
            "0.00012266048709192292\n",
            "0.003062547233796574\n",
            "--------------------\n",
            "count: 1702, item_Code: A234300\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.002662829186435071\n",
            "6.245172257770064e-05\n",
            "0.0031892921646874475\n",
            "--------------------\n",
            "count: 1703, item_Code: A234340\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0031864245849040646\n",
            "1.6942757604207743e-05\n",
            "0.003802684111522689\n",
            "--------------------\n",
            "count: 1704, item_Code: A234690\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.00849324051032454\n",
            "0.00010845603987564011\n",
            "0.009862590258106096\n",
            "--------------------\n",
            "count: 1705, item_Code: A234920\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.10413856835730258\n",
            "0.0004945787400623151\n",
            "0.11933688130967832\n",
            "--------------------\n",
            "count: 1706, item_Code: A235980\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0007827647801605053\n",
            "3.264391216744242e-06\n",
            "0.0008952611486509843\n",
            "--------------------\n",
            "count: 1707, item_Code: A236200\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.037683552995198016\n",
            "0.00016234353814570146\n",
            "0.07156988398098943\n",
            "--------------------\n",
            "count: 1708, item_Code: A236810\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.006477800150969415\n",
            "0.00012043766207832382\n",
            "0.006633150714800097\n",
            "--------------------\n",
            "count: 1709, item_Code: A237690\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0019905346571855867\n",
            "2.4073745586712608e-06\n",
            "0.002315024437781306\n",
            "--------------------\n",
            "count: 1710, item_Code: A237820\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.001464066397481171\n",
            "2.485478388185697e-05\n",
            "0.0021044915302483786\n",
            "--------------------\n",
            "count: 1711, item_Code: A237880\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.00515056563502488\n",
            "2.3284306687894785e-05\n",
            "0.005798629464827275\n",
            "--------------------\n",
            "count: 1712, item_Code: A238090\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0005870419475286325\n",
            "2.111448342708418e-05\n",
            "0.0006448109575025879\n",
            "--------------------\n",
            "count: 1713, item_Code: A238120\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 115)\n",
            "Series([], dtype: int64)\n",
            "0.0006154099814011715\n",
            "7.149622674337401e-06\n",
            "0.0007493629952095633\n",
            "--------------------\n",
            "count: 1714, item_Code: A238200\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.0031493395334715995\n",
            "3.7709244434392094e-05\n",
            "0.0037034554990483326\n",
            "--------------------\n",
            "count: 1715, item_Code: A238490\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.03340725393075748\n",
            "0.0005130808714260007\n",
            "0.03856888098558346\n",
            "--------------------\n",
            "count: 1716, item_Code: A239340\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0010440721001941712\n",
            "2.8891681974931447e-05\n",
            "0.0012174650330634086\n",
            "--------------------\n",
            "count: 1717, item_Code: A239610\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.024406642564038825\n",
            "0.0001252744864744396\n",
            "0.025523671151495782\n",
            "--------------------\n",
            "count: 1718, item_Code: A239890\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0019876793655081806\n",
            "8.601213551053895e-06\n",
            "0.002117454417319695\n",
            "--------------------\n",
            "count: 1719, item_Code: A240810\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 102)\n",
            "Series([], dtype: int64)\n",
            "0.0008058562205405905\n",
            "2.6081012431283653e-06\n",
            "0.0009756756343370938\n",
            "--------------------\n",
            "count: 1720, item_Code: A241520\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0021488001056847376\n",
            "5.52962302311125e-05\n",
            "0.002200559213886668\n",
            "--------------------\n",
            "count: 1721, item_Code: A241560\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.003001645244269942\n",
            "6.128605049298941e-06\n",
            "0.003742866417518312\n",
            "--------------------\n",
            "count: 1722, item_Code: A241590\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.003473063852652558\n",
            "4.2487818696125136e-05\n",
            "0.004099954747256297\n",
            "--------------------\n",
            "count: 1723, item_Code: A241690\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.009890357798091524\n",
            "0.00021423682065070153\n",
            "0.010873561811802851\n",
            "--------------------\n",
            "count: 1724, item_Code: A241710\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.0005061696070091178\n",
            "5.18245352218087e-06\n",
            "0.0005957601864033753\n",
            "--------------------\n",
            "count: 1725, item_Code: A241770\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.008629360330208632\n",
            "7.606377642027374e-05\n",
            "0.010212696157505516\n",
            "--------------------\n",
            "count: 1726, item_Code: A241790\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.011807510311215689\n",
            "8.229112476845353e-05\n",
            "0.01210707514037587\n",
            "--------------------\n",
            "count: 1727, item_Code: A241820\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 102)\n",
            "Series([], dtype: int64)\n",
            "0.002340058537690008\n",
            "2.2742748528540275e-05\n",
            "0.002372671785527363\n",
            "--------------------\n",
            "count: 1728, item_Code: A241840\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.001981948322888153\n",
            "9.33138239895642e-06\n",
            "0.0024223581193590915\n",
            "--------------------\n",
            "count: 1729, item_Code: A242040\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0022025303108421214\n",
            "9.859624250529497e-05\n",
            "0.0025262082677342443\n",
            "--------------------\n",
            "count: 1730, item_Code: A243070\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 103)\n",
            "Series([], dtype: int64)\n",
            "0.003264526253042277\n",
            "9.890893363550783e-06\n",
            "0.0038429707079120435\n",
            "--------------------\n",
            "count: 1731, item_Code: A243840\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0010197896607375392\n",
            "2.0167999284487954e-06\n",
            "0.0012364974999441994\n",
            "--------------------\n",
            "count: 1732, item_Code: A244460\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0006294172110756335\n",
            "1.5286968281158296e-05\n",
            "0.0007457730461276487\n",
            "--------------------\n",
            "count: 1733, item_Code: A244920\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.005202931006048554\n",
            "0.00010875498997086831\n",
            "0.005582952613634917\n",
            "--------------------\n",
            "count: 1734, item_Code: A245620\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0098327665042234\n",
            "0.0005328062412668041\n",
            "0.011524438329470713\n",
            "--------------------\n",
            "count: 1735, item_Code: A246690\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0014725054630010466\n",
            "8.855759401222365e-05\n",
            "0.001902470294742121\n",
            "--------------------\n",
            "count: 1736, item_Code: A246710\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.0020957690074283166\n",
            "1.4430592621565982e-05\n",
            "0.002241293569139332\n",
            "--------------------\n",
            "count: 1737, item_Code: A246720\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.09039925574894975\n",
            "0.0009271137134151304\n",
            "0.09532179227576382\n",
            "--------------------\n",
            "count: 1738, item_Code: A246960\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.002032101083386806\n",
            "2.70506657012739e-05\n",
            "0.0020882384898391593\n",
            "--------------------\n",
            "count: 1739, item_Code: A247540\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0030149329841757814\n",
            "1.159537498859346e-06\n",
            "0.00367048981524305\n",
            "--------------------\n",
            "count: 1740, item_Code: A247660\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.0009598162251980587\n",
            "6.223779499741299e-06\n",
            "0.0011888652759225252\n",
            "--------------------\n",
            "count: 1741, item_Code: A248070\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.006034160523267928\n",
            "2.4647071396287577e-05\n",
            "0.0078061690250798245\n",
            "--------------------\n",
            "count: 1742, item_Code: A248170\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0008065052529370102\n",
            "2.7348825859211137e-06\n",
            "0.0008612667085242125\n",
            "--------------------\n",
            "count: 1743, item_Code: A249420\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.0008334676395558442\n",
            "3.883438224267983e-06\n",
            "0.0009695019637196414\n",
            "--------------------\n",
            "count: 1744, item_Code: A250000\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0010891648214358914\n",
            "1.0242983458837133e-05\n",
            "0.0011386485716172652\n",
            "--------------------\n",
            "count: 1745, item_Code: A250060\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.007211429353780356\n",
            "0.0003228988686899997\n",
            "0.00868547864933285\n",
            "--------------------\n",
            "count: 1746, item_Code: A251270\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0062651219360607985\n",
            "9.403844751248151e-06\n",
            "0.006496962915814919\n",
            "--------------------\n",
            "count: 1747, item_Code: A251370\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.015537722619895552\n",
            "0.00012664016909951309\n",
            "0.01790446086156313\n",
            "--------------------\n",
            "count: 1748, item_Code: A251630\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.004666587789688492\n",
            "5.169535501404471e-05\n",
            "0.0049756565658361\n",
            "--------------------\n",
            "count: 1749, item_Code: A251970\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.01610747268520451\n",
            "8.803548193143907e-05\n",
            "0.01867035154181695\n",
            "--------------------\n",
            "count: 1750, item_Code: A252500\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.004019616856445888\n",
            "0.00036080520634536783\n",
            "0.004637498291969651\n",
            "--------------------\n",
            "count: 1751, item_Code: A252990\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.01485104206494725\n",
            "0.00029426317857853267\n",
            "0.017181748100796656\n",
            "--------------------\n",
            "count: 1752, item_Code: A253450\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.002101323027939846\n",
            "3.0967527018584077e-06\n",
            "0.002396202756593954\n",
            "--------------------\n",
            "count: 1753, item_Code: A253590\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.016693333566490765\n",
            "0.0005109263172185127\n",
            "0.018029752789822318\n",
            "--------------------\n",
            "count: 1754, item_Code: A253840\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.002574863518202619\n",
            "2.990139343352512e-05\n",
            "0.0029592615571364697\n",
            "--------------------\n",
            "count: 1755, item_Code: A254120\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.052310874432805576\n",
            "0.0016629663618250557\n",
            "0.061456917016478124\n",
            "--------------------\n",
            "count: 1756, item_Code: A255220\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0236689631302094\n",
            "0.002032501703579029\n",
            "0.02867321910780867\n",
            "--------------------\n",
            "count: 1757, item_Code: A255440\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.031877598167435885\n",
            "0.00025262324224173426\n",
            "0.03518868750629425\n",
            "--------------------\n",
            "count: 1758, item_Code: A256150\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0006353790529222655\n",
            "7.887978366817168e-06\n",
            "0.000797090078038716\n",
            "--------------------\n",
            "count: 1759, item_Code: A256630\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.012281400040137669\n",
            "0.0006029687709717182\n",
            "0.012516731725846407\n",
            "--------------------\n",
            "count: 1760, item_Code: A256840\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0043094763074805085\n",
            "0.00010811437456909512\n",
            "0.005431725221890038\n",
            "--------------------\n",
            "count: 1761, item_Code: A256940\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.005485193280886354\n",
            "5.350036613821921e-05\n",
            "0.007078688077570445\n",
            "--------------------\n",
            "count: 1762, item_Code: A257370\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 1)\n",
            "Series([], dtype: int64)\n",
            "A257370 is zero\n",
            "--------------------\n",
            "count: 1763, item_Code: A258610\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 103)\n",
            "Series([], dtype: int64)\n",
            "0.01454222219708754\n",
            "0.00035617423511661233\n",
            "0.022496964755435934\n",
            "--------------------\n",
            "count: 1764, item_Code: A258830\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.000598009752411599\n",
            "1.633419412014702e-05\n",
            "0.0006179668107381577\n",
            "--------------------\n",
            "count: 1765, item_Code: A259630\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.003303860501667562\n",
            "2.6682597443550652e-05\n",
            "0.0037374741472236826\n",
            "--------------------\n",
            "count: 1766, item_Code: A260660\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0015690217306958707\n",
            "2.3221125990307597e-05\n",
            "0.0019410628636652863\n",
            "--------------------\n",
            "count: 1767, item_Code: A260930\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.019985971973498332\n",
            "0.0003566898577951288\n",
            "0.029658953127136272\n",
            "--------------------\n",
            "count: 1768, item_Code: A260970\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.003949051948438864\n",
            "2.269371693212426e-05\n",
            "0.005411312810014245\n",
            "--------------------\n",
            "count: 1769, item_Code: A261200\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0023430064118050114\n",
            "2.336996146870132e-05\n",
            "0.0026699887431006616\n",
            "--------------------\n",
            "count: 1770, item_Code: A262260\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0024804139116895384\n",
            "1.4876039177458887e-05\n",
            "0.002795124454855901\n",
            "--------------------\n",
            "count: 1771, item_Code: A262840\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0009426821541031435\n",
            "1.3595679178590939e-05\n",
            "0.001137282497909141\n",
            "--------------------\n",
            "count: 1772, item_Code: A263020\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 116)\n",
            "Series([], dtype: int64)\n",
            "0.02446022874200935\n",
            "0.0007115395620538689\n",
            "0.026534791493394643\n",
            "--------------------\n",
            "count: 1773, item_Code: A263050\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0013970451514978776\n",
            "2.6579561518146753e-05\n",
            "0.0014791040089445027\n",
            "--------------------\n",
            "count: 1774, item_Code: A263540\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 3)\n",
            "Series([], dtype: int64)\n",
            "4948.379486214909\n",
            "45.367946052346234\n",
            "4956.494645372157\n",
            "--------------------\n",
            "count: 1775, item_Code: A263600\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.003704806475919516\n",
            "3.1247032004638015e-05\n",
            "0.005100054475195315\n",
            "--------------------\n",
            "count: 1776, item_Code: A263690\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.0027838351992007424\n",
            "2.41905717079811e-05\n",
            "0.0029812257191910345\n",
            "--------------------\n",
            "count: 1777, item_Code: A263700\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0017784429024686687\n",
            "2.3966166964597944e-05\n",
            "0.0020448858658633742\n",
            "--------------------\n",
            "count: 1778, item_Code: A263720\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.00037794167728861795\n",
            "1.9594839384468857e-06\n",
            "0.0004802578832405594\n",
            "--------------------\n",
            "count: 1779, item_Code: A263750\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0022876143746543677\n",
            "5.228619853094289e-06\n",
            "0.0025592329479982604\n",
            "--------------------\n",
            "count: 1780, item_Code: A263770\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.008490208539690987\n",
            "0.0002016483539165065\n",
            "0.009381618688361591\n",
            "--------------------\n",
            "count: 1781, item_Code: A263800\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.018877522247021262\n",
            "0.00028999990422500616\n",
            "0.02129187248332106\n",
            "--------------------\n",
            "count: 1782, item_Code: A263810\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.028685627896660057\n",
            "0.0005485863844235227\n",
            "0.049435211120658004\n",
            "--------------------\n",
            "count: 1783, item_Code: A263860\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.004665725727318204\n",
            "4.2994212729965366e-05\n",
            "0.005115031474314078\n",
            "--------------------\n",
            "count: 1784, item_Code: A263920\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 73)\n",
            "Series([], dtype: int64)\n",
            "8.730300275298457e-08\n",
            "6.567917782308051e-09\n",
            "1.3731432862247184e-07\n",
            "--------------------\n",
            "count: 1785, item_Code: A264450\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.0006903016409827008\n",
            "5.296907366540651e-06\n",
            "0.0008988319468130224\n",
            "--------------------\n",
            "count: 1786, item_Code: A264660\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.008310435020272658\n",
            "6.37665271310916e-05\n",
            "0.009938627660680998\n",
            "--------------------\n",
            "count: 1787, item_Code: A264850\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 116)\n",
            "Series([], dtype: int64)\n",
            "0.0042334387647618614\n",
            "0.00015970020538114314\n",
            "0.004721072116000016\n",
            "--------------------\n",
            "count: 1788, item_Code: A264900\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.006230368724997485\n",
            "7.491812577351239e-05\n",
            "0.007679278918176323\n",
            "--------------------\n",
            "count: 1789, item_Code: A265520\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0009871069499543712\n",
            "4.785508303678523e-06\n",
            "0.0011074205695683638\n",
            "--------------------\n",
            "count: 1790, item_Code: A265560\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.002124436432253181\n",
            "1.8344456576826693e-05\n",
            "0.0027458668133820844\n",
            "--------------------\n",
            "count: 1791, item_Code: A265740\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 118)\n",
            "Series([], dtype: int64)\n",
            "0.0058678711367974755\n",
            "7.170850691400803e-05\n",
            "0.006477757546838627\n",
            "--------------------\n",
            "count: 1792, item_Code: A267250\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 103)\n",
            "Series([], dtype: int64)\n",
            "0.0002454230450287772\n",
            "4.140050663456737e-07\n",
            "0.000374270080436367\n",
            "--------------------\n",
            "count: 1793, item_Code: A267260\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 101)\n",
            "Series([], dtype: int64)\n",
            "0.0029996896317849558\n",
            "5.920731213663879e-06\n",
            "0.0031732744383294337\n",
            "--------------------\n",
            "count: 1794, item_Code: A267270\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.00031281979900086296\n",
            "5.081470713240334e-07\n",
            "0.0003566762217670946\n",
            "--------------------\n",
            "count: 1795, item_Code: A267290\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.0005230862181633711\n",
            "2.3750097018457646e-06\n",
            "0.0006219027206936467\n",
            "--------------------\n",
            "count: 1796, item_Code: A267320\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.01098528779599898\n",
            "0.00030440193437303103\n",
            "0.012876782049282068\n",
            "--------------------\n",
            "count: 1797, item_Code: A267790\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.006430438675670303\n",
            "8.524770698381851e-05\n",
            "0.007596255574507724\n",
            "--------------------\n",
            "count: 1798, item_Code: A267850\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0035595352691113174\n",
            "2.478393070075773e-05\n",
            "0.003814083452147368\n",
            "--------------------\n",
            "count: 1799, item_Code: A267980\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0020060121775410758\n",
            "4.182026615771673e-06\n",
            "0.002178783427511408\n",
            "--------------------\n",
            "count: 1800, item_Code: A268280\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0004074620024766773\n",
            "2.9540461213749395e-07\n",
            "0.0004481029732351061\n",
            "--------------------\n",
            "count: 1801, item_Code: A268600\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.024692839898489184\n",
            "0.0003696519118921397\n",
            "0.024815158004536174\n",
            "--------------------\n",
            "count: 1802, item_Code: A269620\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 100)\n",
            "Series([], dtype: int64)\n",
            "0.0031251304568816825\n",
            "0.0003515314477090266\n",
            "0.0037581110910791726\n",
            "--------------------\n",
            "count: 1803, item_Code: A270520\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.011933201661304339\n",
            "8.015920376795305e-05\n",
            "0.015255023628851618\n",
            "--------------------\n",
            "count: 1804, item_Code: A270660\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0003035827556838437\n",
            "1.797802305687165e-06\n",
            "0.00036762883791690356\n",
            "--------------------\n",
            "count: 1805, item_Code: A270870\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.001375903894828904\n",
            "1.6011073941903937e-05\n",
            "0.001550238123928909\n",
            "--------------------\n",
            "count: 1806, item_Code: A271560\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0015375166025478394\n",
            "1.0647201277670974e-06\n",
            "0.001821477994038657\n",
            "--------------------\n",
            "count: 1807, item_Code: A271980\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0026393263297601758\n",
            "1.3825670622960693e-05\n",
            "0.0033694563168571385\n",
            "--------------------\n",
            "count: 1808, item_Code: A272110\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.013486593530736475\n",
            "7.777904517825721e-05\n",
            "0.014554620944281667\n",
            "--------------------\n",
            "count: 1809, item_Code: A272210\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0007591719261351197\n",
            "5.253335779072223e-06\n",
            "0.0009500413354098985\n",
            "--------------------\n",
            "count: 1810, item_Code: A272290\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0003751801705220714\n",
            "8.75945232898343e-07\n",
            "0.0004564491459773817\n",
            "--------------------\n",
            "count: 1811, item_Code: A272450\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.015714443610947152\n",
            "9.744638148238454e-05\n",
            "0.016612650621931256\n",
            "--------------------\n",
            "count: 1812, item_Code: A272550\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0012442937889621437\n",
            "6.89461893672295e-06\n",
            "0.0014713210207163306\n",
            "--------------------\n",
            "count: 1813, item_Code: A273060\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.005947996755609589\n",
            "0.00048269005486863305\n",
            "0.00727453611352976\n",
            "--------------------\n",
            "count: 1814, item_Code: A274090\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.007005635416377724\n",
            "4.1097824204019585e-05\n",
            "0.007562386424260856\n",
            "--------------------\n",
            "count: 1815, item_Code: A276730\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.004616420123284115\n",
            "0.0002755820886312302\n",
            "0.00554804436948047\n",
            "--------------------\n",
            "count: 1816, item_Code: A277070\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.009291423709085696\n",
            "0.00014526500980539857\n",
            "0.00947441001385686\n",
            "--------------------\n",
            "count: 1817, item_Code: A277410\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 103)\n",
            "Series([], dtype: int64)\n",
            "0.02363872888095102\n",
            "0.0013412977267893056\n",
            "0.026862435590848898\n",
            "--------------------\n",
            "count: 1818, item_Code: A277810\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.011088740782967458\n",
            "1.0885078318372734e-05\n",
            "0.011606067671864348\n",
            "--------------------\n",
            "count: 1819, item_Code: A277880\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.007323043507494731\n",
            "5.651233925385327e-05\n",
            "0.008868041688571233\n",
            "--------------------\n",
            "count: 1820, item_Code: A278280\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0010654931286505113\n",
            "5.282408956557912e-07\n",
            "0.0013283333293611559\n",
            "--------------------\n",
            "count: 1821, item_Code: A278650\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0020622593940667383\n",
            "4.78274488206972e-05\n",
            "0.0024488821956539838\n",
            "--------------------\n",
            "count: 1822, item_Code: A279600\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.002203874465825114\n",
            "1.8294127655541358e-05\n",
            "0.002551111256472354\n",
            "--------------------\n",
            "count: 1823, item_Code: A280360\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.016488853939032804\n",
            "1.547914409828286e-05\n",
            "0.016593147429867693\n",
            "--------------------\n",
            "count: 1824, item_Code: A281740\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.008966865857413115\n",
            "6.083143006197635e-05\n",
            "0.010569980587229164\n",
            "--------------------\n",
            "count: 1825, item_Code: A281820\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 103)\n",
            "Series([], dtype: int64)\n",
            "0.0011119866452645511\n",
            "5.855558422959878e-06\n",
            "0.0011968966018150898\n",
            "--------------------\n",
            "count: 1826, item_Code: A282330\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0005343221923491607\n",
            "2.9113758612130055e-07\n",
            "0.0006617032922729707\n",
            "--------------------\n",
            "count: 1827, item_Code: A282690\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.014116483468023943\n",
            "0.00011608950590811508\n",
            "0.016055225938782787\n",
            "--------------------\n",
            "count: 1828, item_Code: A282880\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.008866941824574799\n",
            "2.681317897574997e-05\n",
            "0.009286769020933153\n",
            "--------------------\n",
            "count: 1829, item_Code: A284620\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.03672088235580304\n",
            "0.0006106786878781679\n",
            "0.037633523523944945\n",
            "--------------------\n",
            "count: 1830, item_Code: A284740\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.006378962424544928\n",
            "2.513826428713618e-05\n",
            "0.006946000922100941\n",
            "--------------------\n",
            "count: 1831, item_Code: A285130\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0019348563975654543\n",
            "2.5575609048105266e-06\n",
            "0.002248483304144036\n",
            "--------------------\n",
            "count: 1832, item_Code: A285490\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.003651453739682135\n",
            "1.1743559163002046e-05\n",
            "0.004611103589708637\n",
            "--------------------\n",
            "count: 1833, item_Code: A286750\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 103)\n",
            "Series([], dtype: int64)\n",
            "0.033376043581104874\n",
            "0.002260960946743734\n",
            "0.03814109517272074\n",
            "--------------------\n",
            "count: 1834, item_Code: A286940\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0026173480204306544\n",
            "9.097520961764541e-06\n",
            "0.0031477459689477558\n",
            "--------------------\n",
            "count: 1835, item_Code: A287410\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.007324272853899553\n",
            "8.393698989662733e-05\n",
            "0.008391062353077301\n",
            "--------------------\n",
            "count: 1836, item_Code: A288330\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.000482523106620647\n",
            "5.395344494900273e-06\n",
            "0.0005717238425104647\n",
            "--------------------\n",
            "count: 1837, item_Code: A288620\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.009278626264373695\n",
            "4.3998714031108675e-05\n",
            "0.009942730059318112\n",
            "--------------------\n",
            "count: 1838, item_Code: A289010\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0018371795647302255\n",
            "3.6464115833755174e-05\n",
            "0.002163281317426249\n",
            "--------------------\n",
            "count: 1839, item_Code: A289080\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0023094912758097053\n",
            "9.93506325250976e-05\n",
            "0.002686528070750344\n",
            "--------------------\n",
            "count: 1840, item_Code: A289220\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0006186226792730547\n",
            "3.2434355455124336e-06\n",
            "0.0007413538329847716\n",
            "--------------------\n",
            "count: 1841, item_Code: A290120\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.01381333298983615\n",
            "0.00026950191399786975\n",
            "0.018987269742792686\n",
            "--------------------\n",
            "count: 1842, item_Code: A290380\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.015291180844208914\n",
            "0.000658772832292394\n",
            "0.018946157305894627\n",
            "--------------------\n",
            "count: 1843, item_Code: A290520\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0004697148472284122\n",
            "7.2469455375976905e-06\n",
            "0.0005565135376422682\n",
            "--------------------\n",
            "count: 1844, item_Code: A290550\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.0014661686172379026\n",
            "1.4476972267603943e-05\n",
            "0.0019303459989555485\n",
            "--------------------\n",
            "count: 1845, item_Code: A290650\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 103)\n",
            "Series([], dtype: int64)\n",
            "0.002090657346949835\n",
            "6.886430663941564e-06\n",
            "0.0022617293450574246\n",
            "--------------------\n",
            "count: 1846, item_Code: A290670\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0009422409852656226\n",
            "1.388039720180037e-06\n",
            "0.001035583452843223\n",
            "--------------------\n",
            "count: 1847, item_Code: A290690\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.002631493899146638\n",
            "3.658643671276644e-05\n",
            "0.0031483587090213464\n",
            "--------------------\n",
            "count: 1848, item_Code: A290720\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0005329524582824281\n",
            "6.985207311361795e-06\n",
            "0.0006086480390922282\n",
            "--------------------\n",
            "count: 1849, item_Code: A290740\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.025094346145669987\n",
            "0.00032029116745085017\n",
            "0.02615840988130703\n",
            "--------------------\n",
            "count: 1850, item_Code: A291230\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.011761457279256623\n",
            "0.00023484727303349113\n",
            "0.013922166645827508\n",
            "--------------------\n",
            "count: 1851, item_Code: A291650\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.04271824013282336\n",
            "0.0011299883478547317\n",
            "0.04415600788255949\n",
            "--------------------\n",
            "count: 1852, item_Code: A293480\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0033770641546046437\n",
            "2.2857400346049723e-05\n",
            "0.004382303445218551\n",
            "--------------------\n",
            "count: 1853, item_Code: A293490\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.002506775822742687\n",
            "6.136780012278417e-06\n",
            "0.0031299109168551444\n",
            "--------------------\n",
            "count: 1854, item_Code: A293580\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0026648923801985804\n",
            "2.0981969277387442e-05\n",
            "0.004842851348092516\n",
            "--------------------\n",
            "count: 1855, item_Code: A293780\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0007674371562946665\n",
            "7.051621480894556e-06\n",
            "0.0009429454334438965\n",
            "--------------------\n",
            "count: 1856, item_Code: A294090\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.00019971111056899343\n",
            "8.34822565641987e-07\n",
            "0.00022699084177624988\n",
            "--------------------\n",
            "count: 1857, item_Code: A294140\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0066211927760984205\n",
            "8.701632689792131e-05\n",
            "0.008004873759524389\n",
            "--------------------\n",
            "count: 1858, item_Code: A294570\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0007125870741826172\n",
            "2.4301275748546746e-06\n",
            "0.0007692814300964111\n",
            "--------------------\n",
            "count: 1859, item_Code: A294630\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.014239375518112258\n",
            "0.00045843206816334486\n",
            "0.016450281554705064\n",
            "--------------------\n",
            "count: 1860, item_Code: A294870\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0010546126926783473\n",
            "8.730782082969176e-06\n",
            "0.0012097894251645626\n",
            "--------------------\n",
            "count: 1861, item_Code: A297090\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0015077953658571156\n",
            "1.2784254366202772e-05\n",
            "0.0018701191745025452\n",
            "--------------------\n",
            "count: 1862, item_Code: A297570\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.003341390869248547\n",
            "0.00021551385286139438\n",
            "0.004231711840552025\n",
            "--------------------\n",
            "count: 1863, item_Code: A297890\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.007047232018279222\n",
            "4.7990935522806784e-05\n",
            "0.00799871761913147\n",
            "--------------------\n",
            "count: 1864, item_Code: A298000\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0016826344003978496\n",
            "1.5104247653833156e-06\n",
            "0.0019346562024681103\n",
            "--------------------\n",
            "count: 1865, item_Code: A298020\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.002841250237543136\n",
            "7.114579351128772e-07\n",
            "0.00302453894442982\n",
            "--------------------\n",
            "count: 1866, item_Code: A298040\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.00013202188420109452\n",
            "1.6722802338592526e-07\n",
            "0.00015903959102702084\n",
            "--------------------\n",
            "count: 1867, item_Code: A298050\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.000834318782047679\n",
            "2.0379178759196078e-07\n",
            "0.000880863294778183\n",
            "--------------------\n",
            "count: 1868, item_Code: A298060\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0029413123019670214\n",
            "2.381631394050937e-05\n",
            "0.003932539878755114\n",
            "--------------------\n",
            "count: 1869, item_Code: A298380\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0007091879272290195\n",
            "3.191039153014868e-06\n",
            "0.0008950283536990097\n",
            "--------------------\n",
            "count: 1870, item_Code: A298540\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.002335689398266065\n",
            "8.30091380945883e-06\n",
            "0.002894236337972985\n",
            "--------------------\n",
            "count: 1871, item_Code: A298690\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.13964751106762682\n",
            "0.0037089501041873197\n",
            "0.1571774802416584\n",
            "--------------------\n",
            "count: 1872, item_Code: A299030\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0021018965053372085\n",
            "2.8285212868213632e-06\n",
            "0.002486336389793278\n",
            "--------------------\n",
            "count: 1873, item_Code: A299170\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.006457294129798659\n",
            "0.0003244000820182551\n",
            "0.007249726007710387\n",
            "--------------------\n",
            "count: 1874, item_Code: A299660\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0021153252257742378\n",
            "2.2319099317929153e-05\n",
            "0.0024679535344573926\n",
            "--------------------\n",
            "count: 1875, item_Code: A299900\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0012646208983521016\n",
            "1.3859967702223486e-05\n",
            "0.0015154204082894215\n",
            "--------------------\n",
            "count: 1876, item_Code: A300080\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0006445588529459201\n",
            "2.0761863542227895e-06\n",
            "0.0007640058795334768\n",
            "--------------------\n",
            "count: 1877, item_Code: A300120\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0020368447637035083\n",
            "2.958966826607543e-05\n",
            "0.002540597303823598\n",
            "--------------------\n",
            "count: 1878, item_Code: A300720\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.002174041121179471\n",
            "1.7290574538947053e-05\n",
            "0.002589322433411295\n",
            "--------------------\n",
            "count: 1879, item_Code: A301300\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0019405096001719358\n",
            "1.6837394449009034e-05\n",
            "0.0019624056556953665\n",
            "--------------------\n",
            "count: 1880, item_Code: A302430\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.012853158165550363\n",
            "8.533793553687823e-05\n",
            "0.01342605384405133\n",
            "--------------------\n",
            "count: 1881, item_Code: A302440\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.007900004085968249\n",
            "1.0691105023559507e-05\n",
            "0.008668145789719316\n",
            "--------------------\n",
            "count: 1882, item_Code: A302550\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.0027596543975960232\n",
            "2.1233527149395707e-05\n",
            "0.0029811731900241454\n",
            "--------------------\n",
            "count: 1883, item_Code: A303030\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.012662032822542338\n",
            "0.0006124395657015305\n",
            "0.018367090263518623\n",
            "--------------------\n",
            "count: 1884, item_Code: A303360\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0022892980967905412\n",
            "3.0178054350414914e-05\n",
            "0.003243252038676237\n",
            "--------------------\n",
            "count: 1885, item_Code: A304100\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.010044170074252179\n",
            "3.947316269809778e-05\n",
            "0.010316676423071111\n",
            "--------------------\n",
            "count: 1886, item_Code: A304840\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0036206945761174817\n",
            "3.0925716196961125e-05\n",
            "0.003722514243491442\n",
            "--------------------\n",
            "count: 1887, item_Code: A305090\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.0019824937685067805\n",
            "2.2621516124993763e-05\n",
            "0.0023107668749739645\n",
            "--------------------\n",
            "count: 1888, item_Code: A306040\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.010646805807118654\n",
            "7.275412022674359e-05\n",
            "0.011477253260757419\n",
            "--------------------\n",
            "count: 1889, item_Code: A306200\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0009006087707045178\n",
            "6.068974151639098e-07\n",
            "0.0010466234332982517\n",
            "--------------------\n",
            "count: 1890, item_Code: A306620\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0013299289682133046\n",
            "3.866449078383449e-05\n",
            "0.0016237233725040134\n",
            "--------------------\n",
            "count: 1891, item_Code: A307180\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.012163874918223882\n",
            "0.0003124037296861416\n",
            "0.01329690909934406\n",
            "--------------------\n",
            "count: 1892, item_Code: A307280\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.004177890684726056\n",
            "0.0002212374915746341\n",
            "0.005632241824952347\n",
            "--------------------\n",
            "count: 1893, item_Code: A307750\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.004684472823464602\n",
            "7.390218071300203e-05\n",
            "0.005126315413504711\n",
            "--------------------\n",
            "count: 1894, item_Code: A307870\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.004329522204003903\n",
            "0.00022738070412031653\n",
            "0.0051075496735510555\n",
            "--------------------\n",
            "count: 1895, item_Code: A307930\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.003536650365094829\n",
            "6.037170335035716e-05\n",
            "0.0038735098422909738\n",
            "--------------------\n",
            "count: 1896, item_Code: A307950\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.0005846682741927604\n",
            "4.770423118562616e-07\n",
            "0.0005998456594506879\n",
            "--------------------\n",
            "count: 1897, item_Code: A308100\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.060346972503672686\n",
            "0.0007592174861396558\n",
            "0.08634653963012215\n",
            "--------------------\n",
            "count: 1898, item_Code: A308170\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.009612143372335898\n",
            "5.5678600073180834e-05\n",
            "0.012192313883006628\n",
            "--------------------\n",
            "count: 1899, item_Code: A309930\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.010163232332676369\n",
            "0.0002730651967133077\n",
            "0.011518905687426617\n",
            "--------------------\n",
            "count: 1900, item_Code: A310200\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.008882377677643187\n",
            "0.00021553621100764683\n",
            "0.010055833331810277\n",
            "--------------------\n",
            "count: 1901, item_Code: A311390\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0006803819674435847\n",
            "8.253170472170246e-06\n",
            "0.0008016407789773407\n",
            "--------------------\n",
            "count: 1902, item_Code: A311690\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.014507445471341878\n",
            "4.1288243537821076e-05\n",
            "0.01635456511945875\n",
            "--------------------\n",
            "count: 1903, item_Code: A312610\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.007676182678187616\n",
            "0.0001551157106711882\n",
            "0.00994155836923666\n",
            "--------------------\n",
            "count: 1904, item_Code: A313760\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.001205586849876757\n",
            "1.0807624339592209e-05\n",
            "0.0014438785560305614\n",
            "--------------------\n",
            "count: 1905, item_Code: A314130\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.004775109695037827\n",
            "2.124165212170491e-05\n",
            "0.005136895297316719\n",
            "--------------------\n",
            "count: 1906, item_Code: A314930\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.0011177465845927752\n",
            "3.4304074907826246e-06\n",
            "0.001409704639004377\n",
            "--------------------\n",
            "count: 1907, item_Code: A316140\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.00030197680922962414\n",
            "2.5739187916551666e-06\n",
            "0.00037943040581257814\n",
            "--------------------\n",
            "count: 1908, item_Code: A317120\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.005234807047357511\n",
            "9.28240884565681e-05\n",
            "0.005877207222260101\n",
            "--------------------\n",
            "count: 1909, item_Code: A317240\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.004188616049286035\n",
            "0.0008022865021033353\n",
            "0.005048887724953637\n",
            "--------------------\n",
            "count: 1910, item_Code: A317330\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.001187404440861428\n",
            "5.506163706035284e-06\n",
            "0.0014881869844578029\n",
            "--------------------\n",
            "count: 1911, item_Code: A317400\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.006334292651748304\n",
            "9.702161606970734e-05\n",
            "0.007198277770170233\n",
            "--------------------\n",
            "count: 1912, item_Code: A317530\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.00036747340636793523\n",
            "5.489992922504814e-06\n",
            "0.00039250327475614464\n",
            "--------------------\n",
            "count: 1913, item_Code: A317690\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 114)\n",
            "Series([], dtype: int64)\n",
            "0.004707665637276174\n",
            "9.240248509696995e-05\n",
            "0.005000727800197464\n",
            "--------------------\n",
            "count: 1914, item_Code: A317770\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.08958843094781817\n",
            "0.00014058012980689154\n",
            "0.11205118991489951\n",
            "--------------------\n",
            "count: 1915, item_Code: A317830\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.0017164730155248737\n",
            "1.26957729878651e-05\n",
            "0.0020543770984250047\n",
            "--------------------\n",
            "count: 1916, item_Code: A317850\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.010309261715398558\n",
            "8.100626066791925e-05\n",
            "0.012509931426074303\n",
            "--------------------\n",
            "count: 1917, item_Code: A317870\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.0178653694945145\n",
            "0.00013614022404373065\n",
            "0.01844460865776511\n",
            "--------------------\n",
            "count: 1918, item_Code: A318000\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 116)\n",
            "Series([], dtype: int64)\n",
            "0.00602057925001039\n",
            "3.999373276580055e-05\n",
            "0.00789958599559498\n",
            "--------------------\n",
            "count: 1919, item_Code: A318010\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.002967906359117478\n",
            "3.1899019421924376e-05\n",
            "0.0035889727430998375\n",
            "--------------------\n",
            "count: 1920, item_Code: A318020\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 101)\n",
            "Series([], dtype: int64)\n",
            "0.007224725522488976\n",
            "3.991562825598592e-05\n",
            "0.007600812976343679\n",
            "--------------------\n",
            "count: 1921, item_Code: A318410\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 104)\n",
            "Series([], dtype: int64)\n",
            "0.0014537287358204292\n",
            "1.5505489247528905e-05\n",
            "0.001685750215724489\n",
            "--------------------\n",
            "count: 1922, item_Code: A319400\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.04334745854099917\n",
            "0.0009985829692867715\n",
            "0.04768826074400635\n",
            "--------------------\n",
            "count: 1923, item_Code: A319660\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0016538069304563882\n",
            "8.02063545439393e-06\n",
            "0.0019433883090107737\n",
            "--------------------\n",
            "count: 1924, item_Code: A320000\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.15375760872138927\n",
            "0.004400668110400284\n",
            "0.19255622225027783\n",
            "--------------------\n",
            "count: 1925, item_Code: A321260\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.005053790113773478\n",
            "0.00018865103632429017\n",
            "0.005994013795643838\n",
            "--------------------\n",
            "count: 1926, item_Code: A321550\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0015449236170146227\n",
            "1.4152657059733145e-05\n",
            "0.0016872595547348342\n",
            "--------------------\n",
            "count: 1927, item_Code: A322000\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0005685878282141251\n",
            "1.1942556803389111e-06\n",
            "0.0007539979558838307\n",
            "--------------------\n",
            "count: 1928, item_Code: A322180\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0012989773275573194\n",
            "9.311823912290618e-06\n",
            "0.0016024291664281484\n",
            "--------------------\n",
            "count: 1929, item_Code: A322310\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 101)\n",
            "Series([], dtype: int64)\n",
            "0.00138241129728461\n",
            "8.074046272941309e-06\n",
            "0.0017369746118739197\n",
            "--------------------\n",
            "count: 1930, item_Code: A322510\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.019395268236075937\n",
            "0.0003335922068113015\n",
            "0.022612183979487728\n",
            "--------------------\n",
            "count: 1931, item_Code: A322780\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.011230490616587001\n",
            "0.0004397342016959764\n",
            "0.012258660812002537\n",
            "--------------------\n",
            "count: 1932, item_Code: A323280\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.026425017908013614\n",
            "0.0013599756660795095\n",
            "0.028802625659982744\n",
            "--------------------\n",
            "count: 1933, item_Code: A323990\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.00018840841512428596\n",
            "3.6335969529693174e-07\n",
            "0.00021586791159331086\n",
            "--------------------\n",
            "count: 1934, item_Code: A326030\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.002849844605467903\n",
            "4.083234183403421e-06\n",
            "0.003582866760932434\n",
            "--------------------\n",
            "count: 1935, item_Code: A327260\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0001982119645011456\n",
            "1.5343110421870453e-06\n",
            "0.00021577202117799532\n",
            "--------------------\n",
            "count: 1936, item_Code: A330350\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.006360410720844811\n",
            "8.009792676900625e-05\n",
            "0.006919855944699487\n",
            "--------------------\n",
            "count: 1937, item_Code: A330860\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.0005664064864201161\n",
            "2.3565038684118336e-06\n",
            "0.0006779163001447552\n",
            "--------------------\n",
            "count: 1938, item_Code: A331520\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.008848352585952549\n",
            "0.0007641959392047374\n",
            "0.01046074378910052\n",
            "--------------------\n",
            "count: 1939, item_Code: A332290\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.009918381524479021\n",
            "0.0005053676248499515\n",
            "0.012417284701032285\n",
            "--------------------\n",
            "count: 1940, item_Code: A332370\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.002363354992242724\n",
            "5.7141708965140305e-05\n",
            "0.002429270739949978\n",
            "--------------------\n",
            "count: 1941, item_Code: A332570\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.010203738940314603\n",
            "0.0003902803677886203\n",
            "0.012099432371588316\n",
            "--------------------\n",
            "count: 1942, item_Code: A333430\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.045278220825821336\n",
            "0.0015224191976996415\n",
            "0.05545709052594765\n",
            "--------------------\n",
            "count: 1943, item_Code: A333620\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.009240512903852504\n",
            "7.077084070410033e-05\n",
            "0.010138653441452797\n",
            "--------------------\n",
            "count: 1944, item_Code: A334970\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.007267833520078663\n",
            "0.0001527456014118261\n",
            "0.007502334492615413\n",
            "--------------------\n",
            "count: 1945, item_Code: A335810\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.002689131897629219\n",
            "4.773095932179539e-05\n",
            "0.003228460360330844\n",
            "--------------------\n",
            "count: 1946, item_Code: A335890\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.014479222024723034\n",
            "0.00027466676592252084\n",
            "0.017042437950431265\n",
            "--------------------\n",
            "count: 1947, item_Code: A336060\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0026765645577749336\n",
            "0.00018174224393262481\n",
            "0.0032156648874780785\n",
            "--------------------\n",
            "count: 1948, item_Code: A336260\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0010729875175456982\n",
            "3.378761144558075e-06\n",
            "0.0012597886338335893\n",
            "--------------------\n",
            "count: 1949, item_Code: A336370\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.004136683834076393\n",
            "9.884145399825888e-06\n",
            "0.004278896340947374\n",
            "--------------------\n",
            "count: 1950, item_Code: A336570\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.03436123397756698\n",
            "0.0005271183450995355\n",
            "0.038639974865688057\n",
            "--------------------\n",
            "count: 1951, item_Code: A337930\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.002903612375484954\n",
            "5.9535779259084843e-05\n",
            "0.0038854801762609075\n",
            "--------------------\n",
            "count: 1952, item_Code: A338220\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.027571661264422193\n",
            "0.00015396271605004941\n",
            "0.031607056916970834\n",
            "--------------------\n",
            "count: 1953, item_Code: A339770\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.005777590486953462\n",
            "6.350610348080903e-05\n",
            "0.006235224069090684\n",
            "--------------------\n",
            "count: 1954, item_Code: A339950\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.047223875886644846\n",
            "0.0023303036609220347\n",
            "0.05419577355883988\n",
            "--------------------\n",
            "count: 1955, item_Code: A340440\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0025093071873016014\n",
            "8.799927510098748e-05\n",
            "0.003052512448835048\n",
            "--------------------\n",
            "count: 1956, item_Code: A340570\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0004704123935274159\n",
            "1.0105996952052797e-06\n",
            "0.0005787902086610026\n",
            "--------------------\n",
            "count: 1957, item_Code: A340930\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.014841909859630202\n",
            "6.488500419817528e-05\n",
            "0.017414686717444037\n",
            "--------------------\n",
            "count: 1958, item_Code: A344820\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.008373368444639102\n",
            "1.8803707550674927e-05\n",
            "0.008917240650351573\n",
            "--------------------\n",
            "count: 1959, item_Code: A347700\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.007136755873883279\n",
            "0.00014327188043758814\n",
            "0.008085603413432943\n",
            "--------------------\n",
            "count: 1960, item_Code: A347740\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.0018199943082133055\n",
            "5.436442256673941e-05\n",
            "0.0019699115962809005\n",
            "--------------------\n",
            "count: 1961, item_Code: A347770\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.005429607859211197\n",
            "3.7298822968847415e-05\n",
            "0.0065606256018611525\n",
            "--------------------\n",
            "count: 1962, item_Code: A347860\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.001140516794839641\n",
            "1.0252501211384812e-05\n",
            "0.0012513879501807972\n",
            "--------------------\n",
            "count: 1963, item_Code: A347890\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.006028873513544871\n",
            "5.847893676223547e-05\n",
            "0.007749109279835958\n",
            "--------------------\n",
            "count: 1964, item_Code: A348030\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.000619377907908832\n",
            "5.6739415105386215e-06\n",
            "0.0008011611072990603\n",
            "--------------------\n",
            "count: 1965, item_Code: A348150\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 105)\n",
            "Series([], dtype: int64)\n",
            "0.0034295817178644937\n",
            "2.3868353402269504e-05\n",
            "0.003654415031817535\n",
            "--------------------\n",
            "count: 1966, item_Code: A348210\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.007372906786622479\n",
            "1.1246923192091037e-05\n",
            "0.0090779599457783\n",
            "--------------------\n",
            "count: 1967, item_Code: A348350\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.013833063286559385\n",
            "0.0001118778355770408\n",
            "0.01490612691433094\n",
            "--------------------\n",
            "count: 1968, item_Code: A351320\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.011587187026634638\n",
            "0.0002711350100949322\n",
            "0.012702396902858052\n",
            "--------------------\n",
            "count: 1969, item_Code: A351330\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.017463131790039672\n",
            "0.0001558007791249647\n",
            "0.02054832137584768\n",
            "--------------------\n",
            "count: 1970, item_Code: A352480\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.011770355400706952\n",
            "2.988872101406857e-05\n",
            "0.013844482357507666\n",
            "--------------------\n",
            "count: 1971, item_Code: A352700\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.01456734961514788\n",
            "0.000313974223923339\n",
            "0.016606940133881854\n",
            "--------------------\n",
            "count: 1972, item_Code: A352770\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0018415546561300288\n",
            "2.0779535603134674e-05\n",
            "0.002126298762746637\n",
            "--------------------\n",
            "count: 1973, item_Code: A352820\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.0004508697233783702\n",
            "1.6829369873367503e-07\n",
            "0.0004693712499363084\n",
            "--------------------\n",
            "count: 1974, item_Code: A352940\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.01598741516706165\n",
            "0.0003957337283191311\n",
            "0.019562046619923768\n",
            "--------------------\n",
            "count: 1975, item_Code: A353190\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.00452825379315982\n",
            "0.0004458822200878326\n",
            "0.004999488196374808\n",
            "--------------------\n",
            "count: 1976, item_Code: A353200\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0021035730833924996\n",
            "9.472861562126023e-06\n",
            "0.002376092206901044\n",
            "--------------------\n",
            "count: 1977, item_Code: A353810\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.009355157014185048\n",
            "0.0002352868274808924\n",
            "0.011155523196912868\n",
            "--------------------\n",
            "count: 1978, item_Code: A354200\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.0006847858629650242\n",
            "9.059942893924421e-06\n",
            "0.0008038544680241594\n",
            "--------------------\n",
            "count: 1979, item_Code: A355150\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 117)\n",
            "Series([], dtype: int64)\n",
            "0.06228896332574247\n",
            "0.002073418765926578\n",
            "0.06556126363139277\n",
            "--------------------\n",
            "count: 1980, item_Code: A356860\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 116)\n",
            "Series([], dtype: int64)\n",
            "0.0026616929210528423\n",
            "1.370796073233678e-05\n",
            "0.0027858993397945134\n",
            "--------------------\n",
            "count: 1981, item_Code: A356890\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.0006518976775017411\n",
            "5.713322689703147e-06\n",
            "0.0007615533226302006\n",
            "--------------------\n",
            "count: 1982, item_Code: A357230\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.0007806087872571273\n",
            "1.0212981928853233e-05\n",
            "0.000876315879900028\n",
            "--------------------\n",
            "count: 1983, item_Code: A357550\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 112)\n",
            "Series([], dtype: int64)\n",
            "0.01266154574501949\n",
            "2.797897472913936e-05\n",
            "0.015796984785193997\n",
            "--------------------\n",
            "count: 1984, item_Code: A357780\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "5.777337549564739e-05\n",
            "2.594076071148981e-08\n",
            "6.708984504414496e-05\n",
            "--------------------\n",
            "count: 1985, item_Code: A359090\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.012848711671479881\n",
            "0.000996334990771767\n",
            "0.01536826765989744\n",
            "--------------------\n",
            "count: 1986, item_Code: A361390\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 106)\n",
            "Series([], dtype: int64)\n",
            "0.00010194401960082663\n",
            "4.3172146043322115e-07\n",
            "0.00012807055269513037\n",
            "--------------------\n",
            "count: 1987, item_Code: A361610\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0010218653837606932\n",
            "1.262792827382112e-06\n",
            "0.0012039316474772247\n",
            "--------------------\n",
            "count: 1988, item_Code: A363250\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 111)\n",
            "Series([], dtype: int64)\n",
            "0.005632440096572585\n",
            "3.452012277425142e-05\n",
            "0.006641678669652723\n",
            "--------------------\n",
            "count: 1989, item_Code: A363260\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.008384320427785497\n",
            "0.0009620131273239677\n",
            "0.009840572491012568\n",
            "--------------------\n",
            "count: 1990, item_Code: A363280\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.002862367357859815\n",
            "2.656038266171009e-05\n",
            "0.003118255650137842\n",
            "--------------------\n",
            "count: 1991, item_Code: A365590\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.03890460497879455\n",
            "0.0017473684028690407\n",
            "0.043387767605866344\n",
            "--------------------\n",
            "count: 1992, item_Code: A368770\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.022551579583038498\n",
            "0.0006165675293153811\n",
            "0.027296442424912026\n",
            "--------------------\n",
            "count: 1993, item_Code: A369370\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 113)\n",
            "Series([], dtype: int64)\n",
            "0.012983735119163006\n",
            "0.0006377603041745277\n",
            "0.016255513607209855\n",
            "--------------------\n",
            "count: 1994, item_Code: A373200\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.0011197950823770952\n",
            "2.0977198447128233e-05\n",
            "0.001300816813178269\n",
            "--------------------\n",
            "count: 1995, item_Code: A375500\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 108)\n",
            "Series([], dtype: int64)\n",
            "0.002008980715375704\n",
            "5.913424055935793e-06\n",
            "0.0024814255686033\n",
            "--------------------\n",
            "count: 1996, item_Code: A378850\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.002493028554696745\n",
            "5.9799923915303463e-05\n",
            "0.0025832786666057086\n",
            "--------------------\n",
            "count: 1997, item_Code: A383220\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 109)\n",
            "Series([], dtype: int64)\n",
            "0.001815133570926264\n",
            "1.2912457943162579e-06\n",
            "0.0021446813499527693\n",
            "--------------------\n",
            "count: 1998, item_Code: A383310\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 107)\n",
            "Series([], dtype: int64)\n",
            "0.00024861656905462347\n",
            "3.644466100132918e-07\n",
            "0.00028073055665613794\n",
            "--------------------\n",
            "count: 1999, item_Code: A383800\n",
            "--------------------\n",
            "Before feature importances (494, 128)\n",
            "Series([], dtype: int64)\n",
            "after feature importances (494, 110)\n",
            "Series([], dtype: int64)\n",
            "0.025015745771330936\n",
            "0.0002920940980743436\n",
            "0.02673180324091837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(linear_predictions).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5l4bjVsqhJ8",
        "outputId": "af7b80ea-b95c-4581-be2d-44e6648542a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSpoeAEMJyWR"
      },
      "outputs": [],
      "source": [
        "returns_dailys = []\n",
        "for i in range(0, 2000):\n",
        "  linear_prediction = linear_predictions[i].tolist()\n",
        "  returns_daily = (linear_prediction[-1] - linear_prediction[0]) / linear_prediction[0]\n",
        "  returns_dailys.append(returns_daily)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKurN_WNJv4Y",
        "outputId": "4cbc7221-19b6-418b-ea27-7a08d2b49ace"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000,)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "np.array(returns_dailys).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fZ7pohjBzoTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2e7383-2e1d-41af-8490-75248a3f7c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2000it [00:06, 294.03it/s]\n"
          ]
        }
      ],
      "source": [
        "item_Code_list = train.item_Code.unique().tolist()\n",
        "results_df = pd.DataFrame(columns = ['종목코드', '순위'])\n",
        "for idx, item_Code in tqdm(enumerate(item_Code_list)):\n",
        "  results_df = results_df.append({'종목코드': item_Code, 'final_return': np.array(returns_dailys[idx], dtype = float)}, ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZMwT2wt30pTh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "a8de7593-7ca7-4380-a909-31bd13c0a868"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         종목코드   순위           final_return\n",
              "0     A000020  NaN  -0.034362372010946274\n",
              "1     A000040  NaN   -0.06921963393688202\n",
              "2     A000050  NaN   -0.01869654469192028\n",
              "3     A000070  NaN  -0.025315672159194946\n",
              "4     A000080  NaN   -0.03829531371593475\n",
              "...       ...  ...                    ...\n",
              "1995  A375500  NaN                    0.0\n",
              "1996  A378850  NaN  -0.055992692708969116\n",
              "1997  A383220  NaN                    0.0\n",
              "1998  A383310  NaN    0.36611562967300415\n",
              "1999  A383800  NaN   0.027550403028726578\n",
              "\n",
              "[2000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-87de65dd-7094-4da6-8ab4-50170489c694\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>종목코드</th>\n",
              "      <th>순위</th>\n",
              "      <th>final_return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A000020</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.034362372010946274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A000040</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.06921963393688202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A000050</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.01869654469192028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A000070</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.025315672159194946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A000080</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.03829531371593475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>A375500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>A378850</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.055992692708969116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>A383220</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>A383310</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.36611562967300415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>A383800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.027550403028726578</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87de65dd-7094-4da6-8ab4-50170489c694')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-9d2e7e3b-9b6c-4b9c-b46c-d2d83acaf6c3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9d2e7e3b-9b6c-4b9c-b46c-d2d83acaf6c3')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-9d2e7e3b-9b6c-4b9c-b46c-d2d83acaf6c3 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-87de65dd-7094-4da6-8ab4-50170489c694 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-87de65dd-7094-4da6-8ab4-50170489c694');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "d7K_aNw8ziww",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "06317295-f250-4bc8-b98a-f68a24f4a3c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         종목코드    순위\n",
              "0     A000020   872\n",
              "1     A000040  1352\n",
              "2     A000050   674\n",
              "3     A000070   752\n",
              "4     A000080   923\n",
              "...       ...   ...\n",
              "1995  A375500   497\n",
              "1996  A378850  1169\n",
              "1997  A383220   498\n",
              "1998  A383310    17\n",
              "1999  A383800   257\n",
              "\n",
              "[2000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-a90aa1f6-5956-49ef-b097-edb428138f91\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>종목코드</th>\n",
              "      <th>순위</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A000020</td>\n",
              "      <td>872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A000040</td>\n",
              "      <td>1352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A000050</td>\n",
              "      <td>674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A000070</td>\n",
              "      <td>752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A000080</td>\n",
              "      <td>923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>A375500</td>\n",
              "      <td>497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>A378850</td>\n",
              "      <td>1169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>A383220</td>\n",
              "      <td>498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>A383310</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>A383800</td>\n",
              "      <td>257</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a90aa1f6-5956-49ef-b097-edb428138f91')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-6f5aab22-ce36-4602-a49b-556766ae0268\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6f5aab22-ce36-4602-a49b-556766ae0268')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-6f5aab22-ce36-4602-a49b-556766ae0268 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a90aa1f6-5956-49ef-b097-edb428138f91 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a90aa1f6-5956-49ef-b097-edb428138f91');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "sample_submission['순위'] = results_df['final_return'].rank(method = 'first', ascending = False).astype(int)\n",
        "sample_submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFKtHheuQyui"
      },
      "outputs": [],
      "source": [
        "def calculate_sharpe_ratio(returns):\n",
        "  returns = np.array(returns)\n",
        "  risk_free_rate = 0.035\n",
        "  trading_days_per_year = 252\n",
        "  excess_returns = returns - risk_free_rate\n",
        "  annualized_return = np.mean(returns) * trading_days_per_year\n",
        "  annualized_volatility = np.std(returns) * np.sqrt(trading_days_per_year)\n",
        "  sharpe_ratio = annualized_return / annualized_volatility\n",
        "  return sharpe_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TQJfJS3RtHo"
      },
      "outputs": [],
      "source": [
        "sharpe_ratios = []\n",
        "for returns in model_predictions:\n",
        "  sharpe_ratio = calculate_sharpe_ratio(returns)\n",
        "  sharpe_ratios.append(sharpe_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NPfZGSw2jqw"
      },
      "outputs": [],
      "source": [
        "# def annualized_total_assets_final_return_process(closing_price, number_of_trading_days):\n",
        "#   closing_price = np.array(closing_price) # 배열 처리를 위해 변환\n",
        "#   final_buy_yields = np.zeros((2000, ))\n",
        "#   final_short_selling_yields = np.zeros((2000, ))\n",
        "\n",
        "#   for n in range(1, number_of_trading_days):\n",
        "\n",
        "#     # 마지막 거래일의 종가\n",
        "#     closing_price_last_trading_day = closing_price[:, n]\n",
        "\n",
        "#     # 첫번째 거래일의 종가\n",
        "#     closing_price_first_trading_day = closing_price[:, 0]\n",
        "\n",
        "#     # 최종 매수 수익률\n",
        "#     final_buy_yield = (closing_price_last_trading_day - closing_price_first_trading_day) / closing_price_first_trading_day\n",
        "\n",
        "#     # 최종 공매도 수익률\n",
        "#     final_short_selling_yield = ((closing_price_last_trading_day - closing_price_first_trading_day) / closing_price_first_trading_day) * (-1)\n",
        "\n",
        "#     # 200주에 대한 최종 매수 수익률과 공매도 수익률의 총합 계산\n",
        "#     final_buy_yields += final_buy_yield\n",
        "#     final_short_selling_yields += final_short_selling_yield\n",
        "\n",
        "#   total_final_buy_yield = np.sum(final_buy_yields[:200])\n",
        "#   total_final_short_selling_yield = np.sum(final_short_selling_yields[-200:])\n",
        "\n",
        "#   # 총자산 최종 수익률\n",
        "#   total_assets_final_return = (total_final_buy_yield + total_final_short_selling_yield) / 400\n",
        "\n",
        "#   # 연율화된 총자산 최종 수익률\n",
        "#   annualized_total_assets_final_return = total_assets_final_return * (250 / number_of_trading_days)\n",
        "\n",
        "#   return annualized_total_assets_final_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lTZP82UjhYcA"
      },
      "outputs": [],
      "source": [
        "def annualized_total_assets_final_return_process(closing_price, number_of_trading_days):\n",
        "  closing_price = np.array(closing_price) # 배열 처리를 위해 변환\n",
        "\n",
        "  # 마지막 거래일의 종가\n",
        "  closing_price_last_trading_day = closing_price[:, number_of_trading_days-1]\n",
        "\n",
        "  # 첫번째 거래일의 종가\n",
        "  closing_price_first_trading_day = closing_price[:, 0]\n",
        "\n",
        "  # 최종 매수 수익률\n",
        "  final_buy_yield = (closing_price_last_trading_day - closing_price_first_trading_day) / closing_price_first_trading_day\n",
        "\n",
        "  # 최종 공매도 수익률\n",
        "  final_short_selling_yield = ((closing_price_last_trading_day - closing_price_first_trading_day) / closing_price_first_trading_day) * (-1)\n",
        "\n",
        "  total_final_buy_yield = np.sum(final_buy_yield[:200])\n",
        "  total_final_short_selling_yield = np.sum(final_short_selling_yield[-200:])\n",
        "\n",
        "  # 총자산 최종 수익률\n",
        "  total_assets_final_return = (total_final_buy_yield + total_final_short_selling_yield) / 400\n",
        "\n",
        "  # 연율화된 총자산 최종 수익률\n",
        "  annualized_total_assets_final_return = total_assets_final_return * (250 / number_of_trading_days)\n",
        "\n",
        "  return annualized_total_assets_final_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "2druq4qvg2Mk"
      },
      "outputs": [],
      "source": [
        "def total_assets_daily_return_volatility_process(closing_price, number_of_trading_days):\n",
        "  closing_price = np.array(closing_price)\n",
        "  daily_buy_yields = []\n",
        "  daily_short_selling_yields = []\n",
        "  total_assets_daily_return_volatility = 0\n",
        "  ns = []\n",
        "  annualized_daliy_returns = []\n",
        "  daliy_returns = []\n",
        "\n",
        "  for n in range(3, number_of_trading_days):\n",
        "\n",
        "    # 현시점 거래일의 종가\n",
        "    closing_price_current_trading_day = closing_price[:, n]\n",
        "\n",
        "    # 전날 거래일의 종가\n",
        "    closing_price_previous_trading_day = closing_price[:, n-1]\n",
        "\n",
        "    # 일간 매수일의 수익률\n",
        "    daily_buy_yield = (closing_price_current_trading_day - closing_price_previous_trading_day) / closing_price_previous_trading_day\n",
        "    daily_short_selling_yield = ((closing_price_current_trading_day - closing_price_previous_trading_day) / closing_price_previous_trading_day) * (-1)\n",
        "\n",
        "    # 연율화된 일간 수익률\n",
        "    annualized_daliy_return = (daily_buy_yield[:200] + daily_short_selling_yield[-200:]) * 250\n",
        "    daliy_return = (daily_buy_yield[:200] + daily_short_selling_yield[-200:])\n",
        "    annualized_daliy_returns.append(annualized_daliy_return)\n",
        "    daliy_returns.append(daliy_return)\n",
        "\n",
        "  # 총자산 일간 수익률 변동성\n",
        "  for n in range(3, number_of_trading_days):\n",
        "    temp = (np.mean(annualized_daliy_returns[n-3]) - np.mean(daliy_returns[n-3])) ** 2\n",
        "    total_assets_daily_return_volatility += temp / (n - 2)\n",
        "\n",
        "  return np.sqrt(total_assets_daily_return_volatility)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "tbWZ3YLsCkqa"
      },
      "outputs": [],
      "source": [
        "def public_evaluation_process(closing_price, number_of_trading_days):\n",
        "\n",
        "  # 연율화된 총자산 최종 수익률\n",
        "  annualized_total_assets_final_return = annualized_total_assets_final_return_process(closing_price, number_of_trading_days)\n",
        "\n",
        "  # 무위험 수익률\n",
        "  risk_free_rate_of_return = 3.5 / 100\n",
        "\n",
        "  # 총자산 일간 수익률 변동성\n",
        "  total_assets_daily_return_volatility = total_assets_daily_return_volatility_process(closing_price, number_of_trading_days)\n",
        "\n",
        "  # 샤프지수\n",
        "  sharp_index = (annualized_total_assets_final_return - risk_free_rate_of_return) / total_assets_daily_return_volatility\n",
        "  return sharp_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "vrHlT72lURR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "983ac17b-8a58-4a70-d53c-00dafe3f3877"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.001551673256821365"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "public_evaluation_process(model_predictions, 15) # 0.24 -> lstm_final, 0.102 -> bilstm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pgSTGsyu1W_g"
      },
      "outputs": [],
      "source": [
        "sample_submission.to_csv('./final_private.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "TlFb2M7977kT",
        "outputId": "eafe193f-b018-4230-be15-80e6ea2f6425"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         종목코드    순위\n",
              "0     A000020   872\n",
              "1     A000040  1352\n",
              "2     A000050   674\n",
              "3     A000070   752\n",
              "4     A000080   923\n",
              "...       ...   ...\n",
              "1995  A375500   497\n",
              "1996  A378850  1169\n",
              "1997  A383220   498\n",
              "1998  A383310    17\n",
              "1999  A383800   257\n",
              "\n",
              "[2000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-51336ae1-0619-4c33-991f-ca702f930424\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>종목코드</th>\n",
              "      <th>순위</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A000020</td>\n",
              "      <td>872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A000040</td>\n",
              "      <td>1352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A000050</td>\n",
              "      <td>674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A000070</td>\n",
              "      <td>752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A000080</td>\n",
              "      <td>923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>A375500</td>\n",
              "      <td>497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>A378850</td>\n",
              "      <td>1169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>A383220</td>\n",
              "      <td>498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>A383310</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>A383800</td>\n",
              "      <td>257</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51336ae1-0619-4c33-991f-ca702f930424')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-9280ae15-9e2c-499e-8a91-2f9a3faa8874\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9280ae15-9e2c-499e-8a91-2f9a3faa8874')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-9280ae15-9e2c-499e-8a91-2f9a3faa8874 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-51336ae1-0619-4c33-991f-ca702f930424 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-51336ae1-0619-4c33-991f-ca702f930424');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv('/content/bilstm_final_private_50_mac.csv')"
      ],
      "metadata": {
        "id": "0jiR446bDwFi"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "hxkVWZSnD3Z8",
        "outputId": "70b926df-cd1f-492e-8a07-d21bb8580c9f"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         종목코드    순위\n",
              "0     A000020  1014\n",
              "1     A000040  1725\n",
              "2     A000050   548\n",
              "3     A000070   320\n",
              "4     A000080   713\n",
              "...       ...   ...\n",
              "1995  A375500   732\n",
              "1996  A378850  1296\n",
              "1997  A383220  1879\n",
              "1998  A383310    53\n",
              "1999  A383800   291\n",
              "\n",
              "[2000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-9142ce18-8277-4c62-b127-0a01cc0efe5d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>종목코드</th>\n",
              "      <th>순위</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A000020</td>\n",
              "      <td>1014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A000040</td>\n",
              "      <td>1725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A000050</td>\n",
              "      <td>548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A000070</td>\n",
              "      <td>320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A000080</td>\n",
              "      <td>713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>A375500</td>\n",
              "      <td>732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>A378850</td>\n",
              "      <td>1296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>A383220</td>\n",
              "      <td>1879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>A383310</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>A383800</td>\n",
              "      <td>291</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9142ce18-8277-4c62-b127-0a01cc0efe5d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-31be14b1-c6d9-44d7-9cbd-32615c2790ce\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-31be14b1-c6d9-44d7-9cbd-32615c2790ce')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-31be14b1-c6d9-44d7-9cbd-32615c2790ce button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9142ce18-8277-4c62-b127-0a01cc0efe5d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9142ce18-8277-4c62-b127-0a01cc0efe5d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_predictions = np.load('/content/model_predictions_bilstm_fianl_private_50_mac.npy')"
      ],
      "metadata": {
        "id": "6y6WJJ1YAWj4"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eARWk1uMAyfK",
        "outputId": "da5e52a7-a3f7-4251-bbd6-1f6d7c7e4030"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 10010.667  ,  10009.822  ,   9920.27   , ...,   8183.0205 ,\n",
              "          9549.287  ,   9544.381  ],\n",
              "       [   532.0176 ,    522.02734,    516.47894, ...,    476.06537,\n",
              "           473.05383,    475.1204 ],\n",
              "       [  9636.607  ,   9621.921  ,   9642.411  , ...,   9632.15   ,\n",
              "          9555.326  ,   9578.632  ],\n",
              "       ...,\n",
              "       [114324.53   ,  97795.875  , 110919.59   , ...,  97795.875  ,\n",
              "        100297.     ,  97896.36   ],\n",
              "       [ 66626.805  ,  78827.22   ,  87130.555  , ...,  86790.83   ,\n",
              "         85808.13   ,  85627.99   ],\n",
              "       [  8004.335  ,   7980.1475 ,   7968.4937 , ...,   8285.26   ,\n",
              "          8244.689  ,   8245.701  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UMLdjtkBxP7",
        "outputId": "f176c75b-e568-4ceb-b63b-1c1eb8c05aea"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 10010.667  ,  10009.822  ,   9920.27   , ...,   8183.0205 ,\n",
              "          9549.287  ,   9544.381  ],\n",
              "       [   532.0176 ,    522.02734,    516.47894, ...,    476.06537,\n",
              "           473.05383,    475.1204 ],\n",
              "       [  9636.607  ,   9621.921  ,   9642.411  , ...,   9632.15   ,\n",
              "          9555.326  ,   9578.632  ],\n",
              "       ...,\n",
              "       [114324.53   ,  97795.875  , 110919.59   , ...,  97795.875  ,\n",
              "        100297.     ,  97896.36   ],\n",
              "       [ 66626.805  ,  78827.22   ,  87130.555  , ...,  86790.83   ,\n",
              "         85808.13   ,  85627.99   ],\n",
              "       [  8004.335  ,   7980.1475 ,   7968.4937 , ...,   8285.26   ,\n",
              "          8244.689  ,   8245.701  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission['Close'] = list(model_predictions)"
      ],
      "metadata": {
        "id": "Pk_Wjx8B8b0f"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "gsZN8nwMA80G",
        "outputId": "46412a5c-9899-4e1f-9f3b-0742cc12e663"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         종목코드    순위                                              Close\n",
              "0     A000020  1014  [10010.667, 10009.822, 9920.27, 9931.122, 9905...\n",
              "1     A000040  1725  [532.0176, 522.02734, 516.47894, 508.62018, 50...\n",
              "2     A000050   548  [9636.607, 9621.921, 9642.411, 9647.43, 9643.7...\n",
              "3     A000070   320  [68052.26, 70686.41, 70564.16, 70676.33, 70461...\n",
              "4     A000080   713  [20946.152, 20663.934, 20565.465, 19283.93, 20...\n",
              "...       ...   ...                                                ...\n",
              "1995  A375500   732  [31966.734, 31867.459, 31718.16, 31652.498, 31...\n",
              "1996  A378850  1296  [4495.556, 4382.273, 4340.7783, 4271.64, 4216....\n",
              "1997  A383220  1879  [114324.53, 97795.875, 110919.59, 97795.875, 1...\n",
              "1998  A383310    53  [66626.805, 78827.22, 87130.555, 34872.277, 95...\n",
              "1999  A383800   291  [8004.335, 7980.1475, 7968.4937, 7970.7173, 79...\n",
              "\n",
              "[2000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-d15e58a2-61ac-43da-a714-05c67e421c86\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>종목코드</th>\n",
              "      <th>순위</th>\n",
              "      <th>Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A000020</td>\n",
              "      <td>1014</td>\n",
              "      <td>[10010.667, 10009.822, 9920.27, 9931.122, 9905...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A000040</td>\n",
              "      <td>1725</td>\n",
              "      <td>[532.0176, 522.02734, 516.47894, 508.62018, 50...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A000050</td>\n",
              "      <td>548</td>\n",
              "      <td>[9636.607, 9621.921, 9642.411, 9647.43, 9643.7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A000070</td>\n",
              "      <td>320</td>\n",
              "      <td>[68052.26, 70686.41, 70564.16, 70676.33, 70461...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A000080</td>\n",
              "      <td>713</td>\n",
              "      <td>[20946.152, 20663.934, 20565.465, 19283.93, 20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>A375500</td>\n",
              "      <td>732</td>\n",
              "      <td>[31966.734, 31867.459, 31718.16, 31652.498, 31...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>A378850</td>\n",
              "      <td>1296</td>\n",
              "      <td>[4495.556, 4382.273, 4340.7783, 4271.64, 4216....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>A383220</td>\n",
              "      <td>1879</td>\n",
              "      <td>[114324.53, 97795.875, 110919.59, 97795.875, 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>A383310</td>\n",
              "      <td>53</td>\n",
              "      <td>[66626.805, 78827.22, 87130.555, 34872.277, 95...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>A383800</td>\n",
              "      <td>291</td>\n",
              "      <td>[8004.335, 7980.1475, 7968.4937, 7970.7173, 79...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d15e58a2-61ac-43da-a714-05c67e421c86')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-820d245f-92b9-4803-94b5-119c7bb45137\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-820d245f-92b9-4803-94b5-119c7bb45137')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-820d245f-92b9-4803-94b5-119c7bb45137 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d15e58a2-61ac-43da-a714-05c67e421c86 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d15e58a2-61ac-43da-a714-05c67e421c86');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission_copy = sample_submission.copy()\n",
        "sample_submission_copy = sample_submission_copy.drop(['순위'], axis = 1)"
      ],
      "metadata": {
        "id": "sgjapAhG-kKU"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc_close = sample_submission_copy.groupby(['종목코드'])['Close'].apply(list).to_dict()"
      ],
      "metadata": {
        "id": "5bAehvH58nex"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cc_close = {key: value[0] for key, value in cc_close.items()}"
      ],
      "metadata": {
        "id": "4dnzwceY-9F9"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "id": "Li6EVrLBJtaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60efe9cf-0ef1-4519-eddf-2fce07f36df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총자산 일간 수익률 변동성(다른계산법들) 12.55082379310271 12.550820476146402\n"
          ]
        }
      ],
      "source": [
        "cc_rank = sample_submission.sort_values(['순위'])['종목코드'].tolist()\n",
        "\n",
        "np_cc_buy = np.array([ cc_close[cc] for cc in cc_rank[:200] ])\n",
        "np_cc_sell = np.array([ cc_close[cc] for cc in cc_rank[-200:] ])\n",
        "\n",
        "assert np_cc_buy.shape[1] == 15\n",
        "\n",
        "value = {}\n",
        "value['매매일수'] = 15\n",
        "value['무위험수익률'] = 0.035\n",
        "value['총 매수 수익률'] = +np.sum(((np_cc_buy[:,-1] - np_cc_buy[:, 0]) / np_cc_buy[:, 0]))#= + np.sum( np_cc_buy [:,-1]/np_cc_buy [:,0]-1.0 )\n",
        "value['총 공매도 수익률'] = +np.sum(((np_cc_sell[:,-1] - np_cc_sell[:, 0]) / np_cc_sell[:, 0]) * -1)#= - np.sum( np_cc_sell[:,-1]/np_cc_sell[:,0]-1.0 )\n",
        "value['총 자산 최종 수익률'] = ( value['총 매수 수익률'] + value['총 공매도 수익률'] ) / 400\n",
        "value['연율화된 총자산 최종 수익률'] = value['총 자산 최종 수익률'] * 250 / value['매매일수']\n",
        "\n",
        "profit_day_buy = + ( np_cc_buy [:,1:]/np_cc_buy [:,:-1]-1.0 )\n",
        "profit_day_sell = - ( np_cc_sell[:,1:]/np_cc_sell[:,:-1]-1.0 )\n",
        "\n",
        "value['연율화된 일간수익률의 일별 평균'] = ( profit_day_buy + profit_day_sell ).sum(0) * 250 / 400\n",
        "value['연율화된 일간수익률의 평균'] = np.mean(value['연율화된 일간수익률의 일별 평균'])\n",
        "value['총자산 일간 수익률 변동성'] = ( np.sum( ( value['연율화된 일간수익률의 일별 평균'] - value['연율화된 일간수익률의 평균'] ) ** 2.0 ) / (15-2) ) ** 0.5\n",
        "value['샤프지수'] = ( value['연율화된 총자산 최종 수익률'] - value['무위험수익률'] ) / value['총자산 일간 수익률 변동성']\n",
        "\n",
        "print('총자산 일간 수익률 변동성(다른계산법들)',\n",
        "   value['연율화된 일간수익률의 일별 평균'].std() * np.sqrt(14/13),\n",
        "   np.concatenate( ( profit_day_buy, profit_day_sell ) ).mean(0).std() * np.sqrt(14/13) * 250\n",
        "   )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP_gsaSY-L62",
        "outputId": "912eb9f5-7380-4848-b583-d43662dab3be"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'매매일수': 15,\n",
              " '무위험수익률': 0.035,\n",
              " '총 매수 수익률': 80.85518,\n",
              " '총 공매도 수익률': 39.940994,\n",
              " '총 자산 최종 수익률': 0.3019904327392578,\n",
              " '연율화된 총자산 최종 수익률': 5.03317387898763,\n",
              " '연율화된 일간수익률의 일별 평균': array([28.543146 ,  2.4315226,  4.6961284,  4.426783 , 35.893715 ,\n",
              "         4.5735226,  3.9092872,  1.9318991, 25.856997 ,  4.316855 ,\n",
              "        -0.7135781, -4.343806 ,  7.8151746, -4.9374137], dtype=float32),\n",
              " '연율화된 일간수익률의 평균': 8.171446,\n",
              " '총자산 일간 수익률 변동성': 12.55082366139079,\n",
              " '샤프지수': 0.3982347305510441}"
            ]
          },
          "metadata": {},
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "private 50 mac - {'매매일수': 15,\n",
        " '무위험수익률': 0.035,\n",
        " '총 매수 수익률': 80.85519,\n",
        " '총 공매도 수익률': 39.94099,\n",
        " '총 자산 최종 수익률': 0.3019904327392578,\n",
        " '연율화된 총자산 최종 수익률': 5.03317387898763,\n",
        " '연율화된 일간수익률의 일별 평균': array([28.543146 ,  2.4315226,  4.6961284,  4.426783 , 35.893715 ,\n",
        "         4.5735226,  3.9092872,  1.9318991, 25.856997 ,  4.316855 ,\n",
        "        -0.7135781, -4.343806 ,  7.8151746, -4.9374137], dtype=float32),\n",
        " '연율화된 일간수익률의 평균': 8.171446,\n",
        " '총자산 일간 수익률 변동성': 12.55082366139079,\n",
        " '샤프지수': 0.3982347305510441}"
      ],
      "metadata": {
        "id": "gVoKao-YK6V-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "private_50 - {'매매일수': 15,\n",
        " '무위험수익률': 0.035,\n",
        " '총 매수 수익률': 39.739315,\n",
        " '총 공매도 수익률': 20.54947,\n",
        " '총 자산 최종 수익률': 0.1507219696044922,\n",
        " '연율화된 총자산 최종 수익률': 2.5120328267415366,\n",
        " '연율화된 일간수익률의 일별 평균': array([ -1.3785872,   3.1500013,  -5.108241 ,   4.8850703,  31.683222 ,\n",
        "          4.7799764,   1.0837826,   3.49767  ,  21.084288 ,   5.2552257,\n",
        "         -1.9286356,  -4.4341445,   8.721679 , -16.13519  ], dtype=float32),\n",
        " '연율화된 일간수익률의 평균': 3.9397233,\n",
        " '총자산 일간 수익률 변동성': 11.490873357705956,\n",
        " '샤프지수': 0.21556523596010221}"
      ],
      "metadata": {
        "id": "_N84k8SGI3Ds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "private - {'매매일수': 15,\n",
        " '무위험수익률': 0.035,\n",
        " '총 매수 수익률': 48.93498,\n",
        " '총 공매도 수익률': 34.26474,\n",
        " '총 자산 최종 수익률': 0.20799930572509764,\n",
        " '연율화된 총자산 최종 수익률': 3.4666550954182944,\n",
        " '연율화된 일간수익률의 일별 평균': array([ 5.4506407 ,  4.4643264 ,  4.529664  ,  0.59626263, 17.545414  ,\n",
        "         5.949546  ,  1.2998953 ,  2.8833265 ,  2.7587273 ,  1.9126161 ,\n",
        "        -4.782438  ,  0.6964591 ,  0.9037506 ,  5.8440022 ], dtype=float32),\n",
        " '연율화된 일간수익률의 평균': 3.5751572,\n",
        " '총자산 일간 수익률 변동성': 4.917825906193692,\n",
        " '샤프지수': 0.6977992228428299}"
      ],
      "metadata": {
        "id": "KhqArmuvIlVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "bilstm_final - {'매매일수': 15,\n",
        " '무위험수익률': 0.035,\n",
        " '총 매수 수익률': 140.85645,\n",
        " '총 공매도 수익률': 36.04045,\n",
        " '총 자산 최종 수익률': 0.4422422409057617,\n",
        " '연율화된 총자산 최종 수익률': 7.370704015096028,\n",
        " '연율화된 일간수익률의 일별 평균': array([40.572647  , 14.423359  , -3.1744742 , -6.8076286 ,  9.145387  ,\n",
        "         6.2669497 ,  7.2636857 ,  1.4549137 , -8.12414   ,  5.3880687 ,\n",
        "         1.6152933 , 18.258434  ,  0.59860605, 16.204739  ], dtype=float32),\n",
        " '연율화된 일간수익률의 평균': 7.3632746,\n",
        " '총자산 일간 수익률 변동성': 12.475514254107809,\n",
        " '샤프지수': 0.5880081466526001}"
      ],
      "metadata": {
        "id": "GZz2WnZyJHtC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ddjbClEZ_4pv"
      },
      "execution_count": 217,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}